\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Making Sense of Crim Data},
            pdfauthor={Reka Solymosi},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Making Sense of Crim Data}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Reka Solymosi}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-10-08}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

This workbook contains the lab materials and homework assignments for an
introduction to data analysis course designed for LAWS20441 Making Sense
of Criminological Data, a 2nd year undergraduates on the BA Criminology
programme at the University of Manchester.

It makes use of Excel, as we have identified a gap in training students
to use Excel, despite it being a primary tool for data analysis (whether
we like it or not) in many public and private sector organisations. As
many students take
\href{https://www.humanities.manchester.ac.uk/q-step/}{Q-step
internships}, this skill was identified as important.

Making Sense of Crim Data introduces students to data, and the concepts
of descriptive data analysis. The role of this term is to familiarise
students with basic concepts of data analysis, and get aquainted with
descriptive statistics to be able to talk about data about crime,
policing, and criminal justice topics. Details can be found in the
Syllabus.

\hypertarget{disclaimer}{%
\section{Disclaimer}\label{disclaimer}}

Please beware that:

\begin{itemize}
\tightlist
\item
  In making these notes, while I briefly cover some concepts, students
  are expected to do the weekly reading, and attend the weekly lectures,
  as well as participate in lab disucssions to receive a complete course
  experience. These notes are \emph{not} intended to be a stand-alone
  reference or textbook, rather a set of exercises to gain hands-on
  practice with the concepts introduced during the course.
\item
  These pages are the content of the BA Criminology 2nd year course
  Making Sense of Criminological Data. They are meant to (very gently)
  introduce undergraduates to the concept of data analysis, and cover
  descriptive statsitics and the key concepts required to build an
  understanding of quantitative data analysis in crime research. It is
  followed in the second term by Modeling Criminological Data where
  students cover inferential statistics. The notes presented here are
  supported by compulsory reading and some lectures, and so do not
  provide a comprehensive description of these techniques and tools and
  how to use them.
\item
  The handouts below use, among other data sets, dara from the UK data
  service such as the Crime Survey for England and Wales that is
  available under a Open Government Licence. This dataset is designed to
  be a learning resource and should not be used for research purposes or
  the production of summary statistics.
\end{itemize}

\hypertarget{overview}{%
\section{Overview of course}\label{overview}}

\hypertarget{module-structure}{%
\subsection{Module structure}\label{module-structure}}

Hi there and welcome to this course for making sense of criminological
data. This introduction will explain the structure of the course.

The course is 10 weeks, each week made up of 5 elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Preparatory reading
\item
  Lab session
\item
  Post-lab task
\item
  Homework quiz
\item
  Lecture
\end{enumerate}

\hypertarget{preparatory-reading}{%
\subsubsection{1) Preparatory reading}\label{preparatory-reading}}

For each week you will receive some preliminary reading or videos to
watch, before coming to the session. It is very important that you read
these before coming to the lab session, as it will make engaging with
the lab material easier. Also, you can take the labs as an opportunity
to ask questions about the readings, and discuss with myself and the
teaching assistants during the 2-hour lab sessions.

\hypertarget{lab-session}{%
\subsubsection{2) Lab session}\label{lab-session}}

Lab sessions are the two-hour sessions in computer labs, where you are
to work through the lab notes included in this book. You should take
time to engage with these notes, and ask lots of questions from myself
and the teaching assistants present. This is a time to really engage
with the materials.

When you come into a lesson, you will be able to sit at a PC and get
started straight away. You will find the instructions for each week in
this booklet. You can open up the link via Blackboard, and read through
the instructions chronologically. That just means start at the top, and
read through to the bottom.

Usually within the notes there are some general introduction to the
topic covered that week, with links to
\href{http://www.gapminder.org/videos/the-joy-of-stats/}{videos} or
further
\href{http://flowingdata.com/2015/10/26/top-brewery-road-trip-routed-algorithmically/}{reading}.
You should come equipped with headphones to watch the videos.

These lab notes also contain within them activities. {Activities will be
denoted by red colour text, just like this one } You should do these
activities in the lab, and ask for our help when you are stuck, or if
you do not understand a concept. These activities will help you with
your learning, but also will contribute towards your homework. You are
welcome to discuss with each other, and with us, but please do make sure
that when it comes to understanding the learning behind these
activities, you are confident in your ability. The final essay will rely
heavily on your ability to take the concepts you learn duing the
activities, and apply them in a way that shows your understanding.

\hypertarget{post-lab-task}{%
\subsubsection{3) Post-lab task}\label{post-lab-task}}

Each week, after you have completed the lab notes, you must complete
some post-lab tasks. These will take the form of a worksheet. You can
find each worksheet and relevant material (eg: data) on Blackboard in
the folder for that week. There will also always be a link at the bottom
of the lab notes. You have to complete these tasks in order to be able
to take the homework quiz (which is assessed). The tasks will always
mirror the in-lab activities, so if you get through those, the task
should be a breeze.

\hypertarget{homework-quiz}{%
\subsubsection{4) Homework quiz}\label{homework-quiz}}

Each week you will have to complete a homework quiz. This quiz is
assessed, and your score on all the quizzes combined counts for 20\% of
your final mark (the other 80\% is your final essay). The questions in
the homework quiz will ask about key concepts from your reading, and
about the answers to the post-lab task. Make sure that you have finished
the task before you begin your homework quiz, and have it with you while
you do so. The homework quiz will be available on Blackboard, and will
be open on each Friday at 11:00am, and close the following Thursday at
9:00am. You can take the homework quiz any time between these times. You
can take it only once. Once you activate the quiz, you will have only 30
minutes to complete it. Please make sure you are in a quiet environment
where you will not be disturbed, with your reading notes and your
post-lab task with you, so you can complete the homework quiz
successfully. Upon submission you will receive immediate feedback.

Don't forget your homework is graded, and counts towards your final
mark. But you get to practice for completing the tasks with the
activities in the lab. And if you have time left over, you can always
complete these tasks here in the lab. No matter where you do them, by
havign the tasks completed and with you when you take the quiz, you will
be more confident in the homework quiz exercises.

\hypertarget{lectures}{%
\subsubsection{5) Lectures}\label{lectures}}

In the lectures we will go over one last time the basic concepts covered
in that week. The lecture is the final event that should pull together
all your learning from that week. It is your chance to ask questions,
discuss, and further interrogate the material we cover. I encourage you
to bring your own examples to lectures wherever you encounter them.
Lectures are podcast, however they are also attendance monitored. You
must attend, and sign in to these lectures to receive attendance credit.

\hypertarget{week1}{%
\chapter{Week 1}\label{week1}}

\hypertarget{learning-outcomes}{%
\section{Learning outcomes}\label{learning-outcomes}}

Welcome to week 1! This week is all about getting set up, and taking
some first steps in learning about what is \emph{data}. These are the
building blocks of all the weeks to come, so pay special attention, and
do set a pattern of asking questions! Now without further ado, let's get
started.

\hypertarget{setting-up-your-working-environment}{%
\section{Setting up your working
environment}\label{setting-up-your-working-environment}}

There is a myth about the scientist and the messy workspace, typically
illustrated with Albert Einstein:

\includegraphics[width=13.33in]{imgs/einstein_desk}

However many of us need order to be able to work properly. An organised
workspace is also prominent, as we can see with these famous work spaces
of : Galileo, Marie Curie, John Dalton, Alan Turing, and Charles
Dickens:

\includegraphics[width=3.28in]{imgs/galileo_desk}
\includegraphics[width=3.28in]{imgs/marie_curie}
\includegraphics[width=6.28in]{imgs/Dalton_John_desk}
\includegraphics[width=6.94in]{imgs/alan_turing_desk}
\includegraphics[width=6.94in]{imgs/charles_dickens_desk}

When working with data, you have to consider your workspace. You can
think of your computer folders as your desk. It helps immensely to keep
our data, your code and your notes organised. You will likely have a
project folder, where you save your data, your graphs, your analysis
outputs, etc. You want to consider the layout of this folder, how many
subfolders will you have, what is the best structure to work for you?
You might think this is trivial, but when you are working on a project
with multiple data sets, or many graphs, it can get very messy very
quickly. I recommend going through
\href{https://www.data.cam.ac.uk/data-management-guide/organising-your-data}{this
resource from the university of Cambridge data management guide} to
consider Naming and Organising Files, Documentation and Metadata,
Managing References, and Organising E-mail.

Normally, say when you are working on a Windows PC at home, you can save
files to your C: drive, and easily access them the next time you turn
the computer on. When you are using computers at the university,
however, you cannot be certain of being able to use the same computer
again when you want to find and use your file again. Fortunately, once
you have logged on to a computer at the university, a personal drive is
available for you to use, and in effect, this drive follows you around
to whichever PC you are logged on to. All students and staff have a
personal file storage space on the network - known as the P: drive as
this is usually the network drive letter allocated to it. Wherever you
log on to the campus network your P: drive is available (in PC Clusters
the My Documents icon on the desktop is a shortcut to the P: drive).

You should already have a username and password with which to log on.
Your username has 7 or 8 letters/digits, typically beginning
`m\ldots{}'. On the computers in the Faculty clusters (where you work in
class time) you will see the Faculty computer `image' and a version of
the operating systems Windows 7.

Away from the campus you can download and upload files to and from your
P: drive over an Internet connection - for example to and from your home
computer, but please remember that large files may take a long time to
upload/download depending on the speed of your home internet connection.
\href{http://servicedesk.manchester.ac.uk/portal/app/portlets/results/viewsolution.jsp?solutionid=040918305513985\&SToken=A9F6241A4BCC47E813D7C92C68B579F7}{See
here for details about remote access}.

It's generally good to create a folder to save your data and our outputs
in, which we will call a \textbf{working directory}. So firstly, before
we begin to do any work, we should create our \textbf{working
directory}. This is simply a folder where you will save all our data,
and also where you will be reading data in from. You can create a new
folder, where you will save everything for this course, or you can
choose an existing folder. It's advised that you create a folder, and
also give it some name you remember, that will be meaningful. Generally
try to avoid spaces and special characters in folder (and file) names.
\href{http://www2.stat.duke.edu/~rcs46/lectures_2015/01-markdown-git/slides/naming-slides/naming-slides.pdf}{Here
is a handy guide you should read about naming files and folders that
will be relevant for all your future work}.

It's not necessarily a good idea to just dump everything into `Desktop'
either, as you want to be able to find things later, and maybe keep
things tidy. We will now show you where your personal drive is and how
to save your word file to it.

 Just to have some practice with doing this, create a Word document to
put into the folder. Open Microsoft Word now (you can find this on the
`Start' button in the bottom left corner of your screen). The only thing
you have to type in the document is your name.

 Save the file to your p: drive as follows. Click on the \emph{save}
button (or click File then Save in the drop down menus). The dialogue
box below will appear. Click on the down arrow in the `Save in:' box to
change the default directory to your P: drive. The P: drive is the one
that starts with your user name. Click on this drive.

\includegraphics{imgs/save_doc_as.png}

 Now create a new folder within your P: drive, especially for material
for this course. To do so, click on the `Create new folder' icon, as
shown in the dialogue box below.

\includegraphics{imgs/create_folder.png}

{ Label the new folder `LAWS20441'. You will now have a series of
folders in your p: drive (some of these you haven't created yourself,
they have been provided for you by the university), one of which you can
store course material in. Finally, name your Word document `Trial
Document' (in the filename box) and click `Save'. As noted above, you
can remotely access your p: drive (from home or elsewhere). You can do
this by logging into your personalised university portal
(\url{https://my.manchester.ac.uk}). }

A word of caution, if your P: drive is full (and this tends to happen
when you save image or sound files to it), there is a chance that some
of the applications you want to use do not work. So make sure you keep
your P: drive tidy if you don't want to run into problems.

\hypertarget{getting-to-know-excel}{%
\section{Getting to know Excel}\label{getting-to-know-excel}}

Besides having a folder where you save something, you should also get
comfortable with the tools which you will be using throughout this
course. The main tool you'll be using is
\href{https://en.wikipedia.org/wiki/Microsoft_Excel}{Microsoft Excel}.
You will be using it to explore, learn about, and manipulate
criminological data throughout this course.

\begin{figure}
\centering
\includegraphics{imgs/bill_gates_excel.jpg}
\caption{I excel}
\end{figure}

If you are following along on your own laptop (I encourage this
strongly) you can download microsoft excel \textbf{for free}, courtesy
of the UoM library.
\href{http://www.itservices.manchester.ac.uk/students/office365/}{Follow
the instructions here to get Microsoft Office on your laptops}.

Now you likely have come across Excel before, but it's also possible
that you have not, so I will start with the assumption that this is your
first time opening it up. Exciting. So let's get to it. To open excel,
click on the windows icon, and select Microsoft Excel from the Microsoft
Office bundle.

\includegraphics{imgs/open_excel.png}

Now you should have Excel open, and you will see an empty spreadsheet.
We will be using this just a little bit later on. But for now there is
one more step we need to do, to be fully set up. We need to install the
data analysis toolpak.

To do this, click on the `File' tab, and click on `Options':

\includegraphics{imgs/file_tab_options.png}

This will bring up a popup window. Here click on `Add-Ins', then
highlight `Analysis Toolpak' and click on `Go':

\includegraphics{imgs/install_analysis_toolpak.png}

This will open another popup window. Here make sure you tick the box
next to `Analysis Toolpak' and click `OK':

\includegraphics{imgs/install_toolpak_after_go_popup.png}

Click `OK' and you should be done! You can check by clicking on the
`Data' tab, and checking to see if a little Data Analysis icon has
appeared:

\includegraphics{imgs/data_tab_analysis_appears.png}

If you are confused, see
\href{https://support.office.com/en-gb/article/Load-the-Analysis-ToolPak-6a63e598-cd6d-42e3-9317-6b40ba1a66b4}{here}
for instructions how to get this. Once you have successfully installed
the data analysis toolpak it will appear.

And that's it you are now set up! Excellent!

You can now move on to the subtantive part of today's course. In the
next section we will learn about variables and data.

\hypertarget{data-variables-and-observations}{%
\section{Data: Variables and
observations}\label{data-variables-and-observations}}

We know that in the period from May 2016 to May 2017, Greater Manchester
police recorded a total of 420228 crimes. We also know that the largest
number were recorded in Anti-social behaviour crime categeory, with
122443 instances, while the fewest in Possession of weapons, with 2009
instances.

We can also track changes in the number of crimes over time:

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-5-1.pdf}

How do we do this? Well, in the United Kingdom, since 2011 data
regarding individual police recorded crimes have been made openly
available to the public via the \href{http://www.police.uk/}{police.uk}
website
\href{http://www.tandfonline.com/doi/abs/10.1080/15230406.2014.972456}{1}.
This means that by visiting the
\href{https://data.police.uk/}{data.police.uk/} website you can access
data about street-level crime, outcome, and stop and search information,
broken down by police force. What does this mean? What do these data
look like? Let's have a look:

\begin{tabular}{r|l|l|l|r|r|l|l|l|l|l|l|l}
\hline
X & Month & Reported.by & Falls.within & Longitude & Latitude & Location & LSOA.code & LSOA.name & Crime.type & Last.outcome.category & Context & borough\\
\hline
1 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.462774 & 53.6221 & On or near Scout Road & E01012628 & Blackburn with Darwen 018D & Violence and sexual offences & Unable to prosecute suspect & NA & Blackburn with Darwen\\
\hline
2 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.464422 & 53.6125 & On or near Parking Area & E01004768 & Bolton 001A & Anti-social behaviour &  & NA & Bolton\\
\hline
3 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.464422 & 53.6125 & On or near Parking Area & E01004768 & Bolton 001A & Anti-social behaviour &  & NA & Bolton\\
\hline
\end{tabular}

In this data set each row is one crime record:

\begin{figure}
\centering
\includegraphics{imgs/rowIsObservation.png}
\caption{Each row is one observation}
\end{figure}

For every single crime event recorded in this data, there is a row, and
it contains all the information that we know about this crime incident.
It will have a value for each variable that we are interested in. The
variables are the columns.

So for example, \emph{month} is a variable in our data, and for every
row (which is every crime incident) this variable can take a value.
Every crime incident occurred at one specific month, and that month when
each incident happened will be the value that the \emph{month} variable
will take. And the month column will contain all the instances of the
month variable for each crime incident recorded. Each observation
(crime) will have a value for this variable (the month that it was
recorded).

\begin{figure}
\centering
\includegraphics{imgs/columnIsVariable.png}
\caption{Each column is a variable}
\end{figure}

Let's have a go at recording some data observations and putting them
into a database, to give you some hands on experience here.

\hypertarget{activity-1-building-your-own-data}{%
\subsection{Activity 1: Building your own
data}\label{activity-1-building-your-own-data}}

 You will get a better understanding of how data represents what you are
measuring if you have a go at building your own data set. We will do
this here by using data from twitter. You are most likely familiar with
twitter. You probably even tweet yourself. But even if you have never
used twitter, you will no doubt know someone who does. In fact, many
police forces use twitter. GMP is one of these forces, and in
particular, GMP city centre like to keep their followers updated.
Recently, the MEN had an article based on following GMP city centre's
tweets for one Saturday night.
\href{http://www.manchestereveningnews.co.uk/news/greater-manchester-news/what-police-city-centre-deal-13441129}{You
can read about that here}.

Evidently tweets present really exciting and rich data. However they do
not come in a format that is readily available for analysis in the form
that we just presented here. But what you can do is collect data from
tweets. And this is your task for your first lab activity.

I have collected for you a set of tweets. Your task is to turn this into
a rectangular data format, with the columns as variables, and the rows
as observations (tweets). Let's go through how to do this, setp-by-step.

So first things first, we need a tool. As discussed we'll mostly be
using Excel in this course. So open up excel and create a brand new
spreadsheet.

Your first activity is to create a column header for each variable we
want to collect. The easiest way to do this is just to make the first
row your column headers. You can go ahead and create a column for each
of the variables we are interested in collecting about each tweet. These
are:

\begin{itemize}
\tightlist
\item
  Month: The month in which the tweet was sent
\item
  Day: The day of the month in which the tweet was sent
\item
  Hour: The hour when the tweet was sent, in 24h format (where 13:00 is
  1pm and 01:00 is 1am)
\item
  Account: The account who tweeted this tweet
\item
  Tweet: The content of the tweet
\item
  Likes: Number of likes for this tweet
\item
  Retweets: Number of times this tweet was retweeted
\item
  Comments: Number of comments made as reply to this tweet
\end{itemize}

\includegraphics{imgs/column_headers_tweets.png}

Now you will just have to create a new row for each tweet, and populate
a value for each variable we are collecting in our data. I'll go through
the first tweet with you, so we're clear on what's happening. This is
tweet number 1:

\begin{itemize}
\tightlist
\item
  \href{https://twitter.com/GMPCityCentre/status/891900693585506304}{Tweet
  1}
\end{itemize}

You will see this open in a new window. Now let's try to find the value
for each variable in this tweet:

\begin{figure}
\centering
\includegraphics{imgs/tweet1.png}
\caption{Tweet1}
\end{figure}

\begin{itemize}
\tightlist
\item
  \emph{Month}: July
\item
  \emph{Day}: 31
\item
  \emph{Hour}: 06
\item
  \emph{Account}: GMPCityCentre
\item
  \emph{Tweet}: Man left £1000 Stella McCartney bag on seat in Village
  bar with person he had just met, and when returned, woman and his bag
  had gone
\item
  \emph{Likes}: 14
\item
  \emph{Retweets}: 43
\item
  \emph{Comments}: 20
\end{itemize}

So when you enter these values, your data will look like this:

\includegraphics{imgs/tweet1_entered.png}

Make sure that you are copy and pasting the `Tweet' variable, rather
than typing it out yourself, to save time and also ensure accuracy.

\textbf{NOTE:} It is possible that you see a slightly different time
than what I have here. This could be because you are logged into your
own twitter account, and
\href{http://www.adweek.com/digital/tweet-timestamps/}{twitter shows you
the time in your own time zone}. If this is the case, just log out of
twitter (or open an incognito window in Chrome) and it will give you the
time in Pacific Time Zone, which is the time zone they are on in
California. Now that means they are 9 hours behind us (most of the time,
sometimes this varies with daylight savings changes, but should be OK
for this exercise). So if you have this problem,

I would recommend you log in, and set your time zone to GMT
(\href{https://support.twitter.com/articles/20169405}{see how to here})
as it just makes life easier\ldots{}

OK, ready? Then let's build our data by adding the following tweets as
additional rows:

\begin{itemize}
\tightlist
\item
  \href{https://twitter.com/GMPCityCentre/status/891762454337867776}{Tweet
  2}
\item
  \href{https://twitter.com/GMPCityCentre/status/894515606321590273}{Tweet
  3}
\item
  \href{https://twitter.com/GMPCityCentre/status/894024570109386752}{Tweet
  4}
\item
  \href{https://twitter.com/GMPCityCentre/status/891247668772708352}{Tweet
  5}
\item
  \href{https://twitter.com/GMPCityCentre/status/891254643078176768}{Tweet
  6}
\item
  \href{https://twitter.com/GMPCityCentre/status/890871354924421120}{Tweet
  7}
\item
  \href{https://twitter.com/GMPCityCentre/status/890594946536927233}{Tweet
  8}
\item
  \href{https://twitter.com/GMPCityCentre/status/890161961626996736}{Tweet
  9}
\item
  \href{https://twitter.com/GMPCityCentre/status/889084990495051776}{Tweet
  10}
\end{itemize}

Once you have entered all these, you should have a pretty solid set of
tweets, looking something like this:

\includegraphics{imgs/gmp_tweets.png}

While entering your data you probably noticed that there was variation
in when the tweets were made, how much like and retweet they received,
and possibly also started interpreting the meaning of the tweet. Some
disseminate stats, for example about the number of arrests, or share
info about an operation, some appeal for information, for example about
the woman wanted for questions about the racial abuse incident, and some
are just one-off cases presented to the public.

But now, you turned the unstructured data of tweets into a structured
data set, where your observations (tweets) are the rows and the
variables you're interested in (month, day, hour, account, tweet, likes,
retweets, comments) are all columns.

Once you are done, save your data. You can do this by clicking on
``file'' and then ``save as'' and navigating to your working directory
to save your file. You can save it as a \emph{comma separated value}
file, or .csv . Next term you will be dealing with data in this format.
This way any formatting that you do to the spreadsheet (eg making the
column titles bold etc) will \emph{not} be preserved, however the data
is available to read by more software, not just excel. It doesn't hugely
matter at this stage how you save your data.

In any case, if you follow these steps, you will have a saved set of
data, in a csv file, hopefully with some meaningful name:

\includegraphics{imgs/save_tweets.png}

And now you have created your first data set. Your columns are your
variables, which correspond to Month, Day, Hour, Account, Tweet, Likes,
Retweets, and Comments. Your rows are the 10 tweets which you have
collected this information about.

\hypertarget{questions-about-your-data}{%
\section{Questions about your data}\label{questions-about-your-data}}

Why would we do this? Well turning information into data allows us to
ask questions, and draw meaningful conclusions. For example, by looking
at your newly created data set of tweets, you can easily answer the
question below:

\begin{itemize}
\tightlist
\item
  \emph{Which tweet has the highest number of likes?}
\end{itemize}

\hypertarget{activity-2-thinking-about-what-our-data-tells-us}{%
\subsection{Activity 2: Thinking about what our data tells
us}\label{activity-2-thinking-about-what-our-data-tells-us}}

Take a moment to look at your data to answer this question (\emph{Which
tweet has the highest number of likes?}). Which one is it? Read the
content, have a think, and turn to the person next to you to ask them,
why they think that this particular tweet has the highest number of
likes in the group. Now try to come up with an alternative explanation.
I would like you to now talk about your two possible explanations for
why this tweet has the highest number of likes.

\hypertarget{looking-at-real-data}{%
\section{Looking at real data}\label{looking-at-real-data}}

So the above exercise gave you an idea about how observations (in that
case tweets) can be turned into data sets where each row is one
observation, and earch column is one variable.

I demonstrated this above with the police.uk data, which is some real
data that is released about crime statistics, as I mentioned, and
something that you can see and download for yourself.

We can play around with police recorded crime data, which can be
downloaded from the \href{https://data.police.uk/data/}{police.uk}
website.

Let's stick local and download some data for crime in Manchester.

To do this, open the
\href{https://data.police.uk/data/}{data.police.uk/data} website.

\begin{itemize}
\tightlist
\item
  In \texttt{Date\ range} just select a range you want to look at. I
  selected June 2016 - June 2016, but you can choose something more
  recent if you like.
\item
  In \texttt{Force} find \texttt{Greater\ Manchester\ Police}, and tick
  the box next to it.
\item
  In \texttt{Data\ sets} tick \texttt{Include\ crime\ data}.
\item
  Finally click on \texttt{Generate\ File} button.
\end{itemize}

This will take you to a download page, where you have to click the
\texttt{Download\ now} button. This will open a dialogue to save a .zip
file. Navigate to the working directory folder you've readet and save it
there. Unzip the file, by either double clicking it, or by using right
click, and then click on ``extract''. Then open the file in excel (by
double clicking it).

You should be looking at one month worth of crime data from Greater
Manchester Police. Isn't that exciting? Real data, at your fingertips!

\hypertarget{code-books}{%
\section{Code books}\label{code-books}}

So to understand what the variables (columns) in our data mean, we
usually look for resources that can tell us about this. A reference
guide that tells you what the variables mean is usually called a
\textbf{code book}.

Creating data is a gift that keeps on giving, not just for yourself, but
for others as well. Data collected by researchers is often shared and
made available for others to use as well, so that they can explore their
own research questions. For example, the
\href{https://www.ukdataservice.ac.uk/}{UK Data Service} is a large
repository of data where you can sign up, and access secondary data to
analyse. You may have heard of the
\href{http://www.crimesurvey.co.uk/}{Crime Survey for England and Wales}
or the \href{http://content.digital.nhs.uk/catalogue/PUB17879}{Smoking,
Drinking and Drug Use among Young People Survey}. The data collected by
these surveys are online. Many many variables collected about
individuals, neighbourhoods, and other units of analysis (to be returned
to later) are available to us. Isn't that really cool!? If you want to
know, what thousands of people replied to the question asking them what
the most important issue was to them when they voted in an election, you
can find out just by downloading the correct data set!

But there is one important consideration when you are sharing a data
set, and something that is very important to you if you are using a data
set someone else has created - you need to know what the variables
\emph{mean}. This is made possible by the creation of something called a
\emph{codebook} (and sometimes called a \emph{data dictionary}). This is
a note that accompanies a data set, telling the user a bit about the
data, including what each variable means.

Have a look at the codebook for the CSEW
\href{http://doc.ukdataservice.ac.uk/doc/7889/mrdoc/excel/7889_csew_data_dictionary_2002-03_to_2014-15.xlsx}{here}
- note that this will open a force download. Just download it, save it
to your working directory, and then open it up with excel.

It will look something like this:

\includegraphics{imgs/csew_dictionary.png}

You see there is quite a bit of information provided, including the
variable name, the question that was asked, a label, which is a bit of a
description about the variable, and the possible values which the
variable can take.

This is very important for you to be able to think about what questions
you can answer with these data.

If we were to share our tweet data, we would have to create something
similar to this for that as well. Something like this perhaps:

\begin{tabular}{l|l}
\hline
Variable & Description\\
\hline
Month & The month in which the tweet was sent\\
\hline
Day & The day of the month in which the tweet was sent\\
\hline
Hour & The hour when the tweet was sent\\
\hline
\end{tabular}

Is there anything else that you would include? Why or why not? Have a
think, and if you want discuss with a friend. The important thing here
is that you understand what a codebook (or data dictionary) is, and that
if you come across a data set, always make sure to look for the
associated codebook/ data dictionary to be able to understand what each
variable means. If you download your data from the web, you will usually
find a link to the data dictionary on the site where you downloaded the
data from.

Similarly, access to anonymised crime data through
\href{www.police.uk}{police.uk} which you just downloaded, allows us to
ask questions about levels of crime in our local area, and use these
data to answer them. Access to this data allows us to study crime trends
across the UK. It allows us to answer questions that we might have -
such as, which crime category had the highest number of recorded crimes
in the last year? Or is the volume of crime increasing, decreasing, or
staying the same?

To answer these questions we need \textbf{data}. The data you can see
above, on crimes that fall under GMP between May 2016 and May 2017 can
be used to measure crime during this time period in this area. You can
access the data dictionary for these data
\href{https://data.police.uk/about/\#columns}{on the police.uk site
here}.

It is important to always seek out a data dictionary when using data, to
know what the variables represent, and if you're making your own data
set, then to create a data dictionary to let others (and even your
future self) know what your data set is all about.

\hypertarget{levels-of-measurement}{%
\section{Levels of measurement}\label{levels-of-measurement}}

\begin{quote}
The word data is the plural of the Latin datum, meaning a given, or that
which we take for granted and use as the basis of our calculations. This
meaning is carried in the French word for statistical data, données. We
ordinarily think of data as derived from measurements from a machine,
survey, census, test, rating, or questionnaire --- most frequently
numerical. In a more general sense, however, data are symbolic
representations of observations or thoughts about the world. As we have
seen, we do not need to begin with numerals to create a graphic. Text
strings, symbols, shapes, pictures, graphs themselves, can all be
graphed.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

As Leland Wilkinson points out, data can be numeric, but it can be other
things as well. Data could be text, such as the tweets. It can also be a
date, which is a special kind of number, because it has some meaning.
Pictures can also be data, as can video, or audio. You can also have
spatial data, perhaps in the form of the coordinates for where a
particular crime event took place. These are all possible sources of
data, and we could collect them as variables, or column in our data set.
In the tweets, we collected the text of the tweet, as well as the text
of the account, and the month, but also some numbers such as number of
likes, retweets etc. So we know that a variable is something that
varies, that you can note about an observation. But it's improtant to
spot, not only that variables you're using, but what \emph{type} of
variables these are. When we talk about kinds of variables, we begin to
talk about \textbf{levels of measurement}.

We can speak about the \textbf{level of measurement} of a variable,
which refers to whether that variable belongs to the category of
\emph{nominal}, \emph{ordinal}, or \emph{numeric}. Let's explore what
these categories mean.

If we set out to collect our own data, we make sure that we collect all
the variables needed to answer our question, from all the observations
that we have. The kinds of variables we have, determine the kinds of
questions that we ask. For example, if we want to ask questions such as
the one about the tweets above: ``\emph{Which tweet has the highest
number of likes?}'' we need to have \textbf{numeric} a variable.
\textbf{Numeric} variables let us answer questions about quantity. For
example, if we want to know the \emph{average number of crimes per
month}, we will need a numeric variable of number of crimes, for each
month. Just like for the tweets, we had a numeric variable of the
\emph{number of likes} for each tweet. Put simply, number questions are
answered by \textbf{numeric} variables.

\hypertarget{activity-3.1-levels-of-measurement-pt.-1}{%
\subsection{Activity 3.1: Levels of measurement pt.
1}\label{activity-3.1-levels-of-measurement-pt.-1}}

 Have a look back at your tweet data that you created. Which variables
are numeric?

Stop here again and turn to the person next to you. Discuss with them
which variables you think are numeric. Do you agree on your choices?

\textbf{NOTE:} It's important that you take some time to actually try to
think of this on your own - and I would recommend that you talk this
through with someone. If you are happy to chat to the person next to you
- do this now. If not, then raise your hand, and myself or one of the
TAs will come over. Tell us that you don't want the answer, you just
want to talk through which variables you think are numeric. We will
listen, and not judge. I'll just wait a moment while you take time to
speak to someone now.

\ldots{}

\ldots{} \ldots{}

\ldots{}

\ldots{}ready?

OK, here's your answer:

\includegraphics{imgs/num_vars.png}

So is this the same as you thought? If yes, nice work!

If you did not get this right, was that because you also selected the
\textbf{day} and the \textbf{hour} variables? If it was, then that is
perfect, because that is what I was secretly hoping you would do! But
that's not quite correct.

Why are day and hour \textbf{not} numeric variables? After all they
\emph{are} numbers, right? Well a simple way to think about that is -
does it make sense to calculate the average hour for tweets to be sent?
If I told you, the average hour for GMP tweets if 13.5, is that
something meaningful? Or if I told you that the average day is 15? Not
hugely. Hour of the day, and day of the month, which is what these
variables represent, are variables which fall into a different level of
measurement. These are \textbf{ordinal} variables. What does that mean?
Well the clue is in the name, \textbf{ordinal} variables that are not
numeric, but they do fall into a natural order.

\emph{Natural order}?? What's that? Well natural order just means that
there is a meaningful order that you can put these variables in. You
know which comes after which one. For example you can consider letters
of the alphabet to follow a natural order, so common we call it
alphabetical order. If I tell you to arrange medium, large, small, you
know that what I mean is to put them in this order: small, medium,
large. \textbf{Ordinal variables} are variables where such a known order
exists.

\hypertarget{activity-3.2-levels-of-measurement-pt.-2}{%
\subsection{Activity 3.2: Levels of measurement pt.
2}\label{activity-3.2-levels-of-measurement-pt.-2}}

{ So now you know that hour in the day, and day in the month are ordinal
variables. There are many more, such as attitudes towards something
(Strongly agree, agree, neutral, disagree, strongly disagree) or fear of
crime. There is also one more ordinal variable in our twitter data set -
can you find the other ordinal variables in your tweets data? Again take
some time to think about this. }

\ldots{} \ldots{} \ldots{} \ldots{} \ldots{} \ldots{} \ldots{}

Ready? The other ordinal variable is \emph{Month}. You know that if I
say January, February, March, then the value to follow is April, and not
November. There is an order that these values fall, making \emph{Month}
an \textbf{ordinal} variable.

So what about the others? \emph{Account} and \emph{Tweet}? These are
\textbf{nominal} variables. These are sometimes also referred to
qualitative variables. But you can still carry out quantitative analysis
on them. You will very often see \textbf{nominal} variables in
quantitative analysis. In this case, the \emph{Account} variable tells
you who is tweeting, and if you have tweets from many different
accounts, for example if we also looked at @gmptraffic and
@GMPMcrAirport, we could compare tweets between them. These variables
are \textbf{nominal} and \textbf{not} ordinal, because they do not fall
into any particular order. You can arrange them in any order, and it
would look just as legitimate as any other order. For example if I say
January, February, September, May, August \ldots{} you immediately look
and see that is not in it's natural order. However if I say @gmptraffic,
@GMPMcrAirport, @GMPCityCentre or @GMPMcrAirport, @GMPCityCentre,
@gmptraffic, you don't feel a need to reorder one way or the other.
\textbf{Nominal} variables have no natural order.

Starting to make sense? To recap, there are levels of measurement that
each variable can fall into , and these are \textbf{numeric},
\textbf{ordinal}, or \textbf{nominal}. By the way, \textbf{ordinal} and
\textbf{nominal} are also called \textbf{categorical} variables, because
they assign each observation into a \emph{category}. Then, depending on
whether the category values can be put in a meaningful order or not, you
can tell if it's an ordinal-categorical, or nominal-categorical
variable.

Confused? Let's look at this again, but with the crimes data.

Let's glance at the crimes data set first:

\begin{tabular}{r|l|l|l|r|r|l|l|l|l|l|l|l}
\hline
X & Month & Reported.by & Falls.within & Longitude & Latitude & Location & LSOA.code & LSOA.name & Crime.type & Last.outcome.category & Context & borough\\
\hline
1 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.462774 & 53.6221 & On or near Scout Road & E01012628 & Blackburn with Darwen 018D & Violence and sexual offences & Unable to prosecute suspect & NA & Blackburn with Darwen\\
\hline
2 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.464422 & 53.6125 & On or near Parking Area & E01004768 & Bolton 001A & Anti-social behaviour &  & NA & Bolton\\
\hline
3 & 2016-05 & Greater Manchester Police & Greater Manchester Police & -2.464422 & 53.6125 & On or near Parking Area & E01004768 & Bolton 001A & Anti-social behaviour &  & NA & Bolton\\
\hline
\end{tabular}

One variable you can see there is the one called \textbf{Crime.type}.
This variable can take a value that corresponds to one of the crime
types listed in the
\href{https://www.police.uk/about-this-site/faqs/\#what-do-the-crime-categories-mean}{Police.UK
FAQ}. For every crime incident recorded, an officer will have to
classify this crime incident into one of these categories. All of these
categories are all the possible \textbf{values} that the Crime.type
\textbf{variable} can take. This is a \textbf{categorical} variable, as
its possible values are categories. Further subset, this is a
\textbf{nominal} variable, because the categories do not fall into a
natural order. These categories are mutually exclusive (a crime is
classed as either a Burglary or Vehicle Crime, but not both at the same
time) and cannot be ordered in a meaningful way (alphabetical is not
meaningful!). If they did have a meaningful order (for example days of
the week have an order, or the values \emph{small, medium, large} have
an order) they would be \textbf{ordinal} variables. Both ordinal and
nominal variables are categorical, because they deal with values that
can take a finite number of values, or in other words, belong to a set
number of categories. They group your data into one of the available
categories.

We will talk in the coming weeks about creating frequency tables, where
you group your data by categories, and create a new data set, where you
have the group, and the number of observations in each group. For
example, we can look at the \textbf{numeric} variable of \emph{number of
burglaries}. For example, suppose we have created this data set, which
has 2 variables, one \emph{Borough} variable with the name of each
borough, and one \emph{Number of burglaries} variable, with\ldots{} you
guessed it\ldots{} the number of burglaries in that borough.

It would look something like this:

\begin{tabular}{l|r}
\hline
borough & number.of.burglaries\\
\hline
Manchester & 8028\\
\hline
Stockport & 3765\\
\hline
Wigan & 3525\\
\hline
Bolton & 3433\\
\hline
Rochdale & 2958\\
\hline
Oldham & 2834\\
\hline
Tameside & 2754\\
\hline
Salford & 2647\\
\hline
Bury & 2106\\
\hline
Trafford & 1795\\
\hline
\end{tabular}

This data set is made up of 10 \textbf{observations} and 2
\textbf{variables}. You might notice that this maps nicely onto your 10
rows of 2 columns. As noted in the previous section, the columns
represent your \textbf{variables}. The rows reporesent your
\textbf{observations}. Your observations (or rows) are every single
record in your data. So in the case above, every borough has one
observation, or the number of crimes in each area. For each observation,
we record 2 variables. One variable is the name of the borough. This
variable is called \emph{borough}. The other varible is the number of
burglaries that took place in that borough. It's called
\emph{number.of.burglaries}, and it is a \textbf{numeric} variable.

\textbf{Numeric} variables can also be assigned into sub groups.
\textbf{Interval} variables have values of equal intervals that mean
something. For example, if you have results from an IQ score, the
difference of 1 score between 90 and 91 is the same as 91 to 92. But
there is no \emph{true} zero value, and it doesn't make sense to say
someone is twice as smart as someone else. \textbf{Ratio} variables
however have an absolute zero (a point where none of the quality being
measured exists), and using a ratio scale permits comparisons such as
being twice as high, or one-half as much. This can get somewhat
confusing, and there are sometimes people who argue that a particular
type of variable belongs to one group or the other. For example, if you
have a Likert scale of Strongly agree, Agree, Neutral, Disagree,
Strongly disagree, you can say that this is an ordinal variable
(categories that have a natural order). But you could also translate
them into numbers, saying it measures agreement from a scale of 1
(Strongly disagree) to 5 (Strongly agree). In this case it is possible
to treat this as an interval scale variable. The truth is, you can
choose either option, \textbf{but you have to have some good
justification why}. Did someone else do this before you? Did you read a
recent paper where one method was argued to be better than the other?
For some instances it will always be clear what type of variable you
have. But you should always take time to consider what the level of
measurement of your variable is, and what that means for what you can
say about your data. As a personal preference, I'd advise against
treating ordinal data as numeric, but others will advise that it's
generally OK to take means and apply statistical tests to ordinal data,
just be careful about making interval claims such as ``twice as
satisfied.'' \href{http://www.usablestats.com/lessons/noir}{2}

See reading: Chapter 2 Statistics in Criminal Justice - David Weisburd,
Chester Britt for interval/ratio, or for discrete/continuous)

The reason we need to know what type of variable we are dealing with, is
because this will determine the kinds of analyses we can do to it,
further down the line. For example, next week we'll talk about
summarising data. As discussed above, for a numeric variable, we can
take the average, and use this to summarise it, whereas for a
categorical variable you can't.Think about if someone asked you: ``what
is the average gender in the class?'' This doesn't make sense, instead
you would look at the proportions. Gender is a categorical variable.
However, if someone asked you what is the average age in the class, that
is a more possible query to answer. Because age is a numeric variable.

Here are some more examples of each:

\begin{itemize}
\tightlist
\item
  Nominal variables:

  \begin{itemize}
  \tightlist
  \item
    Gender: Male, Female, Other.
  \item
    Hair Color: Brown, Black, Blonde, Red, Other.
  \item
    Type of living accommodation: House, Apartment, Trailer, Other.
  \item
    Religious preference: Buddhist, Mormon, Muslim, Jewish, Christian,
    Other.
  \end{itemize}
\item
  Ordinal variables:

  \begin{itemize}
  \tightlist
  \item
    Socioeconomic status: poor, middle class, rich.
  \item
    The Likert Scale: strongly disagree, disagree, neutral, agree,
    strongly agree.
  \item
    Level of Agreement: yes, maybe, no.
  \item
    Time of Day: dawn, morning, noon, afternoon, evening, night.
  \end{itemize}
\item
  Interval variables:

  \begin{itemize}
  \tightlist
  \item
    Celsius Temperature.
  \item
    Fahrenheit Temperature.
  \item
    IQ (intelligence scale).
  \item
    SAT scores.
  \end{itemize}
\item
  Ratio variables:

  \begin{itemize}
  \tightlist
  \item
    Bank account balance
  \item
    Age in years
  \item
    Height in cm
  \item
    Number of children
  \end{itemize}
\end{itemize}

Now before we move on to the exercise, have another dose of these
concepts through the power of video. Remember in school when the teacher
put on the video to watch? That was the best. Here I will do this too,
keep the nostalgia alive.

Start with this quick one: -
\href{https://www.youtube.com/watch?v=hZxnzfnt5v8}{Levels of measurement
summary here} 6.19min

and then continue by watching Chris Wilde describe them:

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/watch?v=_ROBwTFVldo\&list=PL8CRAVedURQpYNoFt7w6maxaQCn3ZLytu\&index=3}{Data
  Organisation} 5.18min
\item
  \href{https://www.youtube.com/watch?v=38oQwFeCEag\&list=PL8CRAVedURQrlxeFfme0TEgaj1_h67JUR\&index=2}{Categorical
  variables} 4.58min
\item
  \href{https://www.youtube.com/watch?v=xmRuRRHsUeg\&index=3\&list=PL8CRAVedURQrlxeFfme0TEgaj1_h67JUR}{Ordering
  categories} 2.27min
\item
  \href{https://www.youtube.com/watch?v=U3lk2nQYfAQ\&list=PL8CRAVedURQrlxeFfme0TEgaj1_h67JUR\&index=4}{Numeric
  variables} 5.52min
\end{itemize}

Great, by now you are an expert on levels of measurement.

\hypertarget{unit-of-analysis}{%
\section{Unit of analysis}\label{unit-of-analysis}}

We've been speaking (reading) about our variables (columns) a lot, but
let's also not forget to discuss the importance and meaning of our rows.
We know by now that each row is an observation. So in the original data
set about crimes, every single crime incident represents one row. Here
are 3 crimes:

\begin{tabular}{l|r|l|l|l|r|r|l|l|l|l|l|l|l}
\hline
  & X & Month & Reported.by & Falls.within & Longitude & Latitude & Location & LSOA.code & LSOA.name & Crime.type & Last.outcome.category & Context & borough\\
\hline
420226 & 420226 & 2017-05 & Greater Manchester Police & Greater Manchester Police & -2.563888 & 53.47228 & On or near Laburnum Road & E01006341 & Wigan 040D & Violence and sexual offences & Unable to prosecute suspect & NA & Wigan\\
\hline
420227 & 420227 & 2017-05 & Greater Manchester Police & Greater Manchester Police & -2.570889 & 53.46971 & On or near Elton Close & E01006347 & Wigan 040E & Burglary & Investigation complete; no suspect identified & NA & Wigan\\
\hline
420228 & 420228 & 2017-05 & Greater Manchester Police & Greater Manchester Police & -2.570947 & 53.46844 & On or near Fulwood Road & E01006347 & Wigan 040E & Other theft & Investigation complete; no suspect identified & NA & Wigan\\
\hline
\end{tabular}

But we also saw above a case where we were looking at the number of
crimes per borough. In that case, there were only 10 rows, because there
are 10 boroughs, and we only had one observation per borough. Here is
that data set again:

\begin{tabular}{l|r}
\hline
borough & number.of.burglaries\\
\hline
Manchester & 8028\\
\hline
Stockport & 3765\\
\hline
Wigan & 3525\\
\hline
Bolton & 3433\\
\hline
Rochdale & 2958\\
\hline
Oldham & 2834\\
\hline
Tameside & 2754\\
\hline
Salford & 2647\\
\hline
Bury & 2106\\
\hline
Trafford & 1795\\
\hline
\end{tabular}

What is the significance of this?

The unit of analysis is the major entity that is being analyzed in a
study. It is the \emph{what} or \emph{who} that is being studied. Your
unit of analysis will depend on the questions that you are going to be
asking. You will always want your rows to represent your unit of
analysis, so that you can collect data \emph{about} these in the
variables, and you can answer your questions.

Take this example:

We want to see whether boroughs with higher population count have higher
numbers of crimes. To be able to explore this question, we need
information about the number of crimes, and the number of the population
in each\ldots{}

\ldots{}

\ldots{}

\ldots{} borough!

What about this one:

We want to see whether men consume more illegal drugs than women. To be
able to explore this we need information about the gender and the drug
consupmtion of each\ldots{} \ldots{}

\ldots{}

\ldots{}

\ldots{} person!

Are you seeing the pattern? If you are comparing things, whether thats
population and crime, or gender and drug consumption, you are comparing
this between \emph{things}. You are comparind population and crime rates
between \emph{boroughs} and you are comparing gender and drug
consumption between \emph{people}. These are your \textbf{units of
analysis}.

\hypertarget{activity-4-abstract-ing-the-unit-of-analysis}{%
\subsection{\texorpdfstring{Activity 4: \emph{Abstract}-ing the unit of
analysis}{Activity 4: Abstract-ing the unit of analysis}}\label{activity-4-abstract-ing-the-unit-of-analysis}}

 Let's do an exercise. Read this abstract:

\begin{quote}
Over the last 40 years, the question of how crime varies across places
has gotten greater attention. At the same time, as data and computing
power have increased, the definition of a `place' has shifted farther
down the geographic cone of resolution. This has led many researchers to
consider places as small as single addresses, group of addresses, face
blocks or street blocks. Both cross-sectional and longitudinal studies
of the spatial distribution of crime have consistently found crime is
strongly concentrated at a small group of `micro' places. Recent
longitudinal studies have also revealed crime concentration across micro
places is relatively stable over time. A major question that has not
been answered in prior research is the degree of block to block
variability at this local `micro' level for all crime. To answer this
question, we examine both temporal and spatial variation in crime across
street blocks in the city of Seattle Washington. This is accomplished by
applying trajectory analysis to establish groups of places that follow
similar crime trajectories over 16 years. Then, using quantitative
spatial statistics, we establish whether streets having the same
temporal trajectory are collocated spatially or whether there is street
to street variation in the temporal patterns of crime. In a surprising
number of cases we find that individual street segments have
trajectories which are unrelated to their immediately adjacent streets.
This finding of heterogeneity suggests it may be particularly important
to examine crime trends at very local geographic levels. At a policy
level, our research reinforces the importance of initiatives like `hot
spots policing' which address specific streets within relatively small
areas.
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{https://link.springer.com/article/10.1007/s10940-009-9081-y}{Is
  it Important to Examine Crime Trends at a Local ``Micro'' Level?: A
  Longitudinal Analysis of Street to Street Variability in Crime
  Trajectories}
\end{itemize}

What is the unit of analysis here? Take a moment again, turn to the
person next to you, or if you will, get up, stretch your legs, and go
speak to someone on the other side of the room. Go! Get some steps in!
Discuss what you think the unit of analysis is, and more importantly,
why you think this! Then come back.

What did you decide on? The helpful thing here, is to look at what is
the question they are asking - and what are they asking this about? The
key sentence here is this one: \emph{``Indeed, just 86 street segments
in Seattle include one-third of crime incidents in which a juvenile was
arrested during the study period.''} You can see that they are talking
about the \emph{number of arrests} per each \emph{street segment}. So
your unit of analysis is street segments.

Want to play again?

Try this one:

\begin{quote}
This paper examines the importance of neighbourhood context in
explaining violence in London. Exploring in a new context Sampson's work
on the relationship between interdependent spatial patterns of
concentrated disadvantage and crime, we assess whether collective
efficacy (i.e.~shared expectations about norms, values and goals, as
well as the ability of members of the community to realize these goals)
mediates the potential impact on violence of neighbourhood deprivation,
residential stability and population heterogeneity. Reporting findings
from a dataset based on face-to-face interviews with 60,000 individuals
living in 4,700 London neighbourhoods, we find that collective efficacy
is negatively related to police-recorded violence. But, unlike previous
research, we find that collective efficacy does not mediate the
statistical relationship between structural characteristics of the
neighbourhood and violence. After finding that collective efficacy is
unrelated to an alternative measure of neighbourhood violence, we
discuss limitations and possible explanations for our results, before
setting out plans for further research.
\end{quote}

-\href{https://academic.oup.com/bjc/article-abstract/53/6/1050/418215}{Collective
Efficacy, Deprivation and Violence in London}

This one is a bit trciky. You can see they talk about how they collected
data, in the sentence \emph{``Reporting findings from a dataset based on
face-to-face interviews with 60,000 individuals living in 4,700 London
neighbourhoods\ldots{}''}. But remember, we want to look at the
questions they were asking - and you can see they are talking about
\textbf{neighbourhood violence}. You can see this because they talk
about looking into \emph{``statistical relationship between structural
characteristics of the neighbourhood and violence''}. Their unit of
analysis is the neighbourhood.

Of course, you could have also cheated and read the paper. It will not
always be obvious from the paper abstract what the unit of analysis is.
Unless of course, you come across a helpful abstract like this one:

\begin{quote}
Objectives: To test the generalizability of previous crime and place
trajectory analysis research on a different geographic location,
Vancouver BC, and using alternative methods. Methods: A longitudinal
analysis of a 16-year data set \textbf{using the street segment as the
unit of analysis}. We use both the group-based trajectory model and a
non-parametric cluster analysis technique termed k-means that does not
require the same degree of assumptions as the group-based trajectory
model. Results: The majority of street blocks in Vancouver evidence
stable crime trends with a minority that reveal decreasing crime trends.
The use of the k-means has a significant impact on the results of the
analysis through a reduction in the number of classes, but the
qualitative results are similar. Conclusions: The qualitative results of
previous crime and place trajectory analyses are confirmed. Though the
different trajectory analysis methods generate similar results, the
non-parametric k-means model does significantly change the results. As
such, any data set that does not satisfy the assumptions of the
group-based trajectory model should use an alternative such as k-means.
\end{quote}

-\href{https://link.springer.com/article/10.1007/s10940-014-9228-3}{Crime
and Place: A Longitudinal Examination of Street Segment Patterns in
Vancouver, BC}

But the most important thing here is that you understand what is meant
by unit of analysis. It is not always the level at which your data is
collected. For example, we have the crime data from \url{police.uk}
where each row is one measurement. This is called \textbf{individual
level} unit of analysis. But we can still use that to talk about the
number of crimes per neighbourhood. But for us to be able to do that we
need to convert that into a table where each row is the borough, we need
to aggregate up, and just count the number of crimes in each one.
Therefore this is an \textbf{aggregate level} unit of analysis.

Have a watch of this quick video
\href{https://www.youtube.com/watch?v=XHXTR8jeEUg}{here} for some more
examples and explanation.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

In sum, you should now be more familiar with data than you were when you
started. And you should be comfortable with the following terms:

\begin{itemize}
\tightlist
\item
  working directory
\item
  data
\item
  codebook/ data dictionary
\item
  variable
\item
  observation
\item
  levels of measurement

  \begin{itemize}
  \tightlist
  \item
    nominal, ordinal, numeric
  \end{itemize}
\item
  unit of analysis
\end{itemize}

From your readings you should also be comfortable with:

\begin{itemize}
\tightlist
\item
  reliability
\item
  validity
\item
  descriptive statistics
\item
  inferential statistics
\end{itemize}

\hypertarget{week2}{%
\chapter{Week 2}\label{week2}}

\hypertarget{learning-outcomes-1}{%
\section{Learning outcomes}\label{learning-outcomes-1}}

Today we are going to start summarising our variables in our data, in
order to be able to start talking about them in a meaningful way, and
begin to be able to tell a story with our data. Consider this
\href{http://researchbriefings.files.parliament.uk/documents/SN04334/SN04334.pdf}{parliament
research briefing on UK prison population statistics}. It looks at the
number and make up of people in prison in the UK. To do this, it
utilises data about people in prison, which you can imagine based on our
experience with data last week as a spreadsheet with each individual row
representing one individual prisoner. You can also imagine some columns
that contain values that correspond to each prisoner, representing a set
of variables recorded about him or her. But it would not be very
informative to just print out this spreadsheet and hand it to you - or
definitely not to hand it to policy makers who are busy, and most likely
looking for a summary of headline figures, rather than rows and rows of
data. If you did click on the link, you can see that it instead
summarises the data in a way that people can read through, and draw
meaningful conclusions from.

By reading this report, you can come to know that, at 31st March 2017,
the total prison population in England and Wales was just over 85,500.
But going further, one of the variables in the data set is the person's
gender. If we want to talk about this one variable - gender - in this
data set - prison population - we can turn to univariate analysis of
this variable. For example, we could count the number of men versus the
number of women in prison. What do you think this will tell us? Do you
think there will be equal number of men and women? If you've been paying
attention in some of your other courses, you'll likely suspect that
there are some gender differences in the prison population. So if it's
not 50-50 men and women in prisons, then what do you think the split is
like? Do you think it's 60-40? 70-30? 80-20?

Come on, take a guess, I'll hold off telling you. Speak to someone next
to you. Discuss why you think it's the split that you think it is. I'll
wait here.

\includegraphics{imgs/oitnb_dance.gif}

Ready? OK I can tell you now. Actually, according to the count of prison
population in September 2017, the number of men in prison in England and
Wales is 82,312, while the number of women is 3,982 making the split
about 95-5
(\href{https://www.gov.uk/government/statistics/prison-population-figures-2017}{see
for yourself here}). Are you surprised? I definitely was! I had no idea
the difference was this large! You can often gain valuable insight into
topics that you are interested in by looking into one variable - that is
performing univariate analysis on your data. And this is what we will
learn to do today. Excited? Yaaay

\hypertarget{terms-for-today}{%
\subsection{Terms for today:}\label{terms-for-today}}

\begin{itemize}
\tightlist
\item
  Univariate analysis
\item
  Frequency
\item
  Bar charts
\item
  Measures of central tendency
\item
  Histograms
\item
  Distributions
\item
  Measures of variance
\end{itemize}

\hypertarget{univariate-analysis}{%
\section{Univariate analysis}\label{univariate-analysis}}

So you want to analyse your variable. As you have likely pieced together
by now, that \emph{uni}variate analysis simply just means - the analysis
of \emph{one} variable. I am giving you a sneek peak into next week's
session now by telling you that \emph{bi}variate analysis means that you
are looking into the relationship between \emph{two} variables\ldots{}!
And just you wait until we get to \emph{multi}variate analysis which is
the analysis of the relatioship between \emph{more than two}
variables!!!

So just remember - uni = one, bi = two, and multi = many. That's it, no
need to count past two. We data analysts are very lazy people, you will
begin to figure this out as we go.

Right, now that we are confident with out terminology, let's think about
what we can do, in order to carry out some univariate analysis. As
mentioned, univariate analysis is the analysis of one variable. So we
know that we want to be able to talk about \emph{one} variable in our
data set. For this we will need to select a variable we want to talk
about. Often this will depend on the question being asked. So for
example, if someone asked you the question ``How many more men than
women are in prison currently in England and Wales?'' you can begin to
think about the variable you will have to analyse - perhaps the variable
of \textbf{gender}. But once you've picked your variable, how do you
analyse it? That is what today will be about.

\hypertarget{the-importance-of-level-of-measurement}{%
\subsection{The importance of level of
measurement}\label{the-importance-of-level-of-measurement}}

Well remember when we spoke about \textbf{levels of measurement} last
week? We encountered it in the lab exercise, in the reading, and in the
quiz as well. In case you need a refresher, it was the time when we
looked at the different variables in terms of whether they were
\textbf{nominal}, \textbf{ordinal}, or \textbf{numeric}. If it still
doesn't ring a bell, go back to last week's lab and \texttt{ctrl\ +\ f}
for these terms. But hopefully you will have retained some of this.
Remember we can differentiate between numeric and categorical, and then
categorical we can futher sub-divide into nominal and ordinal. Here's a
beautiful and scientific drawing to illustrate:

\includegraphics{imgs/lvl_msr_diagr.png}

So why is this important? Well what level of measurement your variable
falls into dictates what types of summaries are appropriate. Thinking
back to the gender example, it would not make huge amount of sense to
calculate the ``average gender'', would it? Gender is a nominal
variable, and as so an appropriate way to summarise it is not to
calculate an average (mean or median, but we will get into this a bit
later). Instead there are other approaches you could take. For example
you could look into the modal category - which value of the variable
occurs the most frequently? For example, in the prison population data
above, the modal category for gender was male, as this was the most
frequently occuring value for this variable. It occurred exacly 82,312
times, since there were 82,312 men in the data set. The `female' value
for the gender variable only appeared 3,982 times, since that is how
many women were observed. How do we find this out? Well in the simplest
term we could count all the occurrences of each value in the data set.
But remember what I said about analysts being lazy? We don't want to be
going throgh spreadsheets line by line. Instead, we would look at the
frequency of all the values the variable can take, in this case the
frequency of the male and female values for gender. We could do this by
something called a \textbf{frequency table}. Frequency tables are valid
ways for summarising categorical variables, however they might not be
appropriate for numeric variables, which are better suited to measures
like \textbf{average} and \textbf{variance}. But now I'm just throwing
words around.

\includegraphics{b99_hatewords.gif}

Hopefully these words sound familiar from your reading. But it might be
that their meanings are not entirely clear just yet. That's fine. Let's
do some exercises, and demistify these, so that we can get on to telling
some interesting stories with our data!

\hypertarget{summarising-categorical-data}{%
\section{Summarising categorical
data}\label{summarising-categorical-data}}

\hypertarget{activity-1-crime-types}{%
\subsection{Activity 1: Crime types}\label{activity-1-crime-types}}

{ Let's start with some exercises in talking about categorical
variables. We will do this by looking at crime data from 2016-2017
recorded by Greater Manchester Police, available for download from the
\url{police.uk} website. You don't have to go download this yourself
though, because I have put the data on blackboard for you. So just go to
blackboard, and the course content, and then downlod the file
\texttt{gmp\_crimes.csv} into your working directory. Once you have
saved it, open the file using excel. }

It should open up like this, with your \textbf{variable} names as column
headers:

\includegraphics{imgs/open_gmp_crimes.png}

Under the column headers you have your 420228 rows, one for each of the
420228 crimes in your data. Recall that these are your
\textbf{observarions}. Also, that therefore your \textbf{unit of
analysis} in this data at this moment is each individual crime.

Anyway let's say we want to talk about your variables. Like let's say
that you want to talk about the variable \textbf{crime type}.

{ What level of measurement does this variable have? Is it a category?
(hint: yes, it's in the `Summarising categorical data' section, that
sort of gives it away\ldots{}) Does it have a meaningful order? Now this
one is one to think about! What do you think? Is crime type nominal or
ordinal variable? Do you think it has a meningful order? Take a moment
to think about which one you think it is, and most importantly
\textbf{why} you think this is the case. Turn to a friend, tell them
which one you think it is and also why you think this. Then ask them
what they think, as well as their reasoning behind this. Here's a gif to
separate the answer, so you don't ruin the surprise before you have a
chance to discuss. }

\includegraphics{https://media.giphy.com/media/2wQBx10nUP3yM/giphy.gif}

So did you decide that crime type is a nominal variable? If you did nice
work! Indeed it would be very hard to find a meaningful order for the
categories in there. You could order alphabetically, but remember that
is not \emph{meaningful}. You cannot agree what comes first in the same
way that you would be able to for a scale of \emph{strongly disagree} to
\emph{strongly agree}. Therefore it is nominal. This part also does
matter, but we will return to why later.

\hypertarget{frequency-tables}{%
\subsection{Frequency tables}\label{frequency-tables}}

Okay so for now we want to find out about this variable. We know it's a
categorical variable, so if you've done your reading you will now know
that you want to be looking at a frequency table to describe it. A
\textbf{frequency table} will tell you the number of times that each
value that the variable can take appears in your data. In other words,
the frequency! Since each row is a crime incident, every time a
particular value appears in your data, it means that a crime that
belongs to that crime category occurred.

Here's an example of a frequency table. Let's say we have this data set
of waiters and waitresses who work at Lil' Bits restaurant. Here is our
data in table format:

\includegraphics{imgs/waiter_heights.png}

You can see here again that every row represents one waiter or waitress.
I've even put in a little picture of each of them, to make it more
personal. You can see them all now, forming rows of your data. You can
also see one column, for the one variable here, which is gender. For
each person we only recorded their gender, because for now, that's all
we are intersted in. We want to look at the gender of waiters and
waitresses at Lil' Bits. Maybe we think that the manager is sexist and
hires only females. Maybe we want to work out the likelihood of having a
male waiter. Whatever our motivation, we just want to know!!! We want to
know the number of men, and the number of women who work there. And it
really is as simple as that - all we do is count the occurrence of each
value of the variable, and then summarise that count in a table. In this
case, we could the number of times that we record `female' value for the
gender variable, and then the number of crimes that we record `male'
value for the gender variable, and then we say that there are 3 females
and 2 males in our data. That's it. That's a frequency table! You build
it by simply counting the number of times each value is present in the
data frame. Because if each row is an observation, then every time you
see `female' in the gender column accounts for one observation of this
value - one female waitress.

I've even made a gif to illustrate the process, something like this:

\includegraphics{imgs/freq_table_gif.gif}

I hope that illustrates the concept of what a frequency table is. It
should be very easy for you to manually count the number of men and
women working at the Lil Bits restaurant, as they only have 5 front of
house staff apparently, making this a data set of 5 rows. However in
real life you are unlikely to want to manually count each occurrence of
each value the variable can take in your data. It definitely would not
be a fun activity with the 420228 rows in your GMP crimes data.

Luckily excel makes this much easier for us.

\hypertarget{creating-a-frequency-table-in-excel}{%
\subsection{Creating a frequency table in
Excel}\label{creating-a-frequency-table-in-excel}}

Making a frequency table in excel is quite simple, and it is achieved by
using something called a \textbf{pivot table}. As far as I know this
name is specific to Excel. If you apply to public sector jobs,
especially where excel is a requirement, the word pivot table is likely
to come up in interview. It's a handy tool for summarising categorical
data. A pivot table is a tool that lets you build different types of
summary tables from your data. One of these is a frequency table.

\begin{quote}
PivotTables are a great way to summarize, analyze, explore, and present
your data, and you can create them with just a few clicks. PivotTables
are highly flexible and can be quickly adjusted depending on how you
need to display your results.
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{https://support.office.com/en-gb/article/Create-a-PivotTable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576}{The
  microsoft excel sales pitch}
\end{itemize}

If you want to go a bit further in the pivot table knowledge, here's a
handy list of
\href{https://exceljet.net/things-to-know-about-excel-pivot-tables}{23
things you should know about pivot tables}. I like it because it's a
list, and Buzzfeed has taught me that all information is best presented
in list format, preferably with a random number of items in the list,
like 23. We'll cover most of these items during the upcoming weeks.

\hypertarget{activity-2-looking-at-frequency}{%
\subsection{Activity 2: Looking at
frequency}\label{activity-2-looking-at-frequency}}

 So for now, we will now use a pivot table to create a frequency table
of the crime type variable in the GMP crimes data. To do this, go to
your gmp\_crimes data set, opened up in Excel. Download the data from
BB, as we did last week. If it's not downloading for any reason stick up
your hand, we can come around and trouble shoot this for you! Now once
you have the data open in Excel, you can easily create a frequency table
following the below steps:

First you will have to select the pivot tabel option. Click into the
\textbf{Insert} tab, click on \textbf{pivot table} and then again on
\textbf{pivot table}:

\includegraphics{imgs/click_pivot.png}

This will open a popup window, where you want to make sure that you
select `New worksheet' where it asks where your pivot table should be
placed, and then click OK:

\includegraphics{imgs/pivot_popup.png}

Don't worry too much about the top option where you select your data,
because the pivot table will let you select your variables
retrospectively. But just make sure the `Select table or range' option
is selected, and not the `use external data source one'.

Now when you click OK, excel should take you to the new worksheet where
it has set up a pivot table for you, ready to get into your data.

It might also open a toolbar on the side, but it might not do this
automatically. In any case, if the toolbar ever disappears, to summon it
you have to do one simple step, which is to click \emph{anywhere} inside
the pivot tabe area:

\includegraphics{imgs/pivot_shell.png}

Once you do that, a navigation pane should appear. Just like this:

\includegraphics{imgs/click_pivot_activate.gif}

Now you should see all your variables on the side there as well, in this
little panel that has just appeared.

You can scroll through and find crime type. This is the variable we want
to look at in this case.

You can see four windows within the pivot table panel. You've got
\textbf{Filters}, \textbf{Columns}, \textbf{Rows}, and \textbf{Values}.
You can drag your variables into these boxes in order to create a table.
Whatever you drag into the Columns box becomes the columns, and whatever
you drag into the Rows box becomes the Rows. Try it out, drag Crime type
into the Rows box. You should see a list of all the possible values that
the crime type variable can take in the rows. Now drag it over to
columns box, and you'll see it across there. Drag it back to rows and
leave it there:

\includegraphics{imgs/ct_in_rows.png}

While you see the list of possible crime types, there is no value next
to it - it is not yet a frequency table. This is where you need the
\textbf{Values} box on the pivot table toolbar. What you drag into there
determines what values will be displayed. So now grab the ``Crime type''
label from the top again, and drag down, this time to the values box,
like this:

\includegraphics{imgs/ct_in_values.gif}

Now you will see that a new column appeared with the frequency values,
letting you know the number of occurrences of each value. Or in other
words - the number of crimes for each crime type in the yeat May 2016 -
May 2017 in GMP region. Cool, no? Have a look at the resulting table.
Which crime type is the most frequent? Which one is the least? Is this
in line with what you were expecting?

We can see that the most frequent crime type in the data set is
`Anti-social behaviour'. This makes anti-social behaviour \textbf{the
modal category}. The mode is the most frequent score in our data set. It
is possible for there to be more than one mode for the same distribution
of data, (bi-modal, or multi-modal). It would be possible that there
were the same number of crimes recorded in two crime type categories.
But in this case, anti-social behaviour is the \textbf{mode}. It is the
most frequently appearing value for the crime type variable. It is the
most frequently occuring crime type.

But how much of all crimes does `Anti-social behaviour' account for?
When we are talking about your variables, we normally want to give
detail and context, so that we tell a comprehensive, and easy to
understand story with our data. We can at this stage say that the most
frequently occurring crime type (the mode) is anti-social behaviour,
with 122,443 incidents recorded by GMP. But \emph{how much} is that?
Well we can introduce another column to our pivot table, that tells us
more about the \textbf{proportion} of all crimes that each crime type
accounts for.

To do this, drag from the top, the variable Crime Type into the values
box once more:

\includegraphics{imgs/drag_perc_valu.gif}

You will see a third column appear, identical to the second one, with
the frequencies. To turn this into percent values, click on the little
downwards arrow on the yellow box of the value you just dragged into the
values box:

\includegraphics{imgs/down_arrow_perc.png}

When you click on that downwards arrow a menu will appear.

\includegraphics{imgs/value_field_settings.png}

Click on the ``Value Field Settings\ldots{}'' option, to open up a new
menu window, where you can select what you want the column to display.

\includegraphics{imgs/vfs_menu.png}

You can pick any of these, and it will turn your column of counts (you
can see that the default is set to \textbf{Count}) to whatever it is
that you selected. In this case, since we had a frequency table we are
looking at the count of each one so leave this as it is. Instead click
on the tab ``Show Values As''.

\includegraphics{imgs/perc_menu.gif}

Then click on the dropdown menu (initially it will say `No
Calculation'). Again you will see a variety of possible options to
choose. Here we want to select ``\% of Grand Total''. Don't worry about
the other options for now, we will address those next week, when we make
frequency tables with two variables. You can also rename the column
using the `Custom Name' Field. Here I change the name from `Count of
Crime.type2' to `\% of all crimes'. As we discussed last week, it's
always better to have descriptive and meaningful variable names.

Then you click OK, and ta-daa a table appears, which tells you not only
that the most frequently occurring crime type (the mode) is anti-social
behaviour, with 122,443 incidents recorded by GMP, but also that this
accounts for 29\% of all crimes recorded in this time period.

Interesting, no? Sometimes proportions can put things into perspective.
So for example, if we look at total crime, we might imagine that it's a
larger number than we had thought, and feel worried that perhaps there
is more crime in Greater Manchester than we'd anticipated. However, if
you have a look into what these crimes are, this may help interpret the
data. Robbery for example can be a very traumatic event, and is one that
makes people most fearful of crime. However you can see that
volume-wise, it makes up just over 1 per cent of all crimes. So if
robbery is what we are particularly concerned about, we can rest assured
that this is not a frequent crime, all things considered.

Does the frequency of any of these crime types surprise you? Is this
what you expected? When we speak about recorded crime in such general
terms, you have to consider that all these very diverse crime types are
included in such an umbrella term. So if you begin to hear about an
increase in crime, surely you should begin asking - increase in which
crimes? An increase in burglaries is a very different thing from an
increase in robberies, no? They would require different responses from
the police for example, and have different effect on people's
experiences of victimisation and fear of crime. Depending on which one
is driving the increase would dictate whether we need more on-street
foot patrols in robbery hotspots, or whether we need better burglar
alarms. Therefore looking into the types of crime, and their
frequencies, can lead to some very useful insight indeed.

\hypertarget{visualising-a-frequency-table-with-bar-charts}{%
\subsection{Visualising a frequency table with bar
charts}\label{visualising-a-frequency-table-with-bar-charts}}

Bar charts are a simple way of visually presenting a frequency table.
You will have definitely seen bar charts before. We will talk more about
visualisation best practice in later weeks, but for now, have a quick
glance at
\href{https://flowingdata.com/2015/08/31/bar-chart-baselines-start-at-zero/}{this
article}.

In any case, bar charts represent your data by creating a bar for every
category, and then varying the height of this bar to represent the
frequency. Imagine our frequency table turned on its side!

\includegraphics{imgs/table_on_side.png}

Now imagine that the number of crimes was represented by a bar with a
height that corresponds to the value in each cell. That is a bar chart.

So in this case, a bar chart would look something like this:

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-15-1.pdf}

I hope that you can see the resemblance between the table on its side
and the bar chart now! If unclear call us over, we'll explain! While
it's easy to just move on the next step and insert a bar chart, it's
important that you know what you are representing with it!

\hypertarget{activity-3-bar-charts-in-excel}{%
\subsection{Activity 3: Bar charts in
Excel}\label{activity-3-bar-charts-in-excel}}

OK so let's make our own bar chart in Excel. Go back to your pivot
table, that has the frequency of each crime type. Click anywhere in the
table, to highlight it all. However you might not want all the values.
You can see that on the last row we have a total column. If we're
comparing the crime types against each other it might not make sense to
also include a bar for the total. So you might want to highlight
everything except the total bar:

\includegraphics{imgs/crime_type_table.png}

When the whole pivot table is highlighted, click on the charts tab on
the top menu if you are using a mac:

\includegraphics{imgs/click_charts_tab.png}

Or the ``Insert'' tab on a PC:

\includegraphics{imgs/on_pc_insert.png}

Once you click on that you will see a whole menu of possible charts
appear. Click on the one that says `Column'. More options will appear.
Choose `Clustered Column'.

On mac:

\includegraphics{imgs/click_column.png}

On PC:

\includegraphics{imgs/pc_cluster_bar.png}

And that's it! Once you click on that, a chart will appear! Yay!

\includegraphics{imgs/chart_appears.png}

Now you can stylize your graph. First, you might want to arrange crime
type in an order from most to least frequent, rather than alphabetical
order. To do this, you must sort the data in the table.

Highlight the values in the total column, and click the data tab. Click
the little arrow next to the sort icon, and choose descending.

\includegraphics{imgs/sort_graph.png}

Note, depending on the version of excel you have and if you use PC or
Mac, it may say ``smallest to largest'' instead of ascending and
``largest to smallest'' instead of descending - but these mean the same
thing!

You can also stylize your graph, to make it look the way you like. As I
mentioned, we will go through some theory behind data visualisation, but
if you want to spend some time making your graphs nice now, then below
are some links you might find helpful:

\begin{itemize}
\tightlist
\item
  \href{https://support.office.com/en-gb/article/Change-the-layout-or-style-of-a-chart-a346e438-d22a-4540-aa87-bce9feb719cf}{Here
  is some information on how to change the layout or style of your
  graph}.
\item
  \href{http://www.upslide.net/blog/ways-to-make-beautiful-financial-charts-and-graphs-in-excel/}{8
  ways to make beautiful charts}
\end{itemize}

\hypertarget{summarising-numeric-data}{%
\section{Summarising numeric data}\label{summarising-numeric-data}}

So we saw that for categorical data the way we would carry out
univariate analysis is to produce a frequency table, identify the modal
category (the most frequent one), and we can visualise this with a bar
graph. Nice. But what about numeric variables? I've thrown some words
around, like average (mean) and median. Also spoke about the variation.
In this section we will consider these numeric summaries for numeric
variables, and also consider how we can go about visualising these as
well.

But first, to get some numeric data to summarise, let's make another
pivot table, to create a new data set, that tells me the \textbf{number}
of crimes per borough. To do this, let's create a new frequency table in
excel, this time using the `borough' variable. I will leave you on your
own to do this. You can refer back up to the steps above which we
followed to create the crime type frequency table. But instead of crime
type, this time you want to count the frequency of crimes per borough.
So in the end you should end up with a table where there are two
columns, one for `borough' and one for `number of crimes'.

Note that the column with the number of crimes in it might initially be
labelled by your pivot table as something else, for example, it could be
labelled as count of borough (as it counts the occurrence of each
borough, and might not automatically realise that each row/ observation
is one crime). So feel free to rename this column, by simply clicking in
the cell, and writing ``number of crimes''.

In this case, each row will be one borough. Your table will look like
this:

\begin{tabular}{l|r}
\hline
borough & number.of.crimes\\
\hline
Manchester & 117663\\
\hline
Wigan & 40751\\
\hline
Bolton & 40058\\
\hline
Oldham & 37073\\
\hline
Stockport & 35122\\
\hline
Rochdale & 34619\\
\hline
Tameside & 34506\\
\hline
Salford & 33115\\
\hline
Bury & 24588\\
\hline
Trafford & 22587\\
\hline
\end{tabular}

If you consider this frequency table your new data, you can see that you
have two columns, which means two variables. You have one variabe for
the name of each borough. And you have another one, that is the
\emph{number of crimes}. While the borough name is a nominal variable,
the \emph{number} of crimes is\ldots{}

\ldots{}

\ldots{}

\ldots{}

\ldots{} numeric! Yay!

So how do we talk about a numeric variable. You can imagine why a
frequency table doesn't quite make sense. A numeric variable can take
any form between two limits, the \emph{minimum} value and the
\emph{maximum} value. Because they don't map neatly into a few
categories like categorical variables, it is likely that most of them
would have a frequency value of 1. And that is not very interesting.

Dont believe me? You can give it a try. Make a frequency table of a
numeric variable, and nothing exciting will happen. See:

\begin{tabular}{r|r}
\hline
number.of.crimes & frequency\\
\hline
22587 & 1\\
\hline
24588 & 1\\
\hline
33115 & 1\\
\hline
34506 & 1\\
\hline
34619 & 1\\
\hline
35122 & 1\\
\hline
37073 & 1\\
\hline
40058 & 1\\
\hline
40751 & 1\\
\hline
117663 & 1\\
\hline
\end{tabular}

The frequency of each \textbf{number} of crimes is one. It is unlikely
that two boroughs will have exactly the same number of crimes. So it
doesn't make sense to think about numeric variables this way.

\textbf{Important note} If you are not sure \emph{why} this is the case,
or if anything about the above is confusing, \emph{raise your hand now}.
Ask us to explain this. It's not as complicated as it might sound at
first, but it's important that you understand what happens.

Right, so what's a better way to summarise numeric data? It is easier to
summarise them by looking at their \textbf{measures of central
tendencies}. This is what we'll get into in the next section.

\hypertarget{measures-of-central-tendency}{%
\subsection{Measures of central
tendency}\label{measures-of-central-tendency}}

You will often hear numeric variables summarised by the measure of
central tendency. These are the \textbf{mean} and the \textbf{median}.
You will have encountered the mean before, but possible referred to as
the \textbf{average}. In statistical language you will hear people talk
about \emph{the mean number of crimes per borough is 42008.2 crimes}.
This is the exact same thing as talking about the average number of
crimes per borough is 42008.2 crimes. And you would calculate it the
same exact same way.

To calculate the mean, you add up all your observations, and then divide
by the number of observations that you have.

So let's do this for our number of crimes per borough. We have 10
boroughs in total. You know this because you see that there are 10 rows.
Or you might just know that
\href{https://www.britannica.com/place/Greater-Manchester}{Greater
Manchester is made up of 10 metropolitan boroughs}. In any case, you
know that there are a total of 10 observations. You can denote the
\textbf{n}umber of observations with \texttt{n}. So in this case, we
know that \texttt{n=10}.

What is the total number of crimes? Well it's the sum of the
number.of.crimes column. The total number of crimes is the sum of the
crimes for each borough. In this case, the total number is:

\texttt{22587\ +\ 24588\ +\ 33115\ +\ 34506\ +\ 34619\ +\ 35122\ +\ 37073\ +\ 40058\ +\ 40751\ +\ 117663}

This number incidentally is 420082. So how do we get the mean? As I said
above, and as your readings mention, you take the sum of all the values,
and you divide by the number of observations.

You can say:

\texttt{sum(values)/n}

or in this case

\texttt{(22587\ +\ 24588\ +\ 33115\ +\ 34506\ +\ 34619\ +\ 35122\ +\ 37073\ +\ 40058\ +\ 40751\ +\ 117663)/10}

which is

\texttt{420082/10}

which is

\texttt{42008.2}.

So what is the mean number of crimes per borough? You guessed it, the
mean number of crimes per borough is 42,008.2. That means that on
average, there are about 42,000 crimes per borough. And this measure is
the mean.

Is this a good way of describing your data? Well one way to think about
it is to consider how much you distort the data if you use that
measurement to talk about it. Normally when we think about average, we
think that this is a measure that represents a value somewhere in the
middle. But if we look at this value, \emph{42,008.2}, we see that
actually this number is higher than almost all the boroughs. There is
only one borough with a number of crimes that is higher than the
average. All the other boroughs have below average crime rates. Why do
you think this is?

If you have done your readings, then you will know that this is caused
by something called an \textbf{outlier}. An outlier is an observation
that lies an abnormal distance from other values in a random sample from
a population. In a sense, this definition leaves it up to the analyst
(or a consensus process) to decide what will be considered abnormal. In
the most basic sense, an outlier can be an abnormally high or abnormally
low number of crimes per borough, when you compare it to the other
boroughs. In this case, we can see that Manchester borough has far more
crimes than any borough, with 117663 crimes. This can be considered an
\textbf{outlier}. We will talk more later about how you can determine
whether you have outliers in your data.

But take a moment here to think about why Manchester borough might be
such an outlier. It might help to look at where it is on a map, and what
sorts of areas fall within this borough, comparet with some of the other
ones, such as Trafford for example, which has the lowest number of
crimes.

\includegraphics{https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Greater_Manchester_County_\%283\%29.png/800px-Greater_Manchester_County_\%283\%29.png}

Any thoughts? Turn to the person next to you and have a chat about why
you think that we are seeing such a large number of crimes in this
borough compared to the other ones.

Now that you've had this discussion, let's get back to the problem at
hand. One of the issues with outliers is that they can skew your
results. In this case, our \textbf{outlier} borough, Manchester borough
has had a major effect on our \textbf{mean}. Because we include all the
observations, and then divide by total number, we are essentially
assuming an even distribution of crimes in each borough. When we say,
that the \textbf{mean} number of crimes per borough is 42,000 crimes, we
are saying that if you distributed these crimes equally, then that is
how many you would get in each borough. But we can clearly see that the
number of crimes are not distributed equally between boroughs, and
therefore talking about the \textbf{mean} number of crimes might not be
the best ways to summarise the data.

Luckily, this section is called \textbf{measured of central tendency}
rather than \textbf{the mean} section, because we have other options.
The other measure of central tendency, used to summarise numeric
variables is something called \textbf{the median}. The median is the
middle point of your data. It represents the value where, if you arrange
your data, sorting by your numeric variable from smallest to largest,
this value splits the data exactly in half. So 50\% of your data has
values for the numeric variable in question greater than this value, and
50\% has values that are smaller than this value. This is the value that
is right smack in the middle!

How do you calculate the median? Well, one approach is to write the
numbers in order, from smallest to largest. Then, to find the median
number:

\begin{itemize}
\tightlist
\item
  If there is an odd number of results, the median is the middle number.
\item
  If there is an even number of results, the median will be the mean of
  the two central numbers.
\end{itemize}

If you only have a few numbers, then this is feasible. Let's try this
for our number of crimes per borough again. Let's line them all up:

\texttt{22587,\ 24588,\ 33115,\ 34506,\ 34619,\ 35122,\ 37073,\ 40058,\ 40751,\ 117663}

So, we have them in order. First question: are there an even or an odd
number of values? Well, those of us with razor-sharp memories will
remember that when we were calculating the mean, we already counted the
number of values, and found a result of \texttt{n=10}. Those who don't
remember this, count the number above. Are there 10? I hope so!

So is 10 an odd or an even number? (hint: it's even). Because of this,
we know that the median will be the mean of the two central numbers.
Which are the central numbers? Well, count in 5 from the start and 5
from the end of that row, and you will identify our two middle numbers
(why 5? Well if you divide 10 by 2, to get to it's middle\ldots{}!)

Great, now we are almost there! We have identified the two middle
numbers as 34,619 and 35,122. So how do we get the mean? Scroll up if
you're not sure!

If you are sure, quick calculate it.

I'll calculate too:

\texttt{34,619\ +\ 35,122\ =} 69741

69741 \texttt{/2=} 34870.5

Woohoo! The mean of the middle two numbers, which is the median is
34870.5. And this gives us our second measure of central tendency. We
can see that this number is actually quite far off from our mean number
of crimes per borough. This means that our data is \textbf{skewed} by
our outlier! When our data is \textbf{not skewed} our mean and our
median should be the \emph{same value}. The more different they are, the
greater the skew in our data! We will talk about skew and things like
the normal distribution a bit later, and more in your next term, but you
should have a basic understanding of the difference between the mean and
the median, and what this difference means, and when each one is
appropriate to use.

If you are confused about any of this just now then let us know by
raising your hand, and asking one of us to clarify. But first, watch
\href{https://www.youtube.com/watch?v=U3lk2nQYfAQ\&amp=\&list=PL8CRAVedURQrlxeFfme0TEgaj1_h67JUR\&amp=\&index=4}{this
video by Chris Wilde} to explain this to you using some pretty nifty
visual aids. You can skip ahead and start from 2:41 if you wish. Now ask
away. We are here to help!

\hypertarget{activity-4-mean-and-median-in-excel}{%
\subsection{Activity 4: Mean and median in
excel}\label{activity-4-mean-and-median-in-excel}}

Now that you understand how the mean and median are calculated, what
they reprepsent, and what situations are best to each one of them in,
let's move on to the practicalities of how you calculate these in excel.
Of course, I keep telling you we are lazy, and we don't want to be
calculating things by hand. This is why we use Excel. So here's a quick
guide to getting the mean and the median in Excel. It's easy, but we'll
be using \textbf{formulas} again, like last week.

You'll recall, you can set a cell value to a formula by starting what
you write in there with \texttt{=}.

The formula for calulcating the mean (which is the statistical term for
the average) is simply:

\texttt{=average()}

Inside the brackets you have to enter \emph{what} it is that you want to
calulate the average of. Remember, you can either type in the cells you
want to include manually, or you can highlight by clicking on them, and
then hitting Enter.

So choose a cell where you would like your average value to appear. Type
\texttt{=AVERAGE(} and then select the cells which you want to include
in the calculations (both lower case and all caps work for this, exel
likes to shout at you, so it will translate to all caps, but it will
understand even if you type \texttt{=average(}. This should be the value
for number of crimes for each borough. Take care to \emph{not} select
the grand total in your calulcations. Then close the bracket by typing
\texttt{)} and hit enter. The value that appears is the \textbf{mean}
number of crimes for the 10 boroughs of Greater Manchester:

\includegraphics{imgs/calc_avg.gif}

Now Excel is helpful in naming its functions, and the function to
calculate the median is called\ldots{}

\ldots{}

\ldots{}

\ldots{} yes you guessed it, it's \texttt{=MEDIAN()}

Easy to remember right. So to calculate the median, follow the same
steps that you did for calculating the average, but with the median
function. The result you get should look familiar from our manual
calculation. We did the manual calculation so that you understand
exactly how we reach this number. But from now on, you can use Excel's
formulas to do all this hard work for you. Laziness prevails!

Measures of central tendency can be useful when we want to talk about
our data in a single number. Sometimes it can be helpful to know what
the average number of crimes are, or the average numer of arrests per
police force, or the average age of offenders, or the average height for
basketball players. These can tell us very qick reference values, which
we can use to describe our data. It is much more meaningful to tell
someone that the average height of basketball players is 200cm, than to
list all the heights of every person who has ever played for the NBA.
But the using a single number to summarise your data can also hide
important information. Remember the white rainbow, from the Tiger that
Isn't. If you don't, then go read this chapter from your reading. It's
actually a fun read, and also you will understand what I mean. The next
section shows you another, current example of what sort of interesting
information can be hidden by focusing only on the measures of central
tendency.

\hypertarget{distributions}{%
\subsection{Distributions}\label{distributions}}

\begin{quote}
Al Gore's new documentary is divisive. ``An Inconvenient Sequel'' is
among the most controversial and polarizing titles of the year. Because
of the politics surrounding Gore and climate change, the film divides
men and women, critics and fans, and even people who saw the movie and
people who are just rating it. But the movie's aggregate rating hides
many of those divisions, giving us a perfect case study for
understanding a big weakness of online rating systems: separating the
controversial from the mediocre. That weakness could discourage
ambitious-but-controversial work.
\end{quote}

The above is from an
\href{https://fivethirtyeight.com/features/al-gores-new-movie-exposes-the-big-flaw-in-online-movie-ratings/}{article
from the website fivethirttyeight}. It points out that the average IMDB
rating for this film, which is 5.2, actually masks what is interesting
about this film - the extent to which it polarizes people.

We spoke about the measures of central tendencies above, and how they
can be effective summaries of data, but can also mask some important
information. This is a good example of that. Let's consider 6 films from
2017 which all have an IMDB rating of 5.2. This means that the average
of all the ratings from all the people who have seen the film, and then
scored it on IMDB. These are:

\begin{itemize}
\tightlist
\item
  xXx: Return of Xander Cage
\item
  Voice from the Stone
\item
  Once Upon a Time in Venice
\item
  Phoenix Forgotten
\item
  Vengeance: A Love Story
\item
  An Inconvenient Sequel: Truth to Power
\end{itemize}

If we only know the average score on IMDB for these movies, we would
believe that they perform similarly. However we want to look at the
distribution of scores as well. And that is what the guys at
fivethirtyeight did. Have a look at these bar charts that demonstrate
the number of people who gave each star rating to each film:

\includegraphics{https://espnfivethirtyeight.files.wordpress.com/2017/09/mehtahickey-inconvenient-0831-9.png}

You can see that for the other 5 films, the ratings follow what is
essentially a normal distribution (we will return to what a ``normal
distribution'' is later). People seem to agree on these films. Very few
people think that \emph{xXx: Return of Xander Cage} is a terrible movie,
meriting a score of 1 or 2, but also very few people thing that it's
great, worthy of a 9 or a 10. Instead, most people think that it's a
mediocre film, and give it a 5 or a 6 out of 10. This pattern is
reflected in all the other films, \textbf{with the exception of Al
Gore's film}. What's going on there? Well it appears that people either
love it, giving it a score of 10, or they absolutely hate it, giving it
a score of 1. Because of this, when all the scores are added up and
divided by the total number of people who have rated the film, we get a
value in the middle, 5.2, just like we did for \emph{xXx: Return of
Xander Cage}. Except while most viewiers agree that film is mediocre,
most people are \textbf{not} evaluating \emph{An Inconvenient Sequel:
Truth to Power} as mediocre. In fact we see that most people are saying
it's great or it's terrible. And this is why distribution also matters.

What you are seeing in the histograms above are the
\textbf{distributions} of the scores that are given to each film on the
IMDB website.

It is possible to have a look at the distribution of the number of
crimes per borough as well, using a histogram.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-18-1.pdf}

So you can see here that the majority of boroughs are clustered on the
left side of the graph, with the smaller number of crimes, you can see
between over 22,000 and under 40,751 crimes. The position of each bar on
the x axis (the horizontal axis) tells you the values we are looking at,
and the height tells you how many observations fall into each value.

\hypertarget{histograms}{%
\subsection{Histograms}\label{histograms}}

This graph is called a \textbf{histogram}. While it may at first glance
resemble a bar chart, it actually isn't one. If it does't quite make
sense, have a look at \href{http://tinlizzie.org/histograms/}{this
interactive essay on histograms}. Even if you are very confident with
histograms, I would recomment that you take time to go through this
interactive tutorial. It gives you a really great, hands-on experience
in building one. The
\href{https://www.youtube.com/watch?v=U3lk2nQYfAQ\&amp=\&list=PL8CRAVedURQrlxeFfme0TEgaj1_h67JUR\&amp=\&index=4}{Chris
Wild video I linked earlier} also shows you about histograms. They are
excellent for plotting the dirstribution of numeric data. But how do
they work? Well I am really hoping that you have gone through the video
and the tutorial, but I will also just to reinforce your learning,
explain here.

Let's say we have some numeric data. We know already that we can't put
it into categories, that's why we can't build a frequency table. Every
entry would only appear the once. But what we \emph{can} do, is create
\textbf{bins} for our numeric data to fall into. Think of your numeric
variable along the horizontal x-axis. Something like this:

\includegraphics{imgs/hist_blank_x.png}

Now let's say we have some data on the number of chocolate bars that I
ate each day last week. Now I'm not great at collecting data, so I only
have data for 3 days: Monday, Wednesday, and Friday. This is my data:

\begin{tabular}{l|r}
\hline
day & num\_choco\_bars\\
\hline
Monday & 2\\
\hline
Wednesday & 3\\
\hline
Friday & 8\\
\hline
\end{tabular}

Let's say I'm interested in my \emph{numeric} variable here, the number
of chocolate bars. I don't care about which days I ate how many on, I
just want to carry out some \emph{uni}variate analysis on the numeric
variable of number of chocolate bars. I want to look at the distribution
of the numbers. I want to plot this. As I mentioned above, if I wanted
to plot this data on a histogram, I need to first split my data into
\textbf{bins}. What are \textbf{bins}? \textbf{Bins} are the result of
the action of ``bining'' the range of values. That is, you divide the
entire range of values into a series of intervals. So for example, we
can decide to bin our values into groups of fives. Something like this:

\includegraphics{imgs/hist_bins.png}

All that means is that if you look at those purple bins there, any value
between 0-5 will fall in the first one, and any value between 5-10 will
fall in the next one, and so on and so on. The bins are usually
specified as consecutive, non-overlapping intervals of a variable.

And that's all there is to it. Once you have your bins, you just count
how many values fall into each interval.

So if I were to draw this historgam for my chocolate consumption data,
then if I start with Monday, I can see I had 2 chocolate bars, and
therefore I would add one value (one observation) to the 0-5 bin. Like
this:

\includegraphics{imgs/hist_fill_1.png}

Then I look at Wednesday, and I see that I had 3 chocolate bars, adding
another value to the bin that catches values between 0-5. Like so:

\includegraphics{imgs/hist_fill_2.png}

And finally, with great shame I look at Friday, when I put away 8 whole
chocolate bars, and realise that I have to add a value to the next bin,
the one that catched values anywhere between 5-10. As such:

\includegraphics{imgs/hist_fill_3.png}

And that is exactly how you build a histogram. You could draw one by
hand if you wanted to, building it up one by one.

So now, looking back at the crimes per borough histogram, does it make
more sense?

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-20-1.pdf}

If it does, then that is excellent, and you can move on now. If it does
\textbf{not} then please raise your hand, and we will come around and
try to clarify this for you. But also make sure that before you do this
you also go through the resources above, especially
\href{http://tinlizzie.org/histograms/}{the interactive essay}.

So looking back up at our crimes per borough histogram, you can now see
that the majority of observations fall between some values relatively
close together, but there is one observation that has a very high crime
score. This is what we discussed earlier, when we were talking about
\textbf{outliers}. No surprises there. But what is interesting now, is
that you can \textbf{see the distribution} of your data, and you can see
exactly how far this outlier sits. Just like the distribution of the
IMDB scores for the Al Gore movie, a histogram of the number of crimes
per borough also tells a story.

\hypertarget{activity-5-histograms-in-excel}{%
\subsection{Activity 5: Histograms in
Excel}\label{activity-5-histograms-in-excel}}

So how do you build a histogram? Of course I won't make you draw one
manually, every time you need to build one, so let's get to excel and
make our own. This time it's slightly different than just inserting a
bar chart. Remeber that bar charts represent the frequency of a
categorical variable. Building one for a numeric variable, such as
number of crimes, would not make sense. If you are unsure why, look back
at our frequency table for the number of crimes variable. All the
frequencies are 1. So your bars would all be the same height, and we
would be none the wiser about any distribution. One important feature of
numeric variables is that \textbf{the distance between numbers is
meaningful}. Right? Remember this from the definitions for levels of
measurement? So that is why we know how far for example 117,663 is from
40,751, and how much farther it is than 40,751 is from the next number
in our data, 40,058. A histogram will display this for you.

 So to build a histogram in Excel, you will have to have your data, and
you also have to have an idea of the \textbf{bins} that you want. How do
you decide this? Well this is again one of those things where the answer
is that \emph{it depends} on your data. What are meaningful bins? What
are interesting sizes to group the numeric data into? For example, in
this case, would it make sense for a bin width of 10 crimes? If we used
a bin width of 10, we would see something like this:

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-21-1.pdf}

This is not great, because if we split everything into 10s, we are
unlikely to have more than borough fall within that range. We are
dealing with quite high numbers, right? We are dealing with tens of
thousands of crimes, rather than tens of crimes. So perhaps a more
meaningful bin width would be 10,000. Let's try that:

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-22-1.pdf}

Now that's more like it! We can see here, that most of the boroughs fall
into the bins that collect boroughs with crimes between 20,000 - 50,000,
with our outlier borough of central manchester far away on the right
there. Your bin width will always be related to your unit of
measurement. If we were talking about numer of crimes a day, this would
be a very different story, becuase we'll have smaller numbers.

So why do we need to decide bin width? Why don't we let the software
decide for us? Well because this should be a decition made by you, the
analyst. It will depend on how much variety you want to show, or how
much you want to group your observations together. For example, with a
smaller bin width of 5000 crimes, we can separate out the lower crime
boroughs, and see smaller deviations as well.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-23-1.pdf}

There is no wrong answer here, but you want to choose a bin width that
will make a histogram that will best protray the story you want to tell
with your data.

So once you've decided on your bin width, you have to tell excel what
you would like this to be. You can to this by creating a new column,
called bin, and putting your bins in there. So let's pick 10,000 for
this example. In this case you would enter the bins like this:

\includegraphics{imgs/bin_col.png}

The other thing you will need, besides to decide on some bins, is to use
the data analysis toolpak. You will have installed this in the lab
session last week. If you didn't, then refer back to the notes for last
week on how to do this.

If you have the data analysis toolpak installed, then go back to your
pivot table of number of crimes per borough, and select the ``Data''
tab. Then click on the ``Data Analysis'' icon at the top right:

\href{imgs/dat_loc.png}{}

Click on that and from the pop-up window select ``Histogram''. Then
click ``OK'':

\includegraphics{imgs/dat_pick_hist.png}

In the window that pops up, you have to populate the
\texttt{Input\ range} and the \texttt{Bin\ range} fields with the
relevant cells. Input range refers to your data. For this, click in the
box my input range, and then select the cells that contain your numeric
data. When you're done, hit enter.

Then do the same for the bin range, but select the bin range variables:

\includegraphics{imgs/build_histo.gif}

Once you have filled these fields, also click the tick-box next to the
\texttt{Chart\ output} option. When this is ticked as well, just click
``OK''. Your histogram is now ready, and should appear in a new sheet on
your excel workbook:

\includegraphics{imgs/hist_appears.png}

Right, so now we know how to get a feel for the distribution from
looking at a histogram of our numeric variable. We have also begun to
think about outliers, and what they might look like. But how do we
actually talk about distribution? We can put numbers to mean and median,
but how do we quantify distribution? The next sections will teach you
this.

\hypertarget{five-number-summary}{%
\subsection{Five-number summary}\label{five-number-summary}}

Histograms begin to tell you about the \textbf{spread} of your data.
That is - how are your data points scattered around your measures of
central tendency - your mean and your median. But sometimes you want to
put numbers to these measures. There are certain numbers you can use to
begin to talk about the \textbf{spread} of your data. These are together
called a five-number summary. So what are these 5 numbers? They are:

\begin{itemize}
\tightlist
\item
  The minimum value
\item
  The maximum value
\item
  The first quartile
\item
  The third quartile
\item
  The median
\end{itemize}

Some of these we've covered. The median we just discussed. To recap -
The median provides a model for thinking about what a typical value is.
The median is literally the value in the middle. If you rank the
boroughs from the one with the lowest number of crimes to the one with
the highest, the median would be given by the value of crimes in a
borough right in the middle of this rank. About 50\% of the boroughs
would have a crime count higher than this value given by the median and
about half would have a lower crime count. Thus, the median is also
defined as the 50\% percentile.

The minimum and the maxiumum value we touched on last week. These are
the smallest number and the largest number in your set of data. So going
back to our number of crimes per borouch, the minimum value is the
lowest number of crimes per borough (22587), and the maximum value is
the highest number of crimes per borough (22587).

One simple way of characterising how much data values vary is to look at
the \textbf{range}. The \textbf{range} is the difference between the
lowest and the highest data value in the distribution. In the table
above we can see that the lowest value is (22587), and the maximum value
is the highest number of crimes per borough (22587). There are 0
countries with a value lower than 22587 and 100\% of the countries have
a value lower than 117663. The range for the number of crimes is then
95076. Because \texttt{117,663\ -\ 22,587\ =\ 95,076}.

The \textbf{range} has the disadvantage that a single extreme value can
make it very large, giving a value that doesn't represent the data
overall. In these situations a better way to describe spread of a
variable might be to ignore the extremes and concentrate in the middle
of the data. We could look at the range of the middle half of the data.

How do you do that? Put all boroughs in a long line ordered by the
number of crimes. Then divide the cue in half at the median. Now divide
those halves in half again, cutting the data into four quarters. We call
these new dividing points quartiles. Just how the median divides your
data into the bottom 50\% and the top 50\%, the quartiles divide your
data into the top 25\%, the top 50\%, the top 75\%, and the bottom 25\%.
Imagine that your median divided your data into two new data sets. The
medians of these two new data sets are your quartiles. As always, the
clue is in the name. \emph{Quart}iles divide your data into
\emph{quarters}. What does this look like?

Well for a moment consider our data of number of crimes per borough.
Let's plot this. Here, every point represents the number of crimes in
that borogh. They are ordered by number of crimes, from least to most.

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-24-1.pdf}

We can easily identify the minimum and the maximum values, right?

And we can also quite easily draw the median:

\includegraphics{imgs/crimes_dotplot.png}

Now the quartiles are just the medians for the two halves of the data
(which were created by splitting it in half with the median). Like so:

\includegraphics{imgs/crimes_dotplot_quart.png}

One quarter of the data lies below the lower quartile, and one quarter
of the data lies above the upper quartile, so half the data lies between
them. The quartiles border the middle half of the data. The difference
between the lower and the upper quartile tells us how much territory the
middle half of the data covers and is called the interquartile range
(IQR). The lower quartile is also called the 25th percentile, 25\% of
the cases lies below it, and the upper quartile is also called the 75th
percentile, 75\% of the cases lies below it. The 1st quartile is also
called the lower quartile, and the 3rd quartile is also called the upper
quarile. So we are sometimes just saying different names for the same
thing. The 0th quartile = the minimum value (although actually no one
says 0th quartile\ldots{}) 1st quartile = the lower quartile, the 2rd
quartile = the mediad, the 3rd quartile is the upper quartile, and the
4th quartile is the maximum.

In the table with our numerical results we saw the lower quartile for
number of crimes (or the 25th percentile) is 33,463 (25\% of the
boroughs have less crimes than that) and the upper quartile (or 75\%
percentile) was 39,312 (about 25\% of the countries have more crimes
than that). The interquartile range then is 5849
(\texttt{upper\ quartile\ -\ lower\ quartile\ =\ 39312-33463\ =}5849).

Because the interquartile range is excluding everybody with the lowest
and the highest values in the distribution (we are only looking at the
difference between the lower and the upper quartile) is a measure of
spread that is less sensitive to outliers (extreme atypical very high or
very low values).

You will notice that some authors talk about the 5-number summary of a
distribution. The 5-number summary for crimes per borough would be:

\begin{itemize}
\tightlist
\item
  The minimum value: 22,587
\item
  The maximum value: 117,663
\item
  The first quartile: 33463
\item
  The third quartile: 39,312
\item
  The median: 3.48705\times 10\^{}\{4\}
\end{itemize}

These numbers are useful because you can begin to get a feel for your
data, not just in terms of a single number summary, such as the mean or
median number of crimes in a neighbourhood, but you can begin to
describe the spread of the data, and think about what that means.
Earlier I asked you to discuss with a partner about why you think that
Manchester borough is so far away from the others in terms of numbers of
crimes. I hope that your discussion had to do with populations, and that
the city centre is contained there, with it's central shopping areas,
which can act as crime attractors and generators (if you are not sure
what crime attractors/ generators are, I suggest you take some tyime to
learn about them
\href{http://www.popcenter.org/learning/60steps/index.cfm?stepNum=17}{here}).
As well as major transport hubs, and many many people passing through.
This is all interesting to know, but not something that would show up,
if you were just considering the average number of crimes per borough.

\hypertarget{activity-6-calculate-quartiles}{%
\subsection{Activity 6: Calculate
quartiles}\label{activity-6-calculate-quartiles}}

To calculate the quartile in excel, you can use the \texttt{=QUARTILE()}
function. Take a moment and read about how this works
\href{https://support.office.com/en-gb/article/QUARTILE-function-93cf8f62-60cd-4fdb-8a92-8451041e1a2a}{here}.

The idea behind this formula is that inside the brackets you have to
specify two things. First you have to specity the range from which to
calculate. You can specify this by using the drag and drop method, or
you can type it out. The other value you have to specify is the one that
tells excel which quartile you want. There are, 5 possible values, from
0 to 4. You can see what each number corresponds to in the above link.
For example, 0 means the minimum value, and 4 is the maxiumum value.
Let's here type 1, to return the value for the first quartile:

\includegraphics{imgs/quartile.gif}

You can set the last parameter of all the values, from 0 to 4, in order
to produce all the 5 numbers. So when you type 0 you get the minimum,
when you type 1 you get the 1st quartile, when you type 2 you get the
median, when you type 3 you get the 3rd quartile, and finally when you
type 4 you get the maximum value. Feel free to play around with this,
and see how it goes.

\hypertarget{standard-deviation}{%
\section{Standard deviation}\label{standard-deviation}}

Let's now discuss the standard deviation. The interquartile range is
always a reasonably summary of spread, but because it uses only the two
quartiles of the data, it ignores much of the information about how
individual values vary. Clearly 117,663 crimes is very different from
33,46 or 39,312, for example. Yet, that's not something that the
interquartile range takes into account.

A more powerful approach for measuring spread uses the \textbf{standard
deviation}. Like the mean, the standard deviation only provides a good
summary when we do not have highly skewed distributions. What is the
standard deviation? This is the most complex concept we have covered so
far and by now you may be a bit tired and overwhelmed with all we have
covered today; thus, do not worry too much if you get a bit confused. We
will come back to it also next week.

One way of thinking about spread is to examine how far each data value
is from the mean. This difference is called a deviation. If we look at
our dataset we see that Trafford had 22,587 crimes recorded in our data.
If the mean for all the boroughs is 42,008 then Trafford has a deviation
of 19,421 (= 42,008 - 22,587).

The standard deviation tries to measure what this deviation is on
average. That is, what the typical distance to the mean is for all the
cases in your data. Unfortunately, if you just sum all the deviations in
your sample and then divide them by the number of cases (if you attempt
to take the mean of the deviations), the positive and negative
differences will cancel each other out. As a result you will get ``0''.

So to ``average'' the deviations first you need to square these
deviations and only then add them up. If you then divide the resulting
value of summing up the squared deviations by the number of cases, you
will obtain the average squared deviation. This value is called the
variance.

If you want to go back to the original scale of measurement, to the
original metric, you need to take the square root of the variance
(remember the variance is the average squared deviation). The square
root of the variance is the standard deviation. It is another way of
saying that is the average distance of each observation from the mean.

Conceptually understanding the variance and the standard deviation will
possibly take you a while. We will come back to it, so don't worry too
much if you feel a bit confused right now. Make sure you read the
recommended textbooks.

So now we know that to summarise our data, we can produce the following
numbers:

\begin{itemize}
\tightlist
\item
  Mean: 42008.2
\item
  Standard deviation: 27226.27
\item
  IQR: 5849
\item
  0\%: 22587
\item
  25\%: 33462.75
\item
  50\%: 34870.5
\item
  75\%: 39311.75
\item
  100\%: 117663
\end{itemize}

The standard deviation for our data is 27226.27. But, wait a minute!
That is a bit large, isn't it? How is it that half the boroughs have a
value below 34870.5, the mean is 42008.2, and you are telling me the
standard deviation, in other words, the average distance of each
individual value to the mean is 27226.27? This makes no sense? Shouldn't
the standard deviation be smaller, given that most boroughs should be
close to the mean?

Remember what I said at the outset. The standard deviation does not
quite work when you have highly skewed distributions or outliers. When
your standard deviation is larger than the mean, it signifies that you
have a skewed distribution. The larger the difference is, the greater
the skew. In fact, the more important use of the standard deviation in
such cases is as a sign to conclude that you are dealing with a skewed
distribution. --\textgreater{}

\hypertarget{activity-7-standard-deviation-in-excel}{%
\subsection{Activity 7: Standard Deviation in
Excel}\label{activity-7-standard-deviation-in-excel}}

Finally, I will leave you with a note on how to calculate the standard
deviation in excel. Similar to how you calculated the mean and the
median, you can use the formula option to calculate the standard
deviation. The formula you need to use is \texttt{=STDEV()}, and you
apply it the same way you did the functions above, by selecting the
cells which contain the data you want to calulate, to put them inside
the brackets.

\hypertarget{writing-it-all-up}{%
\section{Writing it all up}\label{writing-it-all-up}}

This is all well and good, but what happens now that we have done all
this analysis? Well most of the time you will want to write up your
results. You will want to interpret them, of course, and you will want
to present these interpretations, as well as supporting evidence from
your analysis to share with the world. I will show a brief example here,
of how to begin thinking about your report.

The structure of your report will always depend on what is the purpose
you are writing it for. If you are writing a journal article, you will
have to conform to journal guidelines. For example, if you were hoping
to submit something to the \emph{British Journal of Criminology} you
will have to follow their
\href{https://academic.oup.com/bjc/pages/General_Instructions}{author
guidelines}. If you were writing a
\href{http://whatworks.college.police.uk/Research/Briefings/Pages/default.aspx}{What
Works Briefing} for the
\href{http://www.college.police.uk/Pages/Home.aspx}{College of
Policing}, you would follow their guidelines to make your report follow
the
\href{http://whatworks.college.police.uk/Research/Briefings/Documents/What\%20Works\%20Street\%20Lighting\%20final\%20version\%20June\%202013.pdf}{format
and structure of other reports}. For the assignment for this class, you
will also be given specific guidelines that you will have to adhere to,
both in terms of content and format. But these provide just a skeleton,
which you populate with your own research, findings, and interpretations
of your data. So while there are guidelines and templates, every report
is unique.

Here, we will build a report based on the analysis we carried out above,
to give you a bit of experience in building reports. I'll walk you
through this one, but make sure to ask about anything that might make
you uncertain, as you will be writing your own report for this week's
Task.

\hypertarget{build-a-structure}{%
\subsection{Build a structure}\label{build-a-structure}}

To start, open up a word document. Choose blank document. Before going
into details, start by giving your report a title, something like
\emph{Exploring Police Recorded Crimes in Greater Manchester Between May
2016 and May 2017}. You can choose an alternative, shorter title if you
prefer, but make sure it's descriptive.

After the title, you can set up some sections, so we know what we want
to put there. With most reports there are a few key sections to hit.

\textbf{Headline findings}

You will need to start with some headline findings, these are your top
results, the taglines for your work. If you had to get someone excited
about your results, what 3 lines would you text them?

\textbf{Introduction}

You want to introduce a background to your analysis. This can include
things like the \emph{motivation} for your research. What is the task
you are fullfilling by carrying this out? What is the research question
that you might be answering? What have other people who looked at
similar questions/ topics found? These are all important things to
include.

\textbf{Data}

You also want to quickly discuss about your data. Where does it come
from? What does it represent? We will talk more about the detail and
importance of concepts such as sampling, research design, and other
relevant issues in weeks 4 and 5. For now we will just acknowledge the
source of our data.

\textbf{Methods}

After a discussion of your background and your data, you want to mention
your methods, and why they were the appropriate choice for interpreting
your data. You want to go into enough detail so that people can
reproduce your work from reading your description here, and also justify
your choices so people know why you chose a particular approach over
another. So for example, when you are talking about the frequency of
different types of crime in the data, you would mention that you used a
frequency table, as this is the appropriate method for univariate
analysis for categorical variables.

\textbf{Findings}

Then you want to present your findings. These are the results of any
tests you perform (we'll get back to this in a few weeks), your
frequency tabes, your graphs. Your summaries, your measures of central
tendency. This is where you put all the evidence that will support your
conclusions.

\textbf{Discussion/ conslusions}

Finally you will have a discussion/ conclusion (sometimes these are two
separate sections, sometimes they can be combined, again it depends
where you are sending your report, what their expectations are.) Here
you interpret your findings. What do the numbers mean? Are they big
numbers (ref: Tiger that Isn't). What implications do the results have
for policy and practice? How do they relate to what you thought you
would find based on the background?

\textbf{References}

You will be familiar with citing your sources from writing essays. If
you need any refreshers on how to cite papers (which you will have
mentioned in your \textbf{Background} section) you can check out the
library's
\href{http://www.library.manchester.ac.uk/using-the-library/students/training-and-skills-support/my-learning-essentials/online-resources/?level=3\&level1Link=2\&level2Links=referencing,}{my
learning essentials} helpful resources for referencing. But one thing
you might \emph{not} know is that you should also cite your data. Of
course some times you will not be able to do this, for example if you
collect your own data. However if you are analysing secondary data, you
should always cite it. Read
\href{https://www.ukdataservice.ac.uk/use-data/citing-data}{this page}
on the UK Data Service website, in order to learn about how to cite
data. For a TL;DR version, when you serach for your data, you can click
on ``Item Details'' in order to get some details on the data source, and
further you can click on the Citation tab, which will open up a section
from where you can copy and paste the citation for your data set. We
will come back to this later, when we are working with UKDS data.

In any case, your report should right now look something like this:

\includegraphics{imgs/writeup_outline.png}

So what do we want to say. Well this is where your critical thinking
skills should come in. What do you think that someone would like to know
about the frequency of crimes in our data? What do you think is
interesting here? What do you think is worth menioning or highlighting?
Remember that the whole point here is to \emph{make sense of the data}.
We want to tell a story using our data, and we want that story to be
meaningful, and comprehensive. So if we are tasked with telling a story
about the frequency of each crime type, then we want that to be an
interesting story.

In this week's task we'll be building a report like this together. It
will be quite hand-hold-y, so don't worry, it's meant to guide you
through building a research report. It will feel a bit like a fill in
the blanks exercise even. When you get to your final assignment, that's
when you'll have more freedom to discuss what you wish. So at this
point, go ahead and download the task for week 2, and get started on
that.

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

In sum, you should now be more familiar with data than you were when you
started. And you should be comfortable with the following terms:

\begin{itemize}
\tightlist
\item
  univariate
\item
  frequency table
\item
  mode
\item
  pivot table
\item
  bar chart
\item
  mean
\item
  median
\item
  outlier
\item
  standard deviation
\item
  variance
\item
  histogram

  \begin{itemize}
  \tightlist
  \item
    bins
  \end{itemize}
\item
  5 number summary
\item
  quartiles
\item
  inter quartile range
\end{itemize}

\hypertarget{week3}{%
\chapter{Week 3}\label{week3}}

\hypertarget{learning-outcomes-2}{%
\section{Learning outcomes}\label{learning-outcomes-2}}

Much of statistics is about making comparisons. Human beings are not
good at sifting through large streams of data; we understand data much
better when it is summarized for us. This is true for looking for
patterns in both \emph{uni}variate and \emph{bi}variate analysis. As
discussed last week with univariate, when presenting descriptive
analysis, we often display summary statistics in one of two ways: with
tables and figures.

Tables of summary statistics are very common (we have already created
some of these last week) -- nearly all published studies in criminology
will contain a table of basic summary statistics describing their
sample. However, figures offer a visually more appealing interpretation
of our data, that allows people to easily identify trends from large
amounts of information. This is also true for exploring relationships
between two variables. In this course we will have a look at ways of
producing these visuals (and some tables) that can help you get started
in thinking about the relationships between different variables in your
data. Not only do we want to be able to summarise one variable, but we
want to know, is it related to another variable. Because this is where
the interesting questions are that we can start asking.

For example here are some criminological papers that explore the
relationship between two variables:

\begin{itemize}
\tightlist
\item
  \href{http://onlinelibrary.wiley.com/doi/10.1111/j.1745-9125.1989.tb01051.x/full}{Do
  older people have higher fear of crime?} compares the variables age
  and worry about crime
\item
  \href{http://psycnet.apa.org/record/1990-22928-001}{Is there a
  relationship between adolescent drug use and psychological health?}
  compares the variable measuring psychological health with drug use in
  adolescence
\item
  \href{https://link.springer.com/article/10.1007/s10940-009-9084-8}{Does
  design of a street affect burglary risk of the houses on it?} looks at
  the variable of design of street, and considers its relationship with
  increased burglary risk
\item
  \href{http://journals.sagepub.com/doi/abs/10.1177/1098611104271105}{Does
  ethnicity affect trust in the police?} looks at the variable of
  ethnicity, and its relationship with variable measuring trust in the
  police.
\end{itemize}

And so on and so on and so on. You will notice that most of the research
questions that criminological research attempts to address are based on
comparisons. You want to be able to explore the relationship that one
variable has with other variables. That is where exciting new insights
come from. We will talk next week about how you can go about identifying
what variables to test against each other to be able to answer your
research questions, including how to measure these variables, or even
define them in the first place. But today we will explore how to go
about assessing any \emph{bivariate} (meaning two-way) relationships -
relationships between two variables.

\hypertarget{terms-for-today-1}{%
\subsection{Terms for today}\label{terms-for-today-1}}

\begin{itemize}
\tightlist
\item
  Bivariate analysis
\item
  Categorical v categorical

  \begin{itemize}
  \tightlist
  \item
    Crosstabs with pivot tables
  \item
    Stacked column chart
  \end{itemize}
\item
  Categorical v numeric

  \begin{itemize}
  \tightlist
  \item
    Summaries by group with pivot tables
  \end{itemize}
\item
  Numeric v numeric

  \begin{itemize}
  \tightlist
  \item
    Scatterplot
  \item
    Association, strength, and form of relationship
  \end{itemize}
\end{itemize}

\hypertarget{categorical-v-categorical}{%
\section{Categorical v categorical}\label{categorical-v-categorical}}

Here's an example of a bivariate frequency table. Remember
\textbf{bivariate} just means that there are \textbf{two variables}.
Let's say we have this data set of waiters and waitresses who work at
Lil' Bits restaurant. But this time we don't just have their gender, we
also have some information about their tip earnings. We know whether
they are high earners or low earners. Here is our data in table format:

\includegraphics{imgs/waiter_height_and_tip.png}

Again, all we are doing with a frequency table is counting the number of
occurrences. Except this time we need to know the number of occurrences
of variable \emph{pairs}. So we need to know not only the number of
times that \emph{female} appears, but when the pair of \emph{female} and
\emph{high earner} appear. Again, I made a gif to illustrate:

\includegraphics{imgs/bivar_freq_table_gif.gif}

This two-way frequency table is also called a \textbf{cross table} or
\textbf{crosstab}. Also a \textbf{contingency table}. We like to give
the same thing many names, but just know, if you hear any of these
terms, people are referring to the frequency table that considers the
relationship between \textbf{two categorical variables}.

\begin{quote}
The table of counts for the various combinations of categories is a
contingency table.
\end{quote}

\begin{itemize}
\tightlist
\item
  Agresti, Alan, and Maria Kateri. ``Categorical data analysis.''
  International encyclopedia of statistical science. Springer Berlin
  Heidelberg, 2011. 206-208.
\end{itemize}

For example, a researcher might be investigating the relationship
between the class on which a passenger was travelling on the RMS Titanic
(which, in case you're not familiar, was a British passenger liner that
sank in the North Atlantic Ocean in 1912, after it collided with an
iceberg) and whether that person survived or not. The two variables
would be class (1st, 2nd, 3rd, or crew) and survived (yes/no). The
question is ``Is there a significant relationship between class of
passenger and survival?'' In this course, we don't yet learn how to
answer this question with \emph{inferential statistics}. Instead, here
we only begin to explore this question (and other questions of
comparison) using \emph{descriptive statistics}. If you are unsure about
the difference, ask now, or consult your readings from the first week.

\hypertarget{activity-1-surviving-the-titanic}{%
\subsection{Activity 1: Surviving the
Titanic}\label{activity-1-surviving-the-titanic}}

 So let's return to our question about survival in the Titanic. We want
to know the relationship between the variable for class of passenger,
and the variable for survival. First, we need some data. Data about the
fate of the RMS Titanic's passengers is actually available open data, so
we can use it to explore any questions we might have about it.
\href{http://www.public.iastate.edu/~hofmann/data/titanic.html}{You can
read a bit about the data here, and also find the data dictionary}. In
this instance though, you can download the data from blackboard. As
always it's in a folder under course content \textgreater{} week 3
\textgreater{} data for week 3. It is the one labelled ``Titanic
survivors data''. Under it you will see a link to titanic3.xls. Right
click and select ``Save link as'', and save it in your working directory
you had set up for this course.

Once you download the data open it up in excel. You should see it's the
usual dimension of your variables in your columns, and your observations
in your rows. Each observation is one passenger who was on board the
ship. The variable ``Class'' tells whether the passenger was travelling
1st class, 2nd class, 3rd class, or as a member of the crew. The
variable ``Survived'' tells whether that person has survived the RMS
Titanic's collision with the iceberg or not.

We might be interested in looking at the rate of survival by passengers
who were travelling on various class tickets. For example, we might have
seen the 1997 classic film
\href{http://www.imdb.com/title/tt0120338/}{Titanic} by James Cameron,
and might be wondering - was there a priority given to first class
passengers when boarding the lifeboats, and did that result in them
being more likely to survive? Well to be able to answer questions like
this, we would need to compare the survival (yes/no) between the class
(1st/2nd/3rd/crew) variables. Both of these are categorical, and so to
be able to talk about their relationship, we will have to build a
\textbf{crosstab}.

So to do this, we return to our trusty friend, the pivot table again.
But this time, instead of using it to summarise one variable, we can use
it to summarise the relationship between two variables. The same way
that the gif above illustrated with gender and the tips example, what
this does is it counts the frequency of the combination of each
category. But lets see how to do this.

So first just create the pivot table environment to be able to build our
pivot table. Just like last week, click into the \textbf{Insert} tab,
click on \textbf{pivot table} and then again on \textbf{pivot table}:

\includegraphics{imgs/click_pivot.png}

This will open a popup window, where you want to make sure that you
select `New worksheet' where it asks where your pivot table should be
placed, and then click OK, again just like last week. When you click OK,
excel should take you to the new worksheet where it has set up a pivot
table for you, ready to get into your data, again exactly the same as
last week. Remember, if the toolbar doesn't appear, or ever disappears,
to summon it you have to do one simple step, which is to click
\emph{anywhere} inside the pivot table area.

\includegraphics{imgs/pivot_shell.png}

Great, now you can create your two-way frequency table. First, drag the
``Class'' variable to the ``Row labels'' box, and also into the values
box. This should produce a table that looks familiar, it's a one-way
frequency table. It's what we would do if we were carrying out some
\emph{univariate} analysis on the ``Class'' variable. It should look
something like this:

\includegraphics{imgs/class_uni.png}

We can take a moment to look at the frequency table of just the class
variable. It tells us how many passengers were travelling aboard the
ship in each group. You can see there were the most people travelling as
crew, while the least populous group is the 2nd class ticket holders.

But to answer our question about the relationship between survival and
class, we don't want \emph{univariate} analysis on the ``Class''
variable. We want to compare the survival of passengers between these
classes, so we want \emph{bivariate} analysis on the ``Class'' variable
with the ``Survival'' variable.

Luckily, there is really only one more step we need to take to achieve
this, which is to introduce the Survived variable into our pivot table.
To do this, drag the ``Survived'' variable into the ``Column Labels''
box. Once you've done that, you should see your frequency table appear:

\includegraphics{imgs/crosstabl_pivot.png}

You can see that we have a frequency table that has the variable of
\emph{Class} as the rows, and the variable of \emph{Survived} across the
columns. Note that this is different from a data set, where each
variable would be a column and each row an observation. This here is a
frequency table (or cross tab, or contingency table).

Now you might notice one thing that could be bothering you? So think
back to \emph{levels of measurement}. The ``Survived'' variable is
categorical - nominal. However, the ``Class'' variable does have an
order, it's categorical - ordinal. But Excel here puts it in
alphabetical order.

You can rearrange this manually, and copy over into a new sheet.

To rearrange the order manually just click on the arrow next to
``Class'' \textgreater{} ``More sort options'' \textgreater{}
``Manual''. To open a new sheet just click on the little plus sign on
the bottom of your current sheet:

\includegraphics{imgs/new_sheet_2.png}

Then copy over the values from your pivot table, making sure that your
rows are in order, from 1st, to 2nd, to 3rd, to crew. You can also add
labels if you like. In the end you should have something like this:

\includegraphics{imgs/titanic_freq.png}

So you can see here the number of people who survived or died on the
Titanic, by the class on which they were travelling. Do you see any
interesting patterns? If you've heard of the incident, or watched that
film with Kate Winslet and Leo Dicaprio, then you might be expecting to
see more 1st class ticket holders amongst the survivors than 2nd and 3rd
class, or crew. However if we look at the number of people who survived,
is there a lot of difference? In fact it looks like more crew members
survived than did 1st class passengers\ldots{}

But what about if you look at the column that represents the number of
people who didn't survive instead? Now we see that a lot more people,
volume-wise, did not survive in the 3rd class and Crew groups. But how
can we make meaningful comparisons between these groups? How do we make
sense of this?

One thing you will be able to use, which we approached last week, is the
use of percentages, to make sense of your data. With a frequency table
of only one variable, this was easy, you just consider what percentage
of the whole, each cell represents. However with a \textbf{bivariate}
frequency table, we have \emph{three} different options for percentages.
We can consider the \textbf{row percentage}, the \textbf{column
percentage} or the \textbf{total percentage}. And all three tell us very
different things. Let's take a look.

\hypertarget{row-percentage-column-percentage-or-total-percentage}{%
\subsection{Row percentage, column percentage, or total
percentage}\label{row-percentage-column-percentage-or-total-percentage}}

Column percentages are computed by dividing the counts for an individual
cell by the total number of counts for the column. A column percent
shows the proportion of observations in each row from among those in the
column. Row percentages are computed by dividing the count for a cell by
the total sample size for that row. A row percent shows the proportion
of observations in a column category from among those in the row. Total
percentages are computed by dividing the count for a cell by the total
sample size - the grand total. A total percent shows the proportion of
all observations in you sample that match that particular row and column
combination.

Simply put:

\begin{itemize}
\tightlist
\item
  \textbf{row percentage} is the percent that each cell represents of
  the \textbf{row total}
\item
  \textbf{column percentage} is the percent that each cell represents of
  the \textbf{column total}
\item
  \textbf{total percentage} is the percent that each cell represents of
  the \textbf{grand total}
\end{itemize}

In the case of the Titanic survivors data, we might be wondering three
things:

\begin{itemize}
\tightlist
\item
  \textbf{row percentage:} what percent of the passengers in 1st class
  survived vs did not survive?
\item
  \textbf{column percentage:} what percent of the survivors were
  passengers in 1st class vs 2nd, 3rd class or Crew?
\item
  \textbf{total percentage:} what percent of all passengers were those
  who were 1st class and survived, 1st class and did not survive, 2nd
  class and survived, 2nd class and did not survive, etc etc
\end{itemize}

So to calculate each one of these, we need to know each \textbf{row
total}, each \textbf{column total}, and the \textbf{grand total}. In
this case, if we look at our data, our rows are represented by the
classes, and the columns represent the survival (or not) of passengers
(and crew).

So the row total for the 1st class row will just be the sum of all
people (both those who survived and those who did not) who had 1st class
tickets. And the row total for the 2nd class row will just be the sum of
all people (both those who survived and those who did not) who had 2nd
class tickets. And so on, and so on. Like this:

\includegraphics{imgs/titanic_row_total_calc.png}

For column totals it's the same thing, except you are calculating the
total of each column, so the total number of people who survived, by
adding up survivors in 1st class, 2nd class, 3rd class, and crew, and
the total of those who did not survive, adding up non-survivors in 1st
class, 2nd class, 3rd class, and crew. Like so:

\includegraphics{imgs/titanic_col_total_calc.png}

Summing the observations in either way give you a column total and a row
total column. If you take the sum of those (so the sum of the column
totals, that is all the people who survived and all the people who did
not, \textbf{or} the sum of the row totals, which is the sum of all
people in all the classes and crew), that gives you the grand total,
which, incidentally, is all the people who were on board the RMS
Titanic, in all classes, and whether they survived or not. In this case,
that is a total of 2201 people. You can get this from either the row
total or the column total, they will both equal the same thing, which is
\emph{all your observations}.

\includegraphics{imgs/titanic_w_total_cols.png}

\#\#\#Activity 2: Calculating row vs column percentages

 You can calculate these yourself, in the excel sheet you have
downloaded from blackboard, using the \texttt{=SUM()} function, the way
that we did this in the previous session.

So now that we have these totals, we can calculate our percentages.

To get your row percentage, you have to take each cell and divide it by
the row total (and then times by 100 to get the percent value). You can
do this in a new column, creating a new column for survived - Yes \% and
No \% :

\includegraphics{imgs/surv_perc_cols.png}

Now you can enter the calculation as a formula for each row. Remember
for something to be a formula, you will start with \texttt{=}, and then
follow with your equation. Here the equation will be:

\texttt{cell\ divided\ by\ the\ total,\ times\ 100}

\texttt{cell/total*100}

So for example, for the Yes percent column for first class passengers,
you have to find the cell reference for the cell that represents the
number of people who survived and were 1st class ticket holders, and
divide by the cell reference of the total number of first class ticket
holders (your row total for this row). You can do this by typing in the
reference for each cell (so typing out \texttt{C3} and \texttt{E3}) or
you can do it by highlighting as well. If you're not sure how to do the
highlighting approach, raise your hand now, we will come to help!

But in any case, for the cell that represents the row percentage for the
yes survived and first class combination, you should have the below
formula:

\texttt{=C3/E3*100}

Once you type this, you will see the percent value appear for the
percent of all first class passengers who survived:

\includegraphics{imgs/perc_surv_value.png}

You will see that in the formula bar at the top, you see the formula you
typed, but in the cell you see the value appear. In this case we can see
that about 62.5\% of first class passengers survived the sinking of the
RMS Titanic. Let's calculate the values for the other cells as well:

\includegraphics{imgs/all_row_perc.png}

Row percentages allow you to talk about the percentage of each value in
the variable that's displayed along the rows, in terms of the outcomes
displayed across the columns. What does that mean? Essentially, you can
talk about the percent of each class that belong to each survival
outcome. So you use row percentages to say: 62.5\% of those in first
class survived, but only 25\% of third class passengers did. This is the
kind of stuff we can say with row percentages. You get all sorts of
better insight, than just talking about the number of passengers who
survived. There were 203 1st class survivors, and 178 3rd class, however
this doesn't seem to be that much of a difference. But once you take
into account, how many \emph{more} 3rd class passengers were than 1st
class, you can see that actually it does make a huge difference, as
percentage wise, many more 1st class passengers survived. Turning your
numbers into percentages tells you these kinds of things. Isn't that
exciting? Well, not for Leo\ldots{}

\includegraphics{https://media.giphy.com/media/sbtEb9csrlOda/giphy.gif}

But what about column percentages? What do those tell us? Well just as
row percentages tell you about the percent of the \emph{row} values,
distributed across the categories of the \emph{columns}, column
percentages tell you about the percent of the \emph{column} values,
distributed across the categories of the \emph{rows}. In this case, the
column percentages would tell us what percent of the survivors were 1st,
2nd, 3rd class or crew. Similarly it can also tell us what percent of
the non-survivors were 1st, 2nd, 3rd class or crew. This is a
\emph{slightly} different story to what the row percent says.

So why is that? Well let's look at the numbers to illustrate. To
calculate the column percentages, you have to do the mirror image of
what we did for row percentages, just create some new rows, one
percentage equivalent for each class, so 1st \%, 2nd \%, 3rd \% and crew
\%, and again for each one, populate it with a formula starting with the
\texttt{=} sign, then the cell of the matching value (so C3 for 1st
class survived) divided this time by the column total (C7), and again
times by 100. As such:

\includegraphics{imgs/col_perc_calc.png}

Repeat the same for all cells (remember how you can copy formulas by
clicking on the bottom right corner of the cell there, and dragging? No?
Raise your hand now to ask about it! It saves time, I swear\ldots{}!)

Then you will end up with some results like this:

\includegraphics{imgs/all_col_perc.png}

So what does that tell you? Well this tells you about the \% of
Survivors who were travelling in each class. So we can now see that of
all the survivors, 28.6\% were 1st class, 16.6\% were 2nd class, 25\%
were 3rd class, and 30\% were crew. This doesn't really illustrate any
huge disproportionality, does it? Well again, that's because it doesn't
compare to those who died. Instead it just looks at the distribution of
the survivors between those travelling in different class. And this is
why it's really important for you to consider - what is the best way of
presenting the data, that considers all angles, and presents the most
truthful story? There is a popular book used to teach statistics called
\href{https://en.wikipedia.org/wiki/How_to_Lie_with_Statistics}{How to
Lie With Statistics}. This is a good point to illustrate again, how
important it is for you to understand how to make sense of data, and how
to draw meaning from it, in order to be able to scrutinize what stories
people may tell you. Depending on what results would be presented from
this analysis, we could easily write a different headline.

Consider this:

\textbf{Crew save themselves before passengers - 30\% of the survivors
were Crew members, with 1st class passengers lagging behind at 28.6\%}

The numbers in this headline are all correct. It is true that 30\% of
the survivors were crew members. But it doesn't take into account the
original number of crew members present, from which the survivors could
be selected. A very different headline would be:

\textbf{Rich leave the poor to sink into icy ocean - 62.5\% of 1st class
passengers survive, compared with 41.4\% of 2nd class, 25.2\% 3rd class,
and only 24\% of crew members who made it out}

Very different conclusions, no? All from the same data. The numbers are
correct, but they are framed very very differently. This is why it is so
important to report all your findings, including the right statistics,
as well as to be able to scrutinize other reports as well. To the first
statement you might want to pose the question - OK but what percentage
of the non-survivors were crew members? You would receive the figure of
45\%, immediately indicating that they would be the largest group
represented in the fatalities as well. Then you would begin to realise
that the reason they might be a large proportion of the survivors, is
because they made up such a large proportion of anyone on board, in the
first place. There was simply more of them present! But when you
consider their survival rate (which is illustrated better by the row
percentages) you gain some insight into the inequalities. And our
qualitative information from the movie Titanic further supports this
finding, that we need to consider the proportion who did not make it out
as well:

\includegraphics{https://media.giphy.com/media/bQw045yld4nra/giphy.gif}

I hope that illustrated a bit how you can calculate your row and column
percentages, as well as what they mean, and how they help you extract
meaning from your data. Usually you will only display \emph{either} row
percentages \emph{or} column percentages, \emph{not both}. You will have
to choose which one you think is most appropriate for telling your
story.

But what about total percentages?!?! The truth is, you will very rarely
use these. Total percentages tell you what proportion of all your
passengers were 1t class and survivors, or 2nd class and survivors, and
so on and so on. They are not frequently used to show relationships
between variables. If you want to calculate them, you just have to
divide each cell by the grand total (all the passengers) and times that
by 100. You can go ahead and try if you want. But you won't really get a
lot of insight from it, I have to tell you\ldots{} So we won't even
bother with them any more.

So which one you use (row or column) is dictated by the question you
ask. There's also this youtube video (made for someone called Michelle
apparently\ldots{} I'm not sure the backstory here, but it sounds like
our video creator is excited for Michelle to get back from Spring Break,
and talk about column vs.~row percentages. But in any case, I'm sure
that since it's on YouTube, the rest of us can use it\ldots{}) where the
video's creator switches between row and column percents, this can
further illustrate how it makes a difference which one you use:
\href{https://www.youtube.com/watch?v=cdvTpnHwKjs}{you can watch the
video here}.

If you're still unsure about these let us know, raise your hand, and we
will come to help!

\hypertarget{a-note-on-formatting-cells}{%
\subsection{A note on formatting
cells}\label{a-note-on-formatting-cells}}

Just before we move on, I quickly want to take a side step and talk
about formatting of the cells. You can see that here we see our
percentages appear to very accurate precision, displayed up to 7 decimal
places. This isn't always necessary. Do you think it makes a big
difference to someone if you say ``32\% of all Titanic passengers
survived'' or if you say ``32.303498\% of all Titanic passengers
survived'', in terms of their understanding of what that means? In some
cases it might be important to retain such precision. But often, when
talking about people, (or number of crimes) it doesn't necessarily need
to be so specific. So what if you wanted to format your results?

Well you can always round your numbers manually. But as always, there is
a way you can just do this using excel. To do this, highlight all the
cells with the percentages inside. When you have done this, right-click
anywhere inside this area that you have just highlighted. A menu of
options will appear. Select the option to ``Format Cells\ldots{}''

\includegraphics{imgs/format_cells.png}

This will bring up another window, where on the left hand side you will
see a list of possible types of data that your cell could contain. Most
likely this will be set to ``General'' which is a pretty meaningless
category. Instead, choose the ``Number'' option, as your cells contain
numbers (percentages). When you click on the ``Number'' option, you will
see a text box, that says ``Decimal places:'' in front of it.

\includegraphics{imgs/num_dec_set.png}

Here you can enter the number of decimal places you want to display.
Change it to ``1''. Then click OK. Once you've done that, your
percentages should now be displayed with only one decimal point. A bit
less noise, don't you agree?

\includegraphics{imgs/one_dec_row_perc.png}

\hypertarget{activity-3-visualising-the-relationship---stacked-bar-charts-and-conditional-formatting}{%
\subsection{Activity 3: Visualising the relationship - stacked bar
charts, and conditional
formatting}\label{activity-3-visualising-the-relationship---stacked-bar-charts-and-conditional-formatting}}

So last week we visualised frequency tables using a bar chart. This week
we have two variables to visualise. Luckily we can still do this, but
with a stacked bar chart. What is this? Well this time, the bar chart
will have a consistent height (100\%) but will be shaded according to
which percentage each category takes up. Let's illustrate to make this
more clear.

 We've decided that row percentages are the more meaningful of the two
here for us, so let's go ahead, and highlight the cells where we have
our row percentages. Once this is highlighted, select the stacked bar
chart.

\includegraphics{imgs/stacked_col.png}

Once you click on that, our chart will appear:

\includegraphics{imgs/stacked_col_2.png}

This looks about right, however you can see that our axes are not
properly labelled. Each category is called 1, 2, 3, and 4, instead of
1st class, 2nd class, 3rd class, and crew. So to fix this, you can right
click anywhere in the chart area, and as the little window of options
appears, click on ``Select data\ldots{}'':

\includegraphics{imgs/stacked_col_3.png}

This will open up another pop up window, where you should see the
following:

\includegraphics{imgs/stacked_col_4.png}

Above the right-hand box, you can see a label telling you that this is
to do with the ``Horizontal (Category) Axis Labels. You can click on
the''Edit" button under that which will open a selection window. Select
the row labels by clicking and dragging to highlight them all:

\includegraphics{imgs/add_labels.gif}

Once you hit enter it will populate, and you can click on ``OK'' to
update your graph. You will see it updated hopefully looking like the
graph below:

\includegraphics{imgs/stacked_col_5.png}

you might have noticed there was an option for stacked bad with
percentage next to it. This is quite handy if you have not computed row
percentages. Excel will go ahead and do this for you in graph form. If
you want to give this a go, go back and this time, highlight the count
of the people who survived, not your percentage calculations. Now when
you've highlighted that this time select the percentage graph bar
charts:

\includegraphics{imgs/stacked_col_perc.png}

When your new graph appears, you will see that it looks exactly the same
as when you created the stacked chart from your calculations. Exciting!

\includegraphics{imgs/stacked_col_perc_2.png}

You might be wondering - how does excel know that you want the row
percentages, and not the column percentages, when you are creating your
stacked percentage bar graph just from the counts? Well, tradition holds
in data analysis that you should arrange your data in a way that the row
percentages are the meaningful ones. Think about it, if I wrote the
table with the survival in the rows, and the class in the columns, that
still makes sense - it is still the same data, right? But then what is
represented by column and by row percentages gets switched. The meaning
is the same, but they are just calculated into different places.

Another way to visually display the differences in your data, while
still presenting the numbers in a table, is to use conditional
formatting. To do this, select again the cells with your percentage
values in it:

\includegraphics{imgs/cond_form.png}

This time in the ``Home'' tab, you will see a little sign that says
conditional formatting. Click on the little arrow next to it (with your
data still highlighted):

\includegraphics{imgs/cond_form_2.png}

Hover over ``colour scales'' and pick a colour scale that you think is
appropriate. Often you will hear people talk about a ``RAG'' rating
(red, amber, green). So you might want to choose a red-amber-green
colour scale. However, you might want to consider that some people have
red-green colour blindness. To make your representation accessible to
people with this particular condition, you might actually want to use a
scale that goes from red to blue. Also you want to decide what is
coloured red. You can see the same colour scales on there twice, once
with red at the top, and once flipped around with red at the bottom.
Which one you choose is dependent on the \emph{meaning} of your colour
scale. Is a high number a good thing or a bad thing? In this case this
changes between our two columns. However if we were colour-coding the
number of crimes per borough for example, a table we created last week,
then red would be better suited to illustrate higher numbers (so we
would pick the scales with the red on top) to indicate more crimes in an
area. You can also choose a neutral scale, going from yellow to green
for example and so on. The choice is yours, you have options!

Here's mine anyway:

\includegraphics{imgs/cond_form_3.png}

You can see that the yes \% is quite a high value, an appears with an
orange colour for the 1st class passengers, but for all others, it is
their no \% which contains the higher numbers, with dark red especially
for 3rd class and crew members. Using conditional formatting in this way
can help draw out patters, that further emphasise the story that the
numbers in your crosstab are telling. They can be useful in reports
(just make sure that whoever will be reading them will be reading on
screen, or has access to a colour printer!).

\hypertarget{categorical-v-numeric}{%
\section{Categorical v numeric}\label{categorical-v-numeric}}

The Arrestee Survey, 2003-2006, was the first nationally representative
survey of drugs and crime among the population of individuals
representing arrest events in England and Wales. The survey aimed to
provide information on a range of areas within the drugs and crime
nexus, including the prevalence of problematic drug misuse among
respondents representing arrest events; drug and/or alcohol consumption;
availability of drugs; levels of demand (met and/or unmet) for drug and
alcohol treatment services among respondents; levels of intravenous drug
use among respondents; and gang membership. Topics covered include:
demographic characteristics; arrest, prison history and past contact
with CJS; offending and offence categories; drug and alcohol use; drug
purchasing and availability; drug and alcohol treatment needs; treatment
offered and received; and gang involvement. Some of the above questions
were answered by self-completion questionnaire, and an oral fluid
(saliva) sample was also taken.

If you wanted to have a look at this data set, you can access it through
the UK data service website
\href{https://discover.ukdataservice.ac.uk/catalogue/?sn=5807}{here}.
However to make things easier, I have selected just a few variables for
you here, and uploaded this subset onto blackboard. So go now to
blackboard (course content \textgreater{} week 3 \textgreater{} data for
week 3) and download the arrestees\_subset file. Once downloaded onto
your PC, open it up with excel.

So the first thing we want to do is have a look at our variables. We
have 5 variables, which are:

\begin{itemize}
\tightlist
\item
  \textbf{Interview reference index:} This is just a reference number to
  identify each person interviewed. For anonymity reasons, a number is
  used to identify them, rather than their names\\
\item
  \textbf{Age:} Age of the person being interviewed, at the time of
  interview
\item
  \textbf{Age at first arrest:} The age at which they person had their
  first arrest\\
\item
  \textbf{Number of times arrested:} The number of times this person has
  been arrested, up to this point.
\item
  \textbf{Reason for arrest:} The reason given for this particular
  arrest
\end{itemize}

In the following questions, we will explore the relationship that age
has with offending. In the first instance, we might want to find out the
age profile for different offences. Do you think there would be a
difference between the different offences that people are likely to get
arrested for?

For example
\href{http://www.tandfonline.com/doi/abs/10.1080/10683160512331316343}{joy
riding is a crime traditionally associated with younger people}.
Joyriding is associated with `theft of vehicle' which appears in our
data as a possible value for the ``Reason for Arrest'' variable. So we
might expect for the distribution for age (a numeric variable) to be
different for those arrested for `theft of vehicle' than a crime that
might be associated with an older demographic. Any ideas what crime
types those could be?

\hypertarget{activity-4-age-of-offenders}{%
\subsection{Activity 4: Age of
offenders}\label{activity-4-age-of-offenders}}

 Let's have a look at just the possible values that the ``Reason for
Arrest'' variable. To do this, let's start building a pivot table once
again. You should be pretty comfortable with this by now. Make sure you
are clicked anywhere on a cell that's part of our data set, and then go
to Insert (or Data on a mac) and choose pivot table:

\includegraphics{imgs/click_pivot.png}

Now take the variable `reason for arrest', and drag it to the ``Row
Labels'' box. You should see all the values for the ``Reason for
arrest'' variable :

\includegraphics{imgs/reason_arr_values.png}

You can see here a list of all the possible values that the ``Reason for
arrest'' variable can take. Let's take some time to think about these
categories. We've already discussed joyriding as something that is
generally considered a crime to be committed more often by younger
people. What about the other reasons above. Are there any other ones
that you think would have a generally younger demographics? Let's take
some time to guess first, just to test your possible understanding, and
then check our assumptions with data. After all, that's what this course
is about, right?

So, answer the following questions for me, without peaking ahead, or
checking the data, just in terms of your perceptions of the relationship
between age and types of offences committed:

\begin{itemize}
\tightlist
\item
  Which reason for arrest do you think has the oldest arrestee?
\item
  Which reason for arrest for you think has the highest average age of
  arrestees?
\item
  Which reason for arrest for you think has the lowest average age of
  arrestees?
\end{itemize}

Take some time to think about this.

So now that you've hopefully chosen some categories that you think would
fit the image of younger/ older age profiles, let's have a look at what
the data says. In order to be able to compare a numeric variable across
values of a categorical variable, we can consider different summaries
across each value. Remember last week when we looked into summaries of
numeric variables when considering \emph{uni}variate analysis? Well now
we do the same, create these summaries for the numeric variables, but we
do this \emph{for each value of the categorical variable}. So we can
consider the minimum, the maxiumum, the mean, the median, and the
standard deviation for age across each one of the values given for
`reason for arrest'. Let's have a look on how to do this.

On our pivot table panel, grab the age variable, and drag it into the
``Values'' box. When you let it go, your table should look something
like this:

\includegraphics{imgs/age_to_value.png}

Have a look at this table. What does it tell you? It looks a bit strange
no, these are not really numbers that make sense when we are talking
about people's ages\ldots{} What does 9882 mean when it comes to `Theft
of vehicle'? Well if we look back at our ``Values'' box in the pivot
table panel it gives us an indication as to what's going on. Since we
didn't specify how we want the age variable summarised, Excel has
decided to select the ``sum'' function for us. So what we are seeing
here is essentially a sum of all the ages for all arrests made for
reason of ``theft of vehicle''. This is very hard to make sense of, and
instead we would prefer to see some of our summaries that we discussed
above, as well as last week when we were performing univariate analysis.
So how do we do this?

Well on the right hand side of the item ``Sum of Age'' in the Values box
there is a little letter ``i''. This is on a mac so on a PC it might be
a downwards arrow. Whatever it is, click on it, and a new popup window
will appear.

\includegraphics{imgs/change_from_sum.png}

If you're on a PC, when you click on the downward arrow this set of
options will appear:

\includegraphics{imgs/windows_pivot_change_sum.png}

In this case select ``Value Field Settings\ldots{}'' - this is the same
as the steps we followed last week to add the column of percentages for
out univariate frequency table. Then the popup that appears will look
roughly the same, with the option to change the Field Name (as ``Custom
Name'') and the Summarize by (as ``Summarize Values By'').

In the new popup window, you can see that you have a list of options in
a window under the description that says ``Summarize by''. You can see
that initially (by default) this is set to sum:

\includegraphics{imgs/set_to_sum.png}

Change this to select ``Average'', like so:

\includegraphics{imgs/set_to_avg.png}

Click OK, and you will see your column updated:

\includegraphics{imgs/is_now_avg_col.png}

So now we have a column that tells us the average age of people arrested
for each value of the variable ``Reason for arrest''. So which one has
the highest average age? It appears to be ``Sex Offence'' with an
average age of 35 years old for people arrested for this reason. Is this
what you were expecting? Did you choose something different? What is the
average age for the value for reason of offence that you thought would
be oldest? Were you far off? Take a moment to chat about this with the
person next to you. Hopefully you might find these results interesting,
and therefore are getting some insight into offender demographics for
various crime types. Our youngest average age of offenders does appear
to be in the theft of vehicle category.

\includegraphics{https://i0.wp.com/thehappihippi.com/wp-content/uploads/2016/01/tumblr_nk2z5noRCH1t0093do1_540.gif}

But remember our first question - \emph{Which reason for arrest do you
think has the oldest arrestee}? Well this question is not answered by
the average age column here. And also, we discussed last week why the
mean alone is not always the best summary of our data. We might want to
know about the spread, and the variance as well. So let's add a few more
columns to our summary statistics table here. To do this, just drag
``Age'' to the ``Values'' box again and again, and each time, click on
the little i (or arrow) and select what summary you want to display. So
to add a new column, for minimum value you would drag age, and select
minimum, like so:

\includegraphics{imgs/make_min_col.gif}

Now do this again to create a column for maximum, and another column for
standard deviation to be able to talk about variance as well.

In the end your table should look like this:

\includegraphics{imgs/reason_v_age_no_med.png}

If it doesn't, just raise your hand and we can come around to help you
get there.

Note: for creating the standard deviation column you have two choices
there, StdDev and StdDevp. The one to choose will depend if you would
like to know the standard deviation of your sample or your population.
We will speak about this more next week, but you can imagine that the
data we have come from a \emph{sample} of people, who are used to draw
conclusions about the whole \emph{population}. Rather than interview
everyone who has ever been arrested, we rely on gathering data from the
interviews of enough people that we can make generalisations. These
people who we actually speak to, they are our sample. Once we use
inferential statistics, we can use data from the sample to make
generalisations about the whole population. But with descriptive
statistics we are actually talking only about our sample. So in this
case you want to select \emph{StdDev} and \textbf{not} \emph{StdDevp}.
If you're confused about this ask now!

Right, so now, we can finally answer our first question, \emph{Which
reason for arrest do you think has the oldest arrestee}? So, which is
it?

\includegraphics{https://uproxx.files.wordpress.com/2012/02/grampa-turtle.gif?w=650}

Well the oldest arrestee appears to have been arrested for
\emph{Assault}, at the age of 82 years old. Is this the category you
were expecting the oldest offender in our sample to appear? Why or why
not? Take a moment to try to interpret the data. It's important to
always return to the meaning that we can extract from our numbers. Talk
to someone next to you about this.

The other interesting column here to pay attention to is the standard
deviation (the minimum age column is not super exciting. People under 17
were excluded from the survey, and so the youngest possible age in the
sample is 17. It appears that at least one 17-year-old was arrested in
every single one of these offence categories.) But the standard
deviation, that tells us something new. Remember that it is a measure by
which we can describe the variation of our individual observations
around the mean. So the larger the standard deviation the larger the
variation around the mean age in a particular subset of our data. Which
offence category has the highest standard deviation? Which has the
lowest?

Theft of and theft from vehicle both have a standard deviation of about
7, as well as young mean ages. It appears that these crimes are mostly
committed by younger offenders, with only a few of them committed by
older offenders. Sex offences however appear to have a greater standard
deviation, where offenders come from all ages. Any particular reasons
why you think this might be the case?

Now finally, before we move on, there is something you might have
noticed. We did not include the \textbf{median} in our summary. Did you
see it anywhere in your drop down menu, when selecting how to summarise
the ``age'' variable? Well for some reason it's not as easy as
calculating the other ones. Instead, to display the median we have to
rely on building our own formulas. But you're getting good at formulas
by now, so let's give it a go!

\hypertarget{activity-5-adding-a-column-for-median}{%
\subsection{Activity 5: Adding a column for
median}\label{activity-5-adding-a-column-for-median}}

 To add a column for median, we will have to use something called an
``IF'' statement in Excel. The IF function is one of Excel's logical
functions, used to return one value if a condition is true and another
value if it's false. We can use that here, because we want to include a
value in our calculations \emph{if} if belongs to the value of reason
for offence that we are calculating the median for. If it's any of the
other offences, we \emph{do not} want to include it.

It might help to think about this conceptually, before we write the
formula. Let's say we go through our data row by row. Remember in our
data, each row is one arrestee who was interviewed. We want to go
through every one of these people, and include their age in our
calculation for the median ror each offence category, \textbf{only if}
the reason that that particular person was arrested \emph{is} the
offence category for which we are calculating the median.

So for every person, we want to check if their value for \emph{reason
for arrest} is the one we are currently interested in.

For example, if we are calculating the median age for theft of vehicle,
for every line we first need to make sure that the value in the
\emph{reason for arrest} column is equal to \emph{assault}. If it does,
then we use that person's age to calculate mean age for theft of
vehicle. If it is not, we do not.

How would you write this in a formula? Well for assault we could say
something like:

\texttt{include\ IF\ \textquotesingle{}Reason\ for\ arrest\ column\textquotesingle{}\ =\ \textquotesingle{}assault\textquotesingle{}}

Now we just have to translate this to excel language, and put it inside
a median calculation formula. Remember this one from last week? It's
simply \texttt{=MEDIAN()}.

Now we also have to refer to some cells. You can select these values by
simply clicking on the cells that you need, but I'll go through the
notation here again, just as a refresher.

To specity what sheet you are taking your data from, you put the sheet
name followed by an exclamation mark (\texttt{!}). So to refer to
columns that are on the arrestees\_subset.csv sheet, Excel notation for
that is: \texttt{arrestees\_subset.csv!}.

A cell you refer to by the letter of the column it's in, and the number
of the row that it's in. So the cell that contains the value for
``Assault'' is ``A5''. However, when you copy references, this will
change. If you wanted to statically \textbf{always} refer to A5 you have
to use the dollar sign in front of each element (row letter, column
number). So if I always want A5 to be the cell I refer to, I would type:
\texttt{\$A\$5}.

And to refer to a range of values, you denote them as
\texttt{first\ value:last\ value} . SO to get all the values from row 2
of column E to row number 19897 of column E, I would type:
\texttt{E2:E19897}. And if I wanted to make sure that I \textbf{always}
refer to these values, again I just inject a \texttt{\$} in front of the
column reference (E) and the row reference (2 and 19897). Like so:
\texttt{\$E\$2:\$E\$19897}.

Now building it all together, for assault, we could calculate the median
like so:

\texttt{=MEDIAN(IF(arrestees\_subset.csv!\$E\$2:\$E\$19897=A5,arrestees\_subset.csv!\$B\$2:\$B\$19897))}

Just to re-iterate again, this is what each part of this formula does:

\includegraphics{imgs/med_excel_equ.png}

When you are comfortable with this formula, paste it over into the cell
in the new ``median'' column you've created in your table, and hit
Ctrl+Shift+Enter.

\textbf{NOTE: Hit Ctrl+Shift+Enter} This is important! \emph{Don't just
hit enter} to complete the formula. You must hold Ctrl+Shift+Enter down
together to complete the formula and tell Excel that it is an array
formula. Excel will add curly braces around the formula if this is done
correctly:

\includegraphics{imgs/curly_brack_formula.png}

You can simply copy the formula by double clicking on the bottom right
hand corner of the cell, or grabbing it and dragging it down. Whichever
option you prefer, they achieve the same thing. After which you will end
up with a brand new column you made all on your own, for the median:

\includegraphics{imgs/final_table_with_median.png}

Now, finally you have a median column in your pivot table as well. You
can now begin to think about what the difference between mean and median
means in terms of the skew in your data as well. We don't seem to be
getting huge differences, and this can potentially be put down to our
\emph{large sample size}. We have almost 20,000 rows in this data, which
is 20,000 people who were interviewed and answered all these questions.
We'll speak more about the importance of sample size later in the
course, but now you know, that your means and your medians are not too
far apart in either of the offence categories people were arrested for,
with a maximum difference of around 3 years.

\hypertarget{numeric-v-numeric}{%
\section{Numeric v numeric}\label{numeric-v-numeric}}

Regarding descriptive statistics to explore the relationship between two
numeric variables in our sample, to analyse the relationship between two
quantitative variables, we consider how one variable, called a response
variable, changes in relation to changes in the other variable called an
explanatory variable. Graphically we use scatterplots to display two
quantitative variables, as comparing two numeric variables is best
achieved through the use of graphics and visualisation. As you might
imagine, it becomes very difficult to create any sort of crosstabs
between numbers. Instead, you want to be able to determine whether there
is a relationship between two numbers in other ways. We'll illustrate
with two other variables from the arrestee survey. Let's consider the
relationship between ``Age at first arrest'' and ``Number of arrests''.
We might be interested about this if we are thinking about criminal
trajectories, for example
\href{http://www.journals.uchicago.edu/doi/abs/10.1086/449107}{delinquency
careers} or
\href{http://onlinelibrary.wiley.com/doi/10.1111/j.1745-9125.1995.tb01173.x/full}{life
course trajectories}.

What would our inclination be? Clearly if the person has their first
arrest earlier on in life, they have a lot more time to also have some
more arrests. Or they might have an early arrest and part take in some
sort of intervention whereby they turn their life around, and never
offend again. In other words, we are thinking that either \textbf{as age
of 1st arrest decreases, number of arrests will increase}, or we think
that \textbf{as age of 1st arrest decreases, number of arrests will also
decrease}. These two scenarios describe two different kinds of
relationships, a positive relationship or a negative relationship. In
either scenario we assume that as one goes up the other one goes up or
as one goes up the other one goes down, but that this takes a linear
relationship. If you imagine one numeric variable across the x axis and
another cross the y axis, a positive and a negative relationship would
look something like this:

\includegraphics{http://www.statisticshowto.com/wp-content/uploads/2015/08/lin-rel.png}

\emph{Note:} The figure above shows you what a perfect positive
relationship would look like, or a perfect negative relationship would
look like. It also assume a \textbf{linear} relationship. This just
means that we are looking at a ``straight line trend'' between the two
variables.

So as discussed, a graphical representation, in this case a scatterplot
is the most useful display technique for comparing two quantitative
variables. We plot on the y-axis the variable we consider the response
variable and on the x-axis we place the explanatory or predictor
variable.

How do we determine which variable is which? In general, the explanatory
variable attempts to explain, or predict, the observed outcome. The
response variable measures the outcome of a study. One may even consider
exploring whether one variable causes the variation in another variable
-- for example, a popular research study is that taller people are more
likely to receive higher salaries. In this case, age at first arrest
would be the explanatory variable used to explain the variation in the
response variable number of arrests.

In summarizing the relationship between two quantitative variables, we
need to consider:

\begin{itemize}
\tightlist
\item
  Association/Direction (i.e.~positive or negative)
\item
  Form (i.e.~linear or non-linear)
\item
  Strength (weak, moderate, strong)
\end{itemize}

A scatter plot is a useful visual representation of the relationship
between two numerical variables (attributes) and is usually drawn before
working out a linear correlation or fitting a regression line, which are
the next steps that you would take, if you were to also perform
\emph{inferential statistics}. The resulting pattern indicates the
elements of the relationship outlined above, the association, the form,
and the strength of the relationship between two variables.

So what are some examples of correlations? Here are a few:

\begin{itemize}
\tightlist
\item
  \href{http://asbcllc.com/blog/2014/december/nba_team_corr_matrix/scaled/}{Interactive
  Correlation Matrix and Scatter Plot All NBA Team Statistical Data
  1951-2015 Seasons}
\item
  \href{http://rebrn.com/re/relationship-between-rotten-tomatoes-tomatometer-score-and-box-o-2558335/}{Scatterplot
  of the Relationship Between Rotten Tomatoes Tomatometer Score and Box
  Office Revenue for Movies, for each movie genre}
  -\href{https://fsmedia.imgix.net/af/f3/d7/f5/b2df/470f/8f8c/4b41daf55348/the-correlation-between-trump-voters-and-obesity-has-a-strong-positive-correlation-at-0717.png?auto=format\%2Ccompress\&w=700}{Correlation
  between statewise obesity and voting for Trump}
\end{itemize}

You can understand the basic premise. \#\#\# Activity 6: Does age at
first arrest correlate with total number of arrests?

 So let's build one for our arrestees. Since we are trying to explain
number of arrests with the age of first arrest, we would plot on the
y-axis the variable we consider the response variable (number of
arrests) and on the x-axis we place the explanatory or predictor
variable (age of first arrest).

First, just return to the excel spreadsheet where you have your data.
Firstly arrest the two columns that contain our variables of interest:

\includegraphics{imgs/scatter_1.png}

Then select charts \textgreater{} scatter \textgreater{} marked scatter:

\includegraphics{imgs/scatter_2.png}

The graph that initially appears might look something like this:

\includegraphics{imgs/scatter_3.png}

You can see that actually we have age of first arrest on the x axis, and
number of arrests on the y axis, as we prefer. If this was not the case,
you can change this by right clicking anywhere on the chart, and
selecting ``Select Data\ldots{}'' on the options that pop up:

\includegraphics{imgs/scatter_4.png}

and then on the popup there is an option to switch row/column:

\includegraphics{imgs/scatter_5.png}

But in this case we are alright. One thing you should do though, is
label your axes. Although, we might be able to infer that the axis with
a value that goes up to 300 is not \emph{likely} to be the one for age,
it's always nice to have some certainty, and axis labels will provide
this. To do this, click anywhere inside the chart, and click n the
``Chart Layout'' tab:

\includegraphics{imgs/scatter_6.png}

\includegraphics{imgs/scatter_7.png}

Then click on the ``Axes'' option, and for both horizontal and vertical
axes add a title:

\includegraphics{imgs/scatter_8.png}

You can also stylize your scatterplot, make it look the way that you are
most happy with it.
\href{http://strategyandanalytics.com/5-steps-creating-beautiful-eye-catching-charts-excel/}{Here
are some tips on making your graphs look pretty}. Here's mine:

\includegraphics{imgs/scatter_9.png}

So let's try to infer our indicators of the relationship between the two
variables. What about association/direction? Form? And strength? Well
the easiest way to think about this is to think about drawing a
\emph{line of best fit}. Could you draw a straight line through the
cloud of points? If yes what does this line look like?

We can ask Excel to draw this line for us. You can do this by going back
to the ``Chart layout'' tab, and this time clicking on the ``Trendline''
option:

\includegraphics{imgs/scatter_10.png}

From the dropdown options, select ``Linear trendline'':

\includegraphics{imgs/scatter_11.png}

Now you can see your trendline appear:

\includegraphics{imgs/scatter_12.png}

So what does this line look like? Well it definitely has a negative
slope (it's pointing down, rather than pointing up). Therefore, in terms
of the direction of the relationship, we can conclude that it is
negative. In terms of strength, we want to look at the slope of the
line. We saw above what a perfect linear relationship looks like. It's a
slope of basically one-to-one. In this case it would mean that for every
one year earlier that someone has their first arrest, they have one more
arrest. We can see that the slope of our line is very close to \emph{no
slope} or a slope of zero, because it is essentially a straight line. To
give you an idea of weak/strong relationships based on slope of a line,
here is a handy image:

\includegraphics{http://www.psychology.emory.edu/clinical/bliwise/Tutorials/SCATTER/scatterplots/examples2.jpg}

You can imagine here that our relationship is quite weak, as it is close
to a zero slope. Finally is the relationship linear or non linear? Well
how well does this straight line represent your data? And where the
points deviate from the line, do they do so in a systematic manner? To
be fair, this question is a bit tough to answer, just on visual
assessment through a scatterplot alone. As is the strength without a
numeric interpretation of the slope.

So far we have visualized relationships between two quantitative
variables using scatterplots, and described the overall pattern of a
relationship by considering its direction, form, and strength. We noted
that assessing the strength of a relationship just by looking at the
scatterplot is quite difficult, and therefore we need to supplement the
scatterplot with some kind of numerical measure that will help us assess
the strength. This is what inferential statistics like correlation
coefficients will be able to do, but that's for the future.

Even though in the rest of the time we talk about these we are going to
focus only on linear relationships, it is important to remember that not
every relationship between two quantitative variables has a linear form.
There are several examples of relationships that are not linear. The
statistical tools that will be introduced here are appropriate only for
examining linear relationships, and as we will see, when they are used
in nonlinear situations, these tools can lead to errors in reasoning.
While we don't require it for this session, if you wanted to read up a
bit about identifying linear vs non-linear relationships
\href{http://blog.minitab.com/blog/adventures-in-statistics-2/linear-or-nonlinear-regression-that-is-the-question}{here}.

But based on our observations, we can say that there appears to be a
very weak negative relationship between the age of first arrest, and the
number of times that someone has been arrested to date, in our sample of
arrestees interviewed as part of this arrestee survey. And we can
support this with our scatterplot as well.

\hypertarget{correlation-does-not-mean-causation}{%
\subsection{Correlation does not mean
causation}\label{correlation-does-not-mean-causation}}

You may have heard this phrase before - that correlation does not mean
causation. This is important to mention here, and for you to take this
forward. Just because two variables show a relationship, even if it is a
strong relationship, it does not mean that one actually causes the
other, no matter how attractive telling that story would be.

There is an interesting site here that has a lot of \textbf{spurious}
correlations. Spurious correlations are when there is a strong
correlation between two or more variables that are not causally related
to each other, yet it may be wrongly inferred that they are, due to
either coincidence, or the presence of a certain third, unseen factor
(referred to as a ``common response variable'', ``confounding factor'',
or ``lurking variable''). A good example is the relationship between
drownings and ice cream. Ice cream consumption per capita increases at
the same rate as people drowning in pools. But if you think about why
this might be, I think you might struggle to find a reason why ice cream
would cause drownings. But there is something that would cause an
increase in both\ldots{}! And that is the temperature. As temperature
increases, particularly in the summer, there are more people outside,
buying ice cream, but also more people outside swimming in pools, lakes,
and rivers.

Have a listen to this
\href{https://www.youtube.com/watch?v=8B271L3NtAw}{ted talk} which
explains better.

Spurious correlations are all over the place, and while sometimes they
are very obvious, for example, here is one that correlates the number of
films that Nicholas Cage has appeared in, and the number of people who
drowned by falling into a pool:

\includegraphics{imgs/supr_corr.png}

(\href{http://www.tylervigen.com/spurious-correlations}{and you can see
more strange correlations here})

But in other cases its much more subtle. And can happen between
variables that you can weave together a very nice story or explanation
as for why they should be related. No matter what, you should think
about these examples always when you are looking at correlations. We
will talk later about study design, especially in relation to being able
to infer causality, but do make a note now, to keep in mind in the
future, that \emph{correlation does not prove causation}.

\includegraphics{https://imgs.xkcd.com/comics/correlation.png}

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

In sum, you should now be able to begin to make comparisons between two
variables in your data set, and talk about the possible relationships
between them, and support (or question) the assumptions that you might
have with some evidence. You should be comfortable with the following
terms:

\begin{itemize}
\tightlist
\item
  bivariate
\item
  crosstab (or two-way frequency table, or contingency table)
\item
  row percentage
\item
  column percentage
\item
  stacked bar chart
\item
  conditional formatting
\item
  producing summary statistics by groups
\item
  if statements in excel
\item
  association/ direction of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    positive relationship
  \item
    negative relationship
  \end{itemize}
\item
  form of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    linear relationship
  \item
    non-linear relationship
  \end{itemize}
\item
  strength of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    weak
  \item
    moderate
  \item
    strong
  \end{itemize}
\item
  scatterplot
\item
  trendline
\item
  correlation
\item
  correlation does not mean causation
\end{itemize}

\hypertarget{week4}{%
\chapter{Week 4}\label{week4}}

\hypertarget{learning-outcomes-3}{%
\section{Learning outcomes}\label{learning-outcomes-3}}

We as researchers all start off with a general area that we're
interested in. As someone studying Criminology, you are likely to be
interested in Criminology-related topics. You might want to learn about
policing, or criminal justice practices, or you might be interested in
something like
\href{http://criminology.oxfordre.com/view/10.1093/acrefore/9780190264079.001.0001/acrefore-9780190264079-e-3}{situational
crime prevention}. These are very broad interests that you may have, and
within these there are many many sub-topics, and potentially research
questions that you might want to explore. During the research process,
the researcher becomes an expert in his or her field and the methods and
techniques to be used for research. The researcher goes through several
stages and must deal with the concept of variable and the assumption of
its measurement.

So far in this course we have always started with our variables. We
obtain a data set which has variables that measure the things that we
are interested in. The police.uk data has crime records, that we can use
to explore difference in volume of different crim types, or difference
in number of crimes between neighbourhoods, or potentially look at home
crime rates change over time, with seasons for example. Or with the
arrestee survey we saw variables that related to people's experiences
with their arrests, and in the business victimisation survey with
businesse's experiences of victimisation, and their perceptions of
safety, as well as the precautions they take, measured in terms of their
spending on IT security for example.

In all these cases, we were presented with a set of variables, but
without having much say in how the variables are defined in terms of
representing the concepts that we want to be able to talk about when
making sense of our data. A variable is a structure of characteristics,
qualities, or quantities that in some form provide information about a
specific descriptive phenomenon. Information that is provided by
variables that are under study is fundamental for the researcher and
data analyst. However, this information and its quality will depend on
how variables are quantified and on the quality of its measurements. In
all these cases we started with the data, but actually, most of the
time, you are much more likely to start with the research questions
you're answering. As a researcher you are interested to learn more about
your chosen topic. Whether that's drugs, probation, stop and search
practices of the police, online cryptomarkets, fear of crime, transport
crime, police use of twitter, hate crimes, whatever it is, you want to
focus on exploring your topic. You can explore these topics through
data, if you can collect data, or data is already available for you to
analyse. But how do you go from research topics and ideas and questions
to actual variables? Well this is what we will learn today.

Also we will discuss the manner in which you can collect these variables
about a \emph{sample} of people who represent the \emph{population} in
which you are interested. The term ``\textbf{population}'' is used in
statistics to represent all possible measurements or outcomes that are
of interest to us in a particular study. On the other hand, the term
``\textbf{sample}'' refers to a portion of the population that is
representative of the population from which it was selected. The sample
is the set of people from whom you take measurements. Everyone in your
sample has one row that represents them in your data set. For example,
in the arrestee survey used last week, every person arrested was a part
of the \textbf{sample}, of people interviewed. Then, the results of any
data analysis based on this sample can be used to make inferences about
the \textbf{population} of all people arrested, if this sample was
representative.

This distinction might seem intuitive, but whether we're talking about
our population or our sample when discussing our data, as well as
whether we refer to \emph{sample statistics} or \emph{population
parameters} will also surface as important in this week's material.

\hypertarget{terms-for-today-2}{%
\subsection{Terms for today:}\label{terms-for-today-2}}

\begin{itemize}
\tightlist
\item
  Sample and population
\item
  Conceptualisation
\item
  Operationalisation
\item
  Measurement
\item
  Constructs and composite variables
\item
  Multi-item scales
\item
  Reliability and validity (round 2)
\item
  Recoding
\end{itemize}

\hypertarget{population-parameters-versus-sample-statistics}{%
\section{Population parameters versus sample
statistics}\label{population-parameters-versus-sample-statistics}}

\emph{Hold on a second!} Population standard deviation? But so far we
have only been talking about samples? Well, this week we also discuss
sampling, right? And the differences between sample statistics and
population parameters. A statistic and a parameter are very similar.
They are both descriptions of groups, like ``50\% of dog owners prefer X
Brand dog food.'' The difference between a statistic and a parameter is
that statistics describe a sample. A parameter describes an entire
population. (You might recall this from week 1 readings.)

For example, say you want to know the mean income of the subscribers to
a particular magazine---a parameter of a population. You draw a random
sample of 100 subscribers and determine that their mean income is
\$27,500 (a statistic). You conclude that the population mean income μ
is likely to be close to \$27,500 as well. This example is one of
\textbf{statistical inference}.

If you are taking the second term module, Modeling Criminological Data,
you will be introduced to techniques for making \textbf{statistical
inference}. But essentially it refers to any process where you draw
conclusions about a population based on the data analysis you perform on
a sample of that population. Because you're using the statistics to make
inferences, this process is called \textbf{inferential statistics}.

With \textbf{inferential statistics}, you are trying to reach
conclusions that extend beyond the immediate data alone. For instance,
we use inferential statistics to try to infer from the sample data what
the population might think. Or, we use inferential statistics to make
judgments of the probability that an observed difference between groups
is a dependable one or one that might have happened by chance in this
study. Thus, we use inferential statistics to make inferences from our
data to more general conditions; we use descriptive statistics simply to
describe what's going on in our data.

Take the example of the Crime Survey for England and Wales. The Crime
Survey for England and Wales has measured crime in since 1981. Used
alongside police recorded crime data it is a valuable source of
information for the government about the extent and nature of crime in
England and Wales. The survey measures crime by asking members of the
public, such as yourself, about their experiences of crime over the last
12 months. In this way the survey records all types of crimes
experienced by people, including those crimes that may not have been
reported to the police. It is important that we hear from people who
have experienced crime and also those who have not experienced any crime
in the last 12 months, so that we can show an accurate picture of crime
in the country.

Each year around 50,000 households across England and Wales will be
invited to participate in the survey. In previous years three quarters
of households invited to take part agreed to participate. While this
does encompass a lot of people, you can see how it's not the entire
population of England and Wales. Instead it is a \textbf{representative
sample}.

While reporting sample statistics from the sample might be interesting
(ie to talk about whether men or women have higher worry about crime),
the more interesting approach is to make inferences about the population
at large. Do men and women \emph{generally} have different approached to
fear of crime? In order to answer these questions, criminologists use
*\textbf{p-values} and \textbf{effect sizes} to talk about the
relationships between two variables in a population. Since in this
course we mostly talk about the relationships between variables n a
sample, we talk about the sample statistics. So you will learn about
inferential statistics in the future. But for now it is important that
you note this different between \textbf{sample statistics} and
\textbf{population parameters}.

They even have different notation. For example, you would use x-bar (x̄)
to talk about your sample mean, but mew (μ) to talk about your
population mean. Similarly, you can calculate the standard deviation for
your sample, or for your population. In each instance, you can measure
each for your sample, and then infer it for your population. How do you
do this? Well, you make use of something called the \textbf{sampling
distribution of the mean}.

The mean of the sampling distribution of the mean is the mean of the
population from which the scores were sampled. Therefore, if a
population has a mean μ, then the mean of the sampling distribution of
the mean is also μ. The symbol μM is used to refer to the mean of the
sampling distribution of the mean.

\hypertarget{activity-1-sample-statistisc-vs-population-parameters}{%
\subsection{Activity 1: Sample statistisc vs population
parameters}\label{activity-1-sample-statistisc-vs-population-parameters}}

Let's explore this with some data. Download the heightsAndNames.xlsx
file from Blackboard.

Open this data with Excel. You can see we have 3 variables. One variable
is the people's names, one is their height, and the third variable, I
created, putting these people into 6 different samples, assigning them
randomly. Let's assume that everyone in this excel spreadsheet is our
population. Everyone here is everyone who exists, these 300 people are
the only people that exist in our population. Now let's suppose that we
don't have all their heights. Let's suppose that we do not have the
budget to measure the height of all these 300 people, and instead we
have to measure the height of a sample of 50 people.

Let's say we initially select sample 1. You can filter to show only
sample 1 in excel by using the ``filter'' option. Click on the data tab
in excel, and the filter option (the little funnel icon):

\includegraphics{imgs/filter_data.png}

When you click this, little downward arrows should appear next to the
column names for each one of your variables. Click on the one next to
the ``sample'' variable:

\includegraphics{imgs/filter_arrows.png}

When you do this a little window will appear. In this window you will
see all the possible values that the ``sample'' variable can take. In
this case, it's the numbers 1 through 6, as there are 6 possible samples
of 50 people that we have, from these total population of 300 people.
Everything that has a ticked box next to it will appear, while
everything that does not will get filtered out.

\includegraphics{imgs/filter_options.png}

Untick the box next to every single sample, except sample 1. Like so:

\includegraphics{imgs/filter_sample_1.png}

When you do this you should see in your data that all the other rows
will disappear, and you will see only the ones where the value for the
``sample'' variable is ``1''.

So now we have our first sample of our population. We have a sample of
50 people, and we use these 50 people to collect data. In this case, the
data we collect is their heights. Let's say we're interested in their
average height. We can get this, using the \texttt{=AVERAGE()} function
in excel.

So let's create a small table in another sheet in our excel spreadsheet,
that calculated the average height for our sample 1. To do this, create
a new sheet (by clicking on the little plus sign at the bottom of your
excel page):

\includegraphics{imgs/new_sheet_2.png}

Then in this new sheet, create a cell for each sample, from sample 1 to
sample 6. Like so:

\includegraphics{imgs/set_for_avgs.png}

Then, in the cell next to the ``sample 1'' cell, enter the
\texttt{=AVERAGE()} formula. Click inside the brackets, and go select
the values for height, in your sample 1.

\includegraphics{imgs/avg_sample_1.gif}

Repeat this for each one of the samples. So each time, go to the little
down arrow next to the ``Sample'' column, select the next sample (so
first 2, then 3, then 4, and so on), and each time, calculate the
average height for the sample.

You should end up with the average height for 6 different samples of
your 300 people:

\includegraphics{imgs/avg_per_sample.png}

Now this is interesting. Depending on which sample you end up with, you
get a different \textbf{sample statistic} for the mean. With only one of
the samples, you would use that sample statistic to make inferences
about the population parameter. So if you ended up with sample 1, you
would assume the average height is 169.7 centimetres. On the other hand,
if you ended up with sample 5, you would say the average height is 173.1
centimetres.

Well, in our special case, we can know the population parameter. To get
this, simply get rid of your filters, so you have all 300 observations,
and get the average for these. I'll let you figure out yourself how to
do this. Add it as a row in our table of averages, under the ``sample
6'' average.

So\ldots{} did you get it?

The average height for our population is about 172.1 centimetres. So our
sample statistics are all sort of near this measure, but depending on
which sample we end up with, we do better, or worse with getting near
this number. Now I'm going to show you something neat.

Take the average of the averages. That's right. What is the average of
all 6 sample averages? You guessed it, it's equal to our population
average! It's also about 172.1 centimetres. Now I cheated a bit here,
because we have non-overlapping samples without replacement, so you do
end up taking your population sample, but actually, if you repeatedly
sampled from a population, over and over again, and then took the
average height in each sample, and then finally, after infinitely many
samples, you took the average of all of the averages, it would be equal
to the population parameter.

This is something called the \textbf{sampling distribution of the mean}.

\hypertarget{central-limit-theorem}{%
\section{Central Limit Theorem}\label{central-limit-theorem}}

We will never have many many many samples of the same thing, right?
Surveys are costly, and we are happy if we can run it once. So instead
of running the same survey many many times on different samples of the
population, we can rely on another mathematical rule, called the
\textbf{central limit theorem}. The Central Limit Theorem states that
the sampling distribution of the sampling means approaches a normal
distribution as the sample size gets larger. The Central Limit Theorem
tell us that as the sample size tends to infinity, the of the
distribution of sample means approaches the normal distribution. This is
a statement about the SHAPE of the distribution. A normal distribution
is bell shaped so the shape of the distribution of sample means begins
to look bell shaped as the sample size increases.

You can watch \href{https://www.youtube.com/watch?v=JNm3M9cqWyc}{this
video} to learn about central limit theorem some more.

Further, something called the \textbf{Law of Large Numbers} tells us
where the center (maximum point) of the bell is located. Again, as the
sample size approaches infinity the center of the distribution of the
sample means becomes very close to the population mean. Basically, it
means that the larger our sample, to more we can trust that our sample
statistic reflects the population parameter.

Give this a go in our heights data in excel. Let's select a mega sample
of 150 people. Let's use odd numbered samples: 1, 3, and 5 to do this.
Calculate the average height of all the people who are in these 3
samples. To do this, go back to your filtering, and make sure that the
boxes next to 1, 3, and 5 are all ticked. Then calculate the average.
Add this as a new row in your sheet where you've already calculated all
the averages. Pretty close to our population mean, no?

Let's try also for the even numbered samples, 2, 4, and 6. Add this as a
new row as well. So? Close? I would say so:

\includegraphics{imgs/larger_samples.png}

If you're still unsure,
\href{https://www.youtube.com/watch?v=agBcvkyi6sg}{this video may help}
as well as \href{https://www.youtube.com/watch?v=iN-77YVqLDw}{this one}.

This might not seem obvious at first, but really, it's something that
you probably already use in your everyday life. Think about when you are
visiting a new city and you are looking to find a nice place to eat. You
might use tripadvisor. Or yelp. In case you're not familiar, these are
review sites, where people can give a star rating (usually out of 5) to
an establishment, and then others looking to visit these venues can see
what ratings others have given them. You probably have used something
like this, when you are exploring a new part of town, or in an area you
might be unfamiliar with. And chances are, you refer to the law of large
number when deciding where to eat. Take this example. You come across
the below reviews:

\includegraphics{imgs/review_1.png}

and

\includegraphics{imgs/review_2.png}

You can see that actually review 1 has the higher mean score, right?
It's got a score of 5 stars, the maximum amount of stars which you can
have. Compared to that, the restaurant in review 2 has only 4 stars. So
based on this rating alone, and no additional information, your ideal
option is to choose the restaurant in the first review, ``Fantasy
Manchester''. But you most likely wouldn't. Chances are, if you were
going by the rating information only, then you would be more likely to
select the restaurant in the 2nd review, Little Yang Sing. And that is
because of the number of reviews. With only 2 reviews for ``Fantasy
Manchester'', you cannot be certain that a diverse set of opinions is
represented. It could be two people who are really biased for all you
know, such as friends of the owner. On the other hand, you know that the
33 people who reviewed ``Little Yang Sing'' will represent enough
variation that it might include some biased people, but on the whole
will be a better representation of the experience that all diners have,
and therefore a better indication of what your experience will be like.
In other words, the sample statistic of the 4 stars, is probably closer
to the population parameter, which is the possible star rating that the
restaurant would get if everyone who ever visited, and will ever visit,
were to rate it. On the other hand, the true score for ``Fantasy
Manchester'' might be very different to the 5 stars we see from the
sample, if we were to be able to get a hold of the population parameter.

Hopefully this illustrates little bit about the difference between a
sample and the population, but also how we can make inferences from the
sample about the population, and how the larger your sample is, the more
you can rely on the sample statistics reliably representing the
population parameters.

The population standard deviation you can also estimate from your
sample, and the relevant sample statistics. In fact, we very briefly
touched on this last week. Remember the possible options for STDEV and
STDEVP? When we were calculating the standard deviation column there
were two choices there, StdDev and StdDevp. The one to choose will
depend if you would like to know the standard deviation of your sample
or your population. There is a slight difference in how you calculate
your standard deviation, based on whether we are talking about our
sample or our population. The first part is all the same. We still look
at the difference between each measurement and the mean. And then we
still take the square of these differences. However, the next step,
where we calculate the standard deviation for the sample, we divide by
the number of observations in our sample. \emph{However} if we were
calculating the standard deviation for our \emph{population} we would
divide by the number of observations in our sample \textbf{minus one}.
Why minus one? Well this is because instead of dividing by the sample
size, we are dividing by another values, one called the \textbf{degrees
of freedom}.

What is \textbf{degrees of freedom}? In statistics, the number of
degrees of freedom is the number of values in the final calculation of a
statistic that are free to vary. What does this mean? Well imagine that
we know what our sample statistic for the mean is. Let's say that it's
172. And we also know how many people are in our sample. Let's assume we
have a mini sample of 5 people. The degrees of freedom here represent
the number of people for whom we can freely choose any height we wanted.
Which just happens to be one minus the sample size. Why? Let me
illustrate:

Here are our 5 people: Alice, Bob, Cecilia, Daniel, and Elizabeth. We
know that their average height has to be 172 centimetres. This is a
given.

So how tall can Alice be? Alice can be any height we want. Pick a number
for Alice. Let's say she's tall. Alice is 195cm tall. Good. Now Bob. How
tall is Bob? Again you are free to pick any number. Bob can be 185 cm
tall. Cecilia? Again, any height we want. Let's say 156. Daniel? Again
we can pick any height for Danuel. Let's say he's a tall one, at 200cm.
What about Elizabeth? Well for her, she can only take one possible
height. Why? Well we know the height of the other 4 now, and we also
know the average. If we treat Daniel's height as an unknown \texttt{X}
cm tall, we can write this as the following equation:

\texttt{(195\ +\ 185\ +\ 156\ +\ 200\ +\ X)\ /\ 5\ =\ 172}

From your days of maths this might look familiar, but even if it does
not, you should be able to know that this equation only has one
solution. X can only be one value. Because we \emph{know} what the final
average will be, and because we could freely choose the heights of the
first 4 people, the 5th height is restricted (in this case it will have
to be 124cm, so Daniel is the shortest in our sample). Therefore we are
only \emph{free} to choose the first 4 heights, which is our sample
\emph{minus 1}. Our degrees of freedom are sample \emph{minus 1}. As
degrees of freedom refers to the number of independent ways by which a
dynamic system can move, without violating any constraint imposed on it,
this is why it is called number of degrees of freedom. In other words,
the number of degrees of freedom can be defined as the minimum number of
independent coordinates that can specify the position of the system
completely. As illustrated above, in this case that is the sample size
minus one.

So when you are calculating the standard deviation for \emph{the
population} you divide by the degrees of freedom, rather than by the
number of observations in your sample. This is the difference between
the StdDev and StdDevp functions in Excel to calculate your standard
deviation.

Now, why are we talking about this again? Well this will all come in
handy when we're calculating and validating our measures for the various
concepts that we're going to study and collect data on to be able to
draw meaningful conclusions about. But I'm jumping a bit ahead. Let's
first start with the concepts.

\hypertarget{conceptualisation}{%
\section{Conceptualisation}\label{conceptualisation}}

We will start this week with introducing the term
\textbf{conceptualisation}, which seems appropriate, as you would start
most of your research with this step of the data analysis process.
\textbf{Conceptualisation} is the first step in the process of turning
your research questions and topics of interest into variables that you
can measure and collect data, in order to be able to answer your
questions. \textbf{Conceptualisation} is the process by which you define
the things you're interested in, and define them as \textbf{concepts}.
This means you have to decide what you will and what you won't include
as relevant to your topic, or to your concept. Let's illustrate with an
example.

Let's say we're interested in studying education. Defining what we mean
by education here is the process of conceptualisation of this topic.
Let's say we conceptualise education as ``Amount of knowledge and
training acquired in school.'' Then we can say that we want to consider
all the education that people learned while enrolled in the K-12 system,
but that means that we would not include in this definition
non-traditional approaches to education, such as home schooling, or
people who return to school later, or take classes from prison, for
example. You can start to get a sense of what impact conceptualisation
can have on your findings, no?

Let's take another example, let's consider that we want to study
``social status''. How we conceptualise social status will have
implications for what it is that we might consider in the scope of our
study, and what we would not. For example, if we consider social status
to be made up of the \emph{dimensions} of wealth, prestige, and power,
then we want to somehow collect data about these particular elements of
social status. But we might ignore other elements of the concept, which
others might deem relevant, such as social capital (the networks of
relationships among people who live and work in a particular society,
enabling that society to function effectively). Now this is important,
because if someone else wants to also study social status, and has a
look at our study, they should be clear on what was and what was not
included, and so they can make their own decisions about our study and
approach, but at least we are transparent about this, and can use this
to make sure that we can justify our results.

To really demonstrate this, it's a good idea for you to have a go at
conceptualising something yourself. Let's assume that you are interested
in studying hate crime. In particular you are interested in hate speech
over social media. Let's say you pick twitter as your case study. Now
you have to \textbf{conceptualise} what it is that you mean by hate
speech. This is important so that both you, and anyone reading your
study later is clear about what should be included in your study, and
what should be excluded. You can imagine that this is a very important
step, since what is included/ excluded.

\hypertarget{activity-2-conceptualise-hate-speech.}{%
\subsection{Activity 2: Conceptualise `hate
speech'.}\label{activity-2-conceptualise-hate-speech.}}

So imagine you are about to embark on a study of \emph{hate speech}.
Remember in the first session we spoke about some examples of
research,\href{https://www.washingtonpost.com/news/the-intersect/wp/2016/09/26/these-charts-show-exactly-how-racist-and-radical-the-alt-right-has-gotten-this-year/?noredirect=on\&utm_term=.aeb311ab060c}{and
one was looking at radicalisation towards the alt right using twitter
data}. Let's say we want to carry out a similar study.

You have some twitter data, which includes 100,000 tweets, and you want
to be able to select which one of these tweets includes hate speech, and
which one does not. For you to be able to carry out this research, and
be transparent about it as well, ensuring reproducibility of your work,
you need to conceptualise hate speech in a clear, detailed, and specific
way. So take a moment now, and working with someone near you,
conceptualise hate speech. Write down your conceptualisation of hate
speech - how you define hate speech for the purposes of your study. When
you have your concept ready, talk it over, and make sure that this is a
comprehensive definition that you are all happy with.

It's important that you play along and do this step. I'll wait here
while you discuss.

\includegraphics{https://media.giphy.com/media/3o6ZsUdQYvPAyT1hxm/giphy.gif}

OK, ready? Great, now let's test your conceptualisation of hate speech.
\emph{Relying strictly on the way that you conceptualised hate crime in
the steps above}, for each of the following tweets, record whether it
would be included in your study as exhibiting hate speech or not:

\includegraphics{http://www.pressgazette.co.uk/wp-content/uploads/2017/05/Katie-Hopkins-Western-Men-TWEET.jpg}

What about this one?

\includegraphics{https://ichef.bbci.co.uk/news/624/media/images/80000000/jpg/_80000045_sweaty.jpg}

And finally this third one?

\includegraphics{http://www.pressgazette.co.uk/wp-content/uploads/2017/05/hoops.jpg}

So would you categorise these tweets as hate speech according to your
criteria that you wrote out above? Why or why not? In fact, the included
tweets have all been considered hateful by various people (see
\href{http://www.pressgazette.co.uk/katie-hopkins-leaving-lbc-radio-effective-immediately-station-announces/}{here
for the first and last one}, and
\href{http://www.bbc.co.uk/news/uk-scotland-glasgow-west-30641705}{here
about the second one}). But when you are studying a concept as comlex as
hate speech, it's important that you can justify why or why not you
include something as hate speech.

Reading these tweets, using our human sentiment, we can appreciate that
the tweets are hateful. But how did they do against the way that you
conceptualised hate speech? Would they all be included in your study?
Did you include everything according to your criteria that you would
have liked to include, based on your human understanding of what hate
speech means?

If you did, that is very impressive. If you did not, don't despair, hate
speach is a very complex concept, and therefore tough to conceptualise
and define. There is no criminological consensus on the definition or
even the validity of the concept of hate crime. There is legislation in
this country to account for hate crime, and generally they reflect the
core belief that hate crime policy should support a basic human right to
be free from crime fuelled by hostility because of an individual's
personal characteristics, but even in legislation this is quite hard to
pin down.

One study looking at hate speech is
\href{https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-016-0072-6}{Burnap,
P., \& Williams, M. L. (2016). Us and them: identifying cyber hate on
Twitter across multiple protected characteristics. EPJ Data Science,
5(1), 1. Chicago}. They defined hate speech to include as the following:

\begin{quote}
``Text that is offensive or antagonistic in terms of race
ethnicity/sexual orientation/disability''
\end{quote}

So by these definitions they may include the above tweets. However, in
their conceptualisation of hate crime, they are explicit about not only
what is included, but what may be excluded from their concept: They
specifically not:

\begin{quote}
``Transgender cyber hate was not considered as part of the study. ``
\end{quote}

As you can see, conceptualising what you mean (and therefore also what
you don't mean) by hate crime a is very important step in your research,
as it can influence what data you collect, and therefore the outcomes of
your study, and the findings you'll be able to report. For the purposes
of a research study, it can influence who your results are relevant for.
Remember when we spoke about generalisability and validity in the first
week's feedback session? I'll return to these later, but it's important
to consider that the way that you conceptualise your concepts will
determine how much you can generalise from your results, or how valid
your measurements are. For example, did you include hate against all the
protected characteristics? Did you consider hate as only threats of
violence, or also general antagony as well? Conceptualisation is a very
important step, that affects the measurement we will discuss in the next
section, but that affects all the next steps in your study. How you
decide what to measure has implications for what sorts of conclusions
you can draw from your data after analysis.

\hypertarget{the-importance-of-conceptualisation}{%
\section{The importance of
conceptualisation}\label{the-importance-of-conceptualisation}}

You will have seen this in your preparatory reading, in Chapter 5
(``Research Design'') of the book `The Practice of Social Research' by
Earl Babbie, but I will quickly re-iterate here: based on your
conceptualisation of your variable, it will fall into one of three
categories:

\begin{itemize}
\tightlist
\item
  \emph{Directly observable}: something that you can observe through
  measuring or counting or physical observation. Examples include
  physical characteristics of a person.
\item
  \emph{Indirectly observable}: something that you can measure with a
  single question. Examples include the person's salary, or their
  ethnicity - you can just ask one question and this will be enough to
  identify the value for that variable for the person responding.
\item
  \emph{Constructs} : a more abstract concept, something that isn't
  necessarily ``out there'' to be measured using simple measurement
  tools. Constructs are often representative of complex social
  processes, and require more intricate measurement approaches. Examples
  include fear of crime, and trust in the police.
\end{itemize}

So our concept of hate speech, which one of these objectives does it
fall under? Can you measure it directly or indirectly with a simple
question? Or is it a more complex concept, that perhaps requires the
measurement of multiple indicators of hate, in order to build a fully
complete picture of what is hate speech?

Think about this for a moment. Turn to someone next to you and talk
through which one of these you think hate speech would fall under.
Discuss why you think this. Then read on, to see if you thought the
same.

So you've hopefully taken some time to formulate your own thoughts on
this topic, and now I can go on to discuss some approaches
conceptualising hate speech. First let's consult the legislation:

Not all hate incidents will amount to criminal offences, but those that
do become hate crimes. The Association of Chief Police Officers and the
CPS have agreed a common definition of hate crime: ``Any criminal
offence which is perceived by the victim or any other person, to be
motivated by hostility or prejudice based on a person's race or
perceived race; religion or perceived religion; sexual orientation or
perceived sexual orientation; disability or perceived disability and any
crime motivated by hostility or prejudice against a person who is
transgender or perceived to be transgender.''

This definition is quite an important one, because it has an effect on
sentencing, when it comes to criminal offences. If evidence can be
presented that the offence was motivated by hate, or for any other
strand of hate crime, the CPS can request enhanced sentencing. So this
seems a pretty important definition. But how can we translate this into
a concept of hate speech? How do we make sure that our concept
encapsulates all the possible versions of hate speech that we are
interested to study?

One approach could be to conceptualise hate speech as an
\textbf{indirectly observable} variable. You, could, as was done in this
paper looking at
\href{http://onlinelibrary.wiley.com/doi/10.1002/poi3.85/full}{Cyber
Hate Speech on Twitter} consider hate speech to be the extent to which
people consider a tweet offensive or antagonistic. In this case (we are
jumping slightly ahead into measurement, but it all link in any way),
this would be measured with a single-item question where you just
present people with a tweet, and ask ``is this text offensive or
antagonistic in terms of race ethnicity or religion?'', providing people
with a set of possible answers of yes, no, or undecided. (You might
notice that this study did \emph{not} include all protected
characteristics, mentioned in the CPS definition, instead they focus on
race ethnicity and religion only). In this particular study, they
presented the same tweets to many many people, and so possibly the
single-item measure could have worked as an indicator of hate, since
multiple people rating the same tweet would eventually cancel out people
who have a very high or very low threshold for classifying something as
offensive or antagonistic. In this case, hate speech is conceptualised
as people considering a tweet as antagonistic or offensive in terms of
race ethnicity or religion.

However if you were surveying individuals only about each tweet, and
were interested in what certain people class as hate speech or not hate
speech, you might want to consider a more complex assessment. You might
conceptualise something being hate speech as something that evokes a
variety of emotions from people and you might want to ask about all
these measures in separate questions, to make sure that you are really
tapping into what hate crime means to people. In this case, you would be
conceptualising hate speech as a \textbf{construct}. If you recall from
the reading (or above) \emph{constructs are theoretical creations tat
are based on observations, but that cannot be observed directly or
indirectly. Concepts such as compassion and prejudice are construcs
created from your conception of them, my conception of them, and the
conceptions of all those who have ever used these terms, They cannot be
observed directly or indirectly, because they don't exist. We made them
up.} Constructs are usually complex and not easily measured with a
single item. Instead we tend to approach them my measuring their many
indicators, and assembling the responses to those to create a measure
that is more reliable and less prone to random measurement errors than
single-item measures, since a single item often cannot discriminate
between fine degrees of an attribute. So if you have conceptualised hate
crime as something more abstract and complex, and therefore a construct,
that would mean you would have to measure in a different way, than the
example given from the paper above. In the next section we'll explore
exactly how conceptualisation affects measurement.

\hypertarget{measurement}{%
\section{Measurement}\label{measurement}}

\begin{quote}
In science, we use measurement to make accurate observations. All
measurement must begin with a classification process---a process that in
science is carried out according to systematic criteria. This process
implies that we can place units of scientific study in clearly defined
categories. The end result of classification is the development of
variables.
\end{quote}

\begin{itemize}
\tightlist
\item
  \emph{Chapter 2 Statistics in Criminal Justice - David Weisburd,
  Chester Britt}
\end{itemize}

The point of conceptualising your topics into concepts is to be able to
come up with the optimal approach to measuring them, in order to be able
to draw conclusions, and talk about criminological concepts with the
support of empirical data. Data that arises from measurement are
referred to as \textbf{empirical} data. So what can be empirical data?
Well it's anything that criminologists, data analysts, or anyone
interested in and carrying our research will measure, in order to be
able to answer their research questions, and be able to talk about their
topics of study.

\begin{quote}
Empirical data arise from our observations of the world. (\ldots{})
Among the many prevailing views of the role of empirical data in modern
science, there are two opposing extremes. On the one hand, the realist
assumes data are manifestations of latent phenomena. In this view, data
are pointers to universal, underlying truths. On the other hand, the
nominalist assumes data are what they describe. From this latter point
of view, as the philosopher Ludwig Feuerbach noted, ``you are what you
eat.'' We use the historical realist-nominalist terminology to emphasize
that these differing perspectives, in one form or another, have origins
in Medieval theological controversies and their classical antecedents in
Greek philosophy. Many working scientists adopt some form of a realist
position, particularly those in the natural sciences. Even social
scientists, despite the myriad particulars of their field, have often
endorsed a similar position.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

Depending on how you've conceptualised your topics you're interested in
will affect how you can measure them. To measure is a process that
involves observing and registering information to reflect qualities or
quantities of a particular concept of interest.

Think back to some of the studies you might be learning about in your
other courses. What sort of questions do they answer? How do they decide
how to measure the concepts that they are interested in? It's worth
going through some papers you might be interested in, in order to see
how they all go about these steps. You will normally find this sort of
detail in the \emph{methodology} section of a paper.

For example, in this paper by Tom R. Tyler about
\href{http://journals.sagepub.com/doi/abs/10.1177/1098611104271105}{ethnic
group differences in trust and confidence in the police} if you find the
\emph{method} section, you will see a brief discussion of the sample,
followed by a list of the concepts of interest (which will be the
variables in the analysis, if you skip ahead to results, you will see
this), and a description of how they were measured. For example these
are a few listed:

\begin{itemize}
\tightlist
\item
  \emph{Cooperation with the police}: People responded to three
  questions, which asked ``How likely would you be to call the police to
  report a crime that was occurring in your neighborhood?'' ``How likely
  would you be to help the police to find someone suspected of
  committing a crime by providing them with information?'' ``How likely
  would you be to report dangerous or suspicious activities in your
  neighborhood to the police?''
\item
  \emph{Cooperation with the community}: People responded to three
  questions, which asked the following: ``How likely would you be to
  volunteer your time on nights or weekends to help the police in your
  community?'' ``How likely would you be to volunteer your time on
  nights or weekends to patrol the streets as part of a neighborhood
  watch program?'' ``How likely would you be to volunteer your time on
  nights or weekends to attend a community meeting to discuss crime in
  your community?''
\end{itemize}

So how do we get to this step of measurement, from the conceptualisation
step?

Remember the points from above, and from the reading about how concepts
can be directly observable, indirectly observable, or constructs? Well
depending on what these are, will affect their operationalisation, or
how we can go about measuring them.

Exact sciences work with directly observable variables, such as people's
height, body temperature, heart rate, and so on. These are easy to
measure through direct observation. There are instances when variables
of interest in social sciences as well would fall into the category of
directly observable. Directly observable variables are those which you
can measure by observing. Can you think of any?

\includegraphics{https://media.giphy.com/media/e5nFlFChqFC0M/giphy.gif}

One example of directly observable variables would be the number of
people on the street, at any given time. This would be an important
variable to know if we are trying to accurately estimate crime risk.
Crime risk is calculated by dividing the number of crimes in an area by
a relevant measure of the population at risk. For example, you will
often hear about crimes per 100,000 population. This would be important
to calculate because sometimes a location can seem like it has very high
crime counts, but perhaps that's because there are a lot more people
there. Remember when we compared the number of crimes per borough in
Greater Manchester, and actually found that Manchester had significantly
more than anywhere else? Well Manchester has also a lot more people
going through it. So if we are trying to estimate \emph{risk} it's a
different question. So if there are two streets, both had 100 robberies
on them last year, but one street is oxford road, and the other is a
quiet side street with much fewer people passing by, then even thought
the count is the same, the risk to each individual is very different. So
to be able to calculate this, we would need to count the number of
people who walk down each street, to find an accurate measure of the
total possible people who \emph{could} become victims, and be able to
use this number to calculate risk. This is a \textbf{directly
observable} variable, and so the approach to measuring it can be
something like counting the number of people who walk down the street.

The next category is the \textbf{indirectly observable} variable. They
are ``terms whose application calls for relatively more subtle, complex,
or indirect observations, in which inferences play an acknowledged part.
Such inferences concern presumed connections, usually causal, between
what is directly observed and what the term signifies''
\href{https://books.google.co.uk/books?id=wxwuDwAAQBAJ\&pg=PT89\&lpg=PT89\&dq=\%22terms+whose+application+calls+for+relatively+more+subtle,+complex,+or+indirect+observations,+in+which+inferences+play+an+acknowledged+part.+Such+inferences+concern+presumed+connections,+usually+causal,+between+what+is+directly+observed+and+what+the+term+signifies\%22\&source=bl\&ots=0_ySczx0oG\&sig=WVdwNE7mUzF_d8dfBk2R_Tq4clw\&hl=en\&sa=X\&ved=0ahUKEwihna-v5OrWAhXMYVAKHXvEBpIQ6AEIJjAA\#v=onepage\&q=\%22terms\%20whose\%20application\%20calls\%20for\%20relatively\%20more\%20subtle\%2C\%20complex\%2C\%20or\%20indirect\%20observations\%2C\%20in\%20which\%20inferences\%20play\%20an\%20acknowledged\%20part.\%20Such\%20inferences\%20concern\%20presumed\%20connections\%2C\%20usually\%20causal\%2C\%20between\%20what\%20is\%20directly\%20observed\%20and\%20what\%20the\%20term\%20signifies\%22\&f=false}{Kaplan,
A. (1964). The conduct of inquiry: Methodology for behavioral science.
San Francisco, CA: Chandler Publishing Company, p.~55.}. If we conducted
a study for which we wished to know a person's income, we'd probably
have to ask them their income, perhaps in an interview or a survey. Thus
we have observed income, even if it has only been observed indirectly.
Birthplace might be another indirect observable. We can ask study
participants where they were born, but chances are good we won't have
directly observed any of those people being born in the locations they
report. The way that you would measure these concepts is usually through
single-item questionnaires. What does this mean? Single-item just means
that you ask one single question , and the answer given to that one
question is sufficient to measure your concept.

The next category is where it gets a bit more complicated, but this is
where the beauty of social science measurement really comes to life.
Because we are interested in complex behaviours, interactions,
relationships, and perceptions and opinions, our concepts in social
sciences are often too abstract to be approaches through direct or
indirect observation. Consider the example from the paper on ethnicity
and trust in the police linked above. You can see that each one of those
concepts is measured by people's answers to multiple questions, which
all come together to indicate the concept. These sort of complex
concepts, that require such measurements are our last category, the
\textbf{constructs}. Constructs such as \emph{cooperation with the
police} or \emph{cooperation with the community} are more abstract than
either observational terms or indirect observables, but we can detect
them based on the observation of some collection of observables. Let's
explore how this works, in the section below.

\hypertarget{measuring-constructs-with-composite-variables}{%
\subsection{Measuring constructs with composite
variables}\label{measuring-constructs-with-composite-variables}}

As you see above, \textbf{constructs} are complex variables. As
researchers we try to measure our constructs as best as we can. Often we
can see and measure indicators of the constructs, but we can not
directly observe or measure the constructs themselves. Instead we infer
these constructs, which are unobserved, hidden, or latent, from the data
we collect on \emph{related variables} which we \emph{can} observe and
directly measure.

We can combine the results from these related variables into a
\textbf{composite variable}. A \textbf{composite variable} is a variable
created by combining two or more individual variables, called
indicators, into a single variable. Each indicator alone doesn't provide
sufficient information, but altogether they can represent the more
complex concept. Think of the indicators as pieces of a puzzle that must
be fit together to see the big picture.

A lot of work goes into creating composite variables. The indicators of
the multidimensional concept must be specified. It's important that each
indicator contribute unique information to the final score. The formula
for combining the indicators into a single score, called aggregating
data, must be established. The computation involved will depend on the
type of data that is being aggregated. To aggregate the data, raw scores
might be summed, averaged, transformed, and/or weighted.

One example of a composite measure of health is the Body Mass Index. The
BMI was developed by Lambert A. J. Quetelet in the early 19th century as
a means for assessing the degree of obesity of the general population.
The indicators are height, weight, and age.

A person's BMI is calculated like this: Multiply height in inches by
itself (i.e., if x = height, find x\^{}2). Next you would divide the
same person's weight in lbs by the first number (the square of his or
her height). Last, you would multiply that answer (the quotient) by 705.
The result is a person's final score for the composite variable, BMI.
Based on this score, we can determine a person's relative degree of
obesity - a component of health.

Methods of simple averaging, weighted averaging, and meaningful grouping
can all be used to create composite variables. Different methods are
more appropriate in different contexts. Each approach to creating
composite variables has advantages and disadvantages that researchers
should weigh carefully.

There are several ways to accomplish averaging. The specific method of
averaging can be chosen based on whether the original variables intended
to be combined are \emph{numeric} or \emph{categorical}. When original
variables are \emph{numeric}, simple averaging can be considered. I say
simple averaging, but nothing's ever truly \emph{simple} is it? Let me
briefly touch on this complication:

When you are working with numbers, the unit of measurement can influence
how big or small these numbers are. Consider height for example. If I
measure your height in centimetres, milimeters, or inches, I will get
very different numbers. But if I measured everyone's heights in the
class, and plotted them, no matter what unit I used (\emph{cm},
\emph{mm}, or \emph{in}) I would end up with the sample plot right? You
might be taller (or shorter) than the person sat next to you, and no
matter what unit I use to measure your height, this will stay true.

Similarly, where your height sits in comparison to the average height in
the class with stay too. Remember how we measured the distribution of
data points around the mean in the past weeks? We used the
\emph{standard deviation}, right? Standard deviation was something we
used to indicate the distribution of the individual points we measured
around the mean. And above we discussed how to calculate the standard
deviation for the population, and how that's different to the standard
deviation for the sample. The standard deviation is a tool that we can
use, in order to be able to standardize our numeric measurements. We can
do this using \textbf{z-scores}.

\emph{What is a z-score???} Simply put, a *z-score** is the number of
standard deviations from the mean a data point is. But more technically
it's a measure of how many standard deviations below or above the
population mean a raw score is. A z-score is also known as a standard
score and it can be placed on a normal distribution curve. Z-scores
range from -3 standard deviations (which would fall to the far left of
the normal distribution curve) up to +3 standard deviations (which would
fall to the far right of the normal distribution curve). However you can
get z-scores larger (or smaller) than 3, in the case of outliers, which
are data points that are very far away from the mean. So don't worry
when you get z-scores outside this range. In order to use a z-score, you
need to know the mean μ and also the population standard deviation σ.

Z-scores are a way to compare results from a test to a ``normal''
population. Results from tests or surveys have thousands of possible
results and units. However, those results can often seem meaningless.
For example, knowing that someone's weight is 150 pounds might be good
information, but if you want to compare it to the ``average'' person's
weight, looking at a vast table of data can be overwhelming (especially
if some weights are recorded in kilograms). A z-score can tell you where
that person's weight is compared to the average population's mean
weight.

Let's have a go at calculating z-scores. The basic z score formula for a
sample is:

\texttt{z\ =\ (x\ –\ μ)\ /\ σ}

For example, let's say you are 180 cm tall. Given our mean (μ) of 172.1
and a calculating a standard deviation (σ, in the case of our heights
data, it's 5), we can calculate the z score.

But first, to calculate the z-score, our data needs to meet some
assumptions. First it needs to be a numeric variable. To be fair though,
in order to calculate a mean and a standard deviation in the first
place, we needed this variable to be numeric. But the other assumption
it makes is that this numeric variable follows a \textbf{normal
distribution}. You might remember this concept from the very first week,
when you watched the
\href{http://www.gapminder.org/videos/the-joy-of-stats/}{joy of stats}
video by Hans Rosling. If you didn't watch it, well you definitely
should! But in any case, here's a quick video to re-cap normal
distribution to you, this time by Chris Wilde:

You can identify a normal distribution, because it follows a normal
distribution curve, also called a \emph{bell curve}:

\includegraphics{https://www.mathsisfun.com/data/images/normal-distribution-1.svg}

So how do you test if your numeric variable is normally distributed?
Well, for now we can simply decide this by plotting it in a histogram,
and see if it (roughly) looks like it follows a bell-curve shape or not.
So let's build a histogram of our height variable. Do you remember how
to do this? If not, look back to our week of univariate analysis. You
will need 3 things, your data, the data analysis toolpak, and a list of
bins. You can choose whatever bins you like, I went for 5 cm intervals.
My histogram looks like this:

\includegraphics{imgs/histo.png}

Looks like to me this follows our normal distribution curve. There are
also numeric ways that test whether our data follows a normal
distribution or not, and also ways that you can manipulate the data to
make it follow a normal distribution curve, but we don't cover those
just yet. For now we will rely on our eyeball measure - does this
\emph{look} like it follows a bell curve? I think it does. So now that
we know that our numeric variable of height meets the assumptions that
it's numeric, and also it follows a normal distribution, we can
calculate z-scores for each individual height.

To calculate the z-score, we just need to subtract the mean from the
score, and then divide the result by the standard deviation:

\texttt{z\ =\ (score\ -\ mean)\ /\ standard\ deviation}

\texttt{z\ =\ (x\ –\ μ)\ /\ σ}

\texttt{z\ =\ (180\ –\ 172.1)\ /\ 5}

\texttt{z\ =\ 1.58}

So the z-score for the individual data point (ie: person) who's height
is 180cm tall, is 1.58. That means that they are 1.58 standard
deviations from the mean. If we think back to the bell curve, they sit
somewhere here:

\includegraphics{imgs/your_height_here.png}

\hypertarget{activity-3-computing-a-z-score-in-excel}{%
\subsection{Activity 3: Computing a z-score in
Excel}\label{activity-3-computing-a-z-score-in-excel}}

To compute the z-score for each person in excel, you simply have to turn
the formula above into excel formula language. We know that all our
variables are in the ``height'' column, which is column ``B''. So we
need to get the values for each cell, then the (population) standard
deviation for height, and the mean for height. So for example, for the
first row (row 2, since row 1 contains our column headers, or variable
names) your cell is B2. In this case you would translate the formula as
follows:

\texttt{z\ =\ (score\ -\ mean)\ /\ standard\ deviation}

\texttt{z\ =\ (x\ –\ μ)\ /\ σ}

\texttt{=(B2-AVERAGE(B:B))/STDEVP(B:B)}

If any one of these steps doesn't quite make sense, raise your hand now,
and ask us to explain!

Now create a column called ``z-score'', and populate it with the
z-scores of each individual's height, using the formula above. You
should end up with something like this:

\includegraphics{imgs/z-score-col.png}

If you had multiple variables, for example we also had measurement for
these people for the length of their arms and legs and so on, we could
combine all the z-scores into a composite variable, that would be able
to tell us about that person's ``tallness'' is that was a variable we
conceptualised as made up of these factors, and operationalised with
this composite measure.

\hypertarget{activity-4-twitter-data-revisited}{%
\subsection{Activity 4: Twitter data
revisited}\label{activity-4-twitter-data-revisited}}

Let's demonstrate with some actual data, that is relevant to
criminology. Let's suppose that we are interested in the popularity of
tweets sent out by greater manchester police, city centre division. How
would we go about measuring popularity? Well it depends on how we
conceptualise this. But let's say that we conceptualise the popularity
of a tweet, but considering the amount of responses it was getting from
the public, both in favourites, in retweets, and in comments. Let's grab
a large-ish sample of GMP tweets. Don't worry, I won't make you manually
input these again. Instead, you can just download some tweets that I've
collected using the \href{https://developer.twitter.com/en/docs}{twitter
API}. You can download this data from Blackboard.

Now if you open it up in excel you will see a column for unique ID to
identify each tweet, one for the date created which represents when the
tweet was written, one for the text of the tweet, then 3 numeric
columns, one for the number of favourites, one for the number of
retweets, and one for the number of replies. Finally you have a column
for the hashtags used by GMP in that tweet.

So let's say we want to be able to talk about the popularity of these
tweets. As discussed, we conceptualise the popularity as having to do
with favourites, retweets and responses. We believe that these three
dimensions quantify the engagement that each individual tweet is
getting. So then, what we can do is use these three \textbf{indicators}
to create a \textbf{composite variable} that takes into account all
three directly observable measures to produce an estimate for our
construct of popularity of tweet.

To do this, we have to first convert each column to a z-score, rather
than just the count, and then we have to take the average of the
z-scores.

So let's first create 3 columns, one for each z-score:

\includegraphics{imgs/z-cols_created.png}

Now remember our equation. For each cell, we need to subtract the mean,
and then from that divide by the standard deviation, for the appropriate
column. So for us to make a z-score for favourites, we must for each
number of favourites, subtract the mean number of favourites, and divide
by the standard deviation number of favourites. Like so:

\includegraphics{imgs/fave_z.png}

When you are done, you should have three columns for z-scores, one for
favourites, one for retweets, and one for replies.

\includegraphics{imgs/3_z_cols.png}

Now you can use these z-scores to calculate an average between them,
which will give you your measure for your composite variable. To take
the average of 3 values, you can just use the \texttt{=AVERAGE()}
formula, and highlight the columns from which you want to use the values
to calculate the average from, in this case the new z-score values
you've just created:

\includegraphics{imgs/calc_popul.png}

Now, finally, you have a popularity score, that tells you which tweets
by GMP city centre were most popular, depending on the number of
favourites, retweets, and replies, taking into account all these three
dimensions that we decided are important indicators of our construct,
popularity. Hint: sort your data using the sort option by the popularity
variable, from largest to smallest (or in descending order).

So, based on this measure, which tweet is the most popular?

It appears that the most popular tweet GMP has made is a pun\ldots{} :

\emph{``Several shoplifters detained today, including woman who stole a
vibrator - Why do they do it - is it just for the buzz\ldots{}'''}

Very good GMP, very good. But something really interesting as well, if
you look at a few of the tweets below, that show up as quite highly
ranked based on our popularity score, it comes up as high on retweets,
but earning 0 favourites! See:

\includegraphics{imgs/low_fave_high_rt.png}

This should illustrate the importance of measuring something as a
construct. We could have conceptualised popularity of GMP's tweets as an
indirectly observable concept, which we could then operationalise as
``number of favourites per tweet''. However, this would be a
one-dimensional measure of our concept. Instead, by choosing to
conceptualise popularity as a construct, made up of favourites,
retweets, and replies, we create a composite variable, that reflects
more the nuances of our concept.

In fact most of the things we study in the social sciences, and in
criminology, are very complex social processes and often best
represented by constructs, which can be measured this way through
composite variables.

\hypertarget{composite-variables-created-from-categorical-variables}{%
\section{Composite variables created from categorical
variables}\label{composite-variables-created-from-categorical-variables}}

Another thing to mention regarding measurement in social sciences, and
criminology, is that often times our data come from surveys. And in
surveys you don't only deal with numeric variables, but indeed with
categorical as well. And it is in these surveys that we can ask people
some questions about the really nuanced and complex issues, such as
trust in the police, or fear of crime. Of course the responses to such
questions tend to be categorical, often ordinal, asking people to rank
their agreement with various statements. So we should also take time to
consider the other case, where your indicator variables that you wish to
combine into a composite measure are \emph{categorical}. These are most
often made up of multi-item scales, which we'll move on to in the next
section.

Meaningful grouping is the nonstatistical combination of selected
original variables based on the interpretation of the variables' values
or scores, guided by the science of the field. Meaningful grouping can
be used to create a composite outcome variable from multiple continuous
or categorical variables or both. These original variables, when
combined into a composite, can indicate an attribute (e.g., high risk
for mortality) that is meaningful. A composite variable created by
meaningful grouping is often categorical. For example, a composite
variable may include categories improved, no change, and worse to
indicate the direction of overall change from baseline, to determine
whether or not an intervention was efficacious. The key point is that
the composite should be meaningful with respect to the context and
purpose of the study and should be determined based on the science of
the field, with a predefined algorithm.

Three common composite measures include:

\begin{itemize}
\tightlist
\item
  \textbf{indexes} - measures that summarize and rank specific
  observations, usually on the ordinal scale
\item
  \textbf{scales} - advanced indexes whose observations are further
  transformed (scaled) due to their logical or empirical relationships
\item
  \textbf{typologies} - measures that classify observations in terms of
  their attributes on multiple variables, usually on a nominal scale
\end{itemize}

Here we will consider scales for creating composite variables from
numeric indicators, and indexes for creating composite variables from
categorical indicators.

\hypertarget{multi-item-scales-for-categorical-constructs}{%
\subsection{Multi-item scales for categorical
constructs}\label{multi-item-scales-for-categorical-constructs}}

A lot of criminological research uses multi-item scales to measure
constructs. For example we saw in the paper earlier how the constructs
of \emph{cooperation with the police} or \emph{cooperation with the
community} were measured through asking various questions which were
compiled together into these variables.

One primary technique for measuring concepts important for theory
development is the use of multi-item scales. In many cases, single-item
questions pertaining to a construct are not reliable and should not be
used in drawing conclusions. There have been examination of the
performance of single-item questions versus multi-item scaled in terms
of reliability, and
\href{https://scholarworks.iupui.edu/handle/1805/344}{By comparing the
reliability of a summated, multi-item scale versus a single-item
question, the authors show how unreliable a single item is; and
therefore it is not appropriate to make inferences based upon the
analysis of single-item questions which are used in measuring a
construct}.

Oftentimes information gathered in the social sciences, including
criminology, will make use of Likert-type scales. Rensis Likert was one
of the researchers who worked in a systematized way with this type of
variable. The Likert methodology is one of most used in many fields of
social sciences, and even health sciences and medicine. When responding
to a Likert item, respondents specify their level of agreement or
disagreement on a symmetric agree-disagree scale for a series of
statements. Thus, the range captures the intensity of their feelings for
a given item.

You will have definitely seen Likert scales before, but you might have
just not known they were called as such. Here is one example:

\includegraphics{http://hosted.jalt.org/test/Graphics/bro34.gif}

So in sum a Likert scale must have:

\begin{itemize}
\tightlist
\item
  a set of items, composed of approximately an equal number of favorable
  and unfavorable statements concerning the attitude object
\item
  for each item, respondents select one of five responses: strongly
  agree, agree, undecided, disagree, or strongly disagree.\\
\item
  the specific responses to the items combined so that individuals with
  the most favorable attitudes will have the highest scores while
  individuals with the least favorable (or unfavorable) attitudes will
  have the lowest scores
\end{itemize}

After studies of reliability and analysis of different items, Rensis
Likert suggested that attitude, behavior, or other variables measured
could be a result of the sum of values of eligible items, which is
something referred to as \textbf{summated scales}. While not all
summated scales are created according to Likert's specific procedures,
all such scales share the basic logic associated with Likert scaling
described in the steps above.

In general, constructs are best measured using multi-item scales. Since
they are usually complex, they are not easily measured with a single
item. Multi-item scales are usually more reliable and less prone to
random measurement errors than single-item measures, as a single item
often cannot discriminate between fine degrees of an attribute.

Creating multi-item scales is associated with test results for validity
and reliability with respect to each scale are disclosed.

One important reason for constructing multi-item scales, as opposed to
single-item measurements, is that the nature of the multiple items
permits us to validate the consistency of the scales. For example, if
all the items that belong to one multi-item scale are expected to be
correlated and behave in a similar manner to each other, rogue items
that do not reflect the investigator's intended construct can be
detected. With single items, validation possibilities are far more
restricted.

\begin{quote}
Under most conditions typically encountered in practical applications,
multi-item scales clearly outperform single items in terms of predictive
validity. Only under very specific conditions do single items perform
equally well as multi-item scales. Therefore, the use of single-item
measures in empirical research should be approached with caution, and
the use of such measures should be limited to special circumstances.
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{https://link.springer.com/article/10.1007/s11747-011-0300-3}{Guidelines
  for choosing between multi-item and single-item scales for construct
  measurement: a predictive validity perspective}
\end{itemize}

So how do you make one of these? Well you have to consider many factors,
and consult both theory, and analysis in order to make sure that your
measurement covers everything you need to be able to talk about the
construct it is meant to represent. You need theory to identify the
indicators of the construct, which you will need to include, as your
items that make up the multi-item scale. So let us first consider the
construct of feeling unsafe in one's neighbourhood. We can conceptualise
this as people feeling unsafe in their neighbourhoods in three separate
settings. They can feel unsafe walking around in the daytime. They can
feel unsafe walking around after dark. And they can also feel unsafe in
their homes. We can consider a 3-item Likert scale, with responses that
range from Very safe to Fairly safe to A bit unsafe to Very unsafe. This
is the example we'll use throughout this lab. So, download this data set
from Blackboard (it's under csew\_small.xlsx). You can see that there
are the three variables, of walking in day, walking after dark, and
feeling safe in home alone. I have included responses from 10 people, so
that we can use this to assess reliability and validity of the measures.

Multi-item scales open up a whole range of techniques for construct
validity. For multi-item scales comprised of itemswith discrete response
choices, reliability is most commonly assessed using Cronbach's
coefficient alpha, but I'm jumping ahead, we will now explore further
the validity and reliability of measures, and how you can test this in
the next section.

\hypertarget{a-note-on-validity-and-reliability}{%
\section{A note on Validity and
Reliability}\label{a-note-on-validity-and-reliability}}

All measurements should satisfy basic properties if they are to be
useful in helping researchers draw meaningful conclusions about the
world around us. These are primarily validity, reliability,
repeatability, sensitivity and responsiveness. We have, briefly, touched
on reliability in the first session, and these two are the main concepts
which we will cover today as well. but I wanted to mention the others as
well, so that you have a complete picture of the expectations your
variables need to be able to meet in order to be robust and reliable
when used to describe the world of criminology.

\textbf{Validation} is the process of determining whether there are
grounds for believing that the instrument measures what it is intended
to measure. For example, to what extent is it reasonable to claim that a
`fear of crime questionnaire' really is assessing someone's fear of
crime? Since we are attempting to measure an ill-defined and
unobservable construct, we can only infer that the instrument is valid
in so far as it correlates with other observable behaviour. Validity can
be sub-divided into:

\begin{itemize}
\tightlist
\item
  \textbf{Content validity}: is the measurement sensible? Do the
  indicators they reflect the intended construct?
\item
  \textbf{Criterion validity}: is the measurement associated with the
  external criteria, for example other measurements of the same
  construct?
\item
  \textbf{Construct validity}: what is the relationship of the
  indicators to one another? And to the construct it is intended to
  measure? Construct validity has two sub-types:

  \begin{itemize}
  \tightlist
  \item
    \textbf{convergent validity}: extent to which indicators are
    associated with one-another (they measure the same thing)
  \item
    \textbf{divergent validity}: extent to which indicators differ (they
    measure distinct things)
  \end{itemize}
\end{itemize}

\textbf{Reliability} and \textbf{repeatability} attempt to address the
variability associated with the measure. You want to have a measure that
repeatedly produces the same results when administered in the same
circumstances, and that any differences in answers between people is the
result of their differing attitudes on the construct which you are
measuring, rather than due to any variation introduced by the
measurement itself.

The below image might help you conceptualise reliability and validity.
Reliability refers to getting consistent results each time you measure
your concept. Validity refers to the extent to which your measurement of
the concept actually reflects the concept itself.

\begin{quote}
Validity answers the question, ``Am I measuring what I think I am?'' In
shooting terms, this is ``accuracy.'' My shots may or may not be loosely
clustered, but they're all relatively close to the bull's-eye.
Reliability answer the question, ``Am I consistent?'' In shooting terms,
this is ``precision.'' My shots may or may not be relatively close to
the bull's-eye, but they're tightly clustered. This leads us to four
possible outcomes as illustrated below.
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{http://www.socingoutloud.com/2012/10/teaching-validity-and-reliability-in.html}{Bradley
  A. Koch}
\end{itemize}

So you can have a reliable but not valid measure, or a valid but not
reliable measure. Imagine it like this:

\includegraphics{http://4.bp.blogspot.com/-Px8qzh8Umy0/UHxrSqVczCI/AAAAAAAABLE/oRAbLSGTQUM/s1600/bullseye.jpg}

\begin{quote}
The worst-case scenario, when we have low validity and low reliability
(lower left), looks like buckshot, scattered all over the target. We are
neither accurate nor precise. We're not measuring what we think we are,
and at that even, we're doing it inconsistently. When we have high
validity but low reliability (upper left), our packing may be loose, but
the shots are near the bull's-eye. We are accurate but not precise.
We're likely measuring what we think we are, just not consistently. When
we have high reliability but low validity (upper right), we may be off
of the bull's-eye, but our packing is tight. We are precise but not
accurate. We're not measuring what we think we are, but whatever we're
measuring, we're doing so consistently. The best-case scenario, high
validity and high reliability (lower right), is when the shots are
clustered on the bull's-eye. We are both accurate and precise. In other
words, our question/variable consistently measures the intended concept.
\end{quote}

-\href{http://www.socingoutloud.com/2012/10/teaching-validity-and-reliability-in.html}{Bradley
A. Koch}

\textbf{Sensitivity} is the ability of measurements to detect
differences between people or groups of people. Your measurement
approach should be sensitive enough to detect differences between people
for example those who have differing levels of worry about crime.

\textbf{Responsiveness} is similar to sensitivity, but relates to the
ability to detect changes when a person fluctuates in the construct
being measured. If your measure is meant to detect change over time, for
example in fear of crime, it needs to be able to detect changes. A
sensitive measurement is usually, but not necessarily, also responsive
to changes.

These are the main criteria that your measurements need to meet, in
order to be considered robust. Pretty daunting, eh? No wonder secondary
data analysis is so popular! (Is it though? Well it should be. It is
amongst lazy people like myself, who would much rather acquire data and
make pretty graphs, than design multi-scale items for measuring complex
constructs in the first place\ldots{}.!) It is much nicer when someone
has gone through the hard work of designing great measures for us, and
we can use the excellent data provided. And also, we should appreciate
the work that these people have put in. There are many initiatives to
try to encourage the rest of us to use the data they've collected.
Organisations such as the \href{https://www.ukdataservice.ac.uk/}{UK
Data Service} (UKDS) collate all sorts of data, and make them easily
available and ready for us to use, to make these data more enticing.
They also like to reward analysis of their data. For example, next year
you will have the option to take a \textbf{secondary data analysis
pathway} for your dissertation. If you do this, and you make use of a
data set from the UKDS, you can enter to win the
\href{https://www.ukdataservice.ac.uk/use-data/student-resources/dissertation-prize}{dissertation
prize}! It's a pretty awesome thing to have on your CV, and it also
comes with a cash prize of £500 for first place. Not bad\ldots{}! And we
have
\href{https://www.ukdataservice.ac.uk/news-and-events/newsitem/?id=4650}{already
had a BA Crim student win from Manchester before}, so it's definitely an
attainable goal with your training and skills!

But as I was saying, building reliable and valid measurement is tough
work. People at the Office of National Statistics work tirelessly to
come up with multi-item scales to measure fear of crime, trust in the
police, and similar constructs using the Crime Survey for England and
Wales. For them to introduce a new construct, and some new questions
takes, \emph{literally} years of work. So it's definitely a good
approach to try to make use of the data which they collect. If you are
interested, have a watch of
\href{https://www.youtube.com/watch?v=cCpHnqn0Q2c\&list=PLG87Imnep1SljSqc0yLIHYBP1w0saMJn-\&index=2}{this
video on what sorts of data they have}, and
\href{https://www.youtube.com/watch?list=PLG87Imnep1SljSqc0yLIHYBP1w0saMJn-\&v=TzRWJK1MtrU}{this
other video on how to acquire data}.

But if I've still not convinced you, and you want to go out and collect
your own data, by taking the topics that you are interested in and
conceptualising and operationalising them your way, and you want to
develop your own multi-item scales, and then test their validity and
reliability, well then, I guess I should equip you with some basic
abilities to know how. You might get asked to create a questionnaire in
your next workplace. While creating questionnaires is very tough and
nuanced job, and I could write a whole course on only that, replying
this to your line manager, when they ask you to build a survey will not
win you many brownie points. Instead it's best that you make at least
some sort of attempt to ensure that your constructs are being measured
by the most valid and reliable measures possible, or at least you can
have some indication as to the error produced by your measurement. Later
in this lab we will address this through something called Cronbach's
alpha. But first, let's calculate a summative score for ``feelings of
unsafety''.

\hypertarget{activity-5-calculating-a-summative-score}{%
\subsection{Activity 5: Calculating a summative
score}\label{activity-5-calculating-a-summative-score}}

So back to our data about collecting people's feelings of unsafety, in
their neighbourhoods. We have our three indicator questions:

\begin{itemize}
\tightlist
\item
  To what extent do you feel safe walking alone in the day?
\item
  To what extent do you feel safe walking alone after dark?
\item
  To what extent do you feel safe at home alone at night?
\end{itemize}

Well how can we tell if these measures are any good? One approach is to
calculate Cronbach's alpha. There is some debate around what this
approach can tell you about your questions, but Cronbach's alpha is used
frequently as a measure in constructing a summary scale for Likert type
questions or opinions. The Cronbach's alpha gives a measure of the
internal consistency or reliability of such a multi-item scale. It
essentially considers the variation in one person's answers between the
many indicators of the construct. If they are all supposed to measure
the same thing, then you should have little variation, and therefore get
a high Cronbach's alpha. If on the other hand you have high variation,
and get a low Cronbach's alpha, that might mean that your indicators are
not so inter-linked as you had imagined, and perhaps measure distinct
things, rather than elements of the same construct.

 So let's give it a go. Open up your data (csew\_small.xlsx) and have a
look at the answers people provide. Does it look consistent, from
question to question? What are your initial thoughts?

Well now we can consider calculating the variation in answers, to be
able to support (or challenge) your perceptions with data. But the first
step to this is to \textbf{re-code} your variables from the text value
(very worried, etc) to a number that indicates the \textbf{rank} of the
ordinal response. How we do this is important. While the order of an
ordinal variable is fixed, whether you go from high to low or low to
high score is not. How do you determine what your score should be? Well
on the first instance, I would suggest that you think about what your
construct is. In this case, our construct is fear - or unsafety. In this
case, it would make sense for a higher score to represent \emph{more
fear}. For the scores to reflect this, lower scores of worry should have
lower numbers, and higher scores of worry should have higher numbers. So
``very safe'' should be equal to 1, and ``very unsafe'' should be equal
to 4.

Once we've decided on a scoring system like this, we can write it down
in a little table. This is both for ourselves, so we know what the
numbers mean later (and maybe also for anyone else using our data
afterwards), but also so that we can use this as a lookup table for our
re-coding.

So create a mini table in your data, that has your codes and values.
Something like this:

\includegraphics{imgs/lookup_table.png}

Now also create headings for 3 new columns, one for each of our
indicator variables:

\includegraphics{imgs/num_cols_lookup.png}

So now we can recode these variables into these new columns, where we
will see the numeric value for each text value. We could do this
manually. But we don't want to. Remember we are lazy. And also remember
that this is a very small subset of the actual Crime Survey for England
and Wales (CSEW) data. The actual data has thousands of rows. 46,031 in
the 2011/12 sweep to be exact. So that is not something that you would
want to do manually, right? No.

Instead we will learn a new function in excel, one which you will often
use, called \texttt{VLOOKUP()}. What this function does, is it uses a
lookup table, in order to assign variables to a new column. In this
case, our lookup table is the one on the bottom there, that we created,
that tells excel that for us, Very safe is 1, and Fairly safe is 2, and
A bit unsafe is 3, and Very unsafe is 4. We can use this, in order to
create our new, re-coded variables.

The \texttt{VLOOKUP()} function takes 4 parameters. You have to tell it
the \textbf{lookup\_value} - the value to search for. This can be either
a value (number, date or text) or a cell reference (reference to a cell
containing a lookup value), or the value returned by some other Excel
function. For example:

\begin{verbatim}
Look up for number: =VLOOKUP(40, A2:B15, 2) - the formula will search for the number 40.
\end{verbatim}

You then have to tell it the \textbf{table\_array} - which is your
lookup table, with two columns of data. The VLOOKUP function
\emph{always} searches for the lookup value in the first column of
table\_array. Your table\_array may contain various values such as text,
dates, numbers, or logical values. Values are case-insensitive, meaning
that uppercase and lowercase text are treated as identical.

\begin{verbatim}
So, our formula =VLOOKUP(40, A2:B15,2) will search for "40" in cells A2 to A15 because A is the first column of the table_array A2:B15. 
\end{verbatim}

You then have to tell it the \textbf{col\_index\_num} - the column
number in table\_array from which the value in the corresponding row
should be returned. The left-most column in the specified table\_array
is 1, the second column is 2, the third column is 3, and so on. Well,
now you can read the entire formula =VLOOKUP(40, A2:B15,2). The formula
searches for ``40'' in cells A2 through A15 and returns a matching value
from column B (because B is the 2nd column in the specified table\_array
A2:B15).

But finally, you still have to specify the \textbf{range\_lookup}. This
determines whether you are looking for an exact match (when you set to
FALSE) or approximate match (when you set to TRUE or you omit it). This
final parameter is optional but very important. In this case, we want an
exact match, so we will set this parameter to ``FALSE''.

So, to look up the value for the cell B2, in the lookup table that
ranges from \$E\(20:\)F\$23 (remember to include dollar signs to make
sure nothing changes when you copy an paste), find the values in the 2nd
column of our lookup table, and return only exact matches, we can put
the below formula into the first cell for our Walkdark\_num variable:

\texttt{=VLOOKUP(B2,\$E\$20:\$F\$23,2,\ FALSE)}

To return this:

\includegraphics{imgs/first_lookup.png}

We can quickly verify that this looks right, since the value there in
the `Walkdark' column is ``very safe'', we know that this should
numerically return ``1''. And it does, so we are happy. Now copy the
formula down the column, and also repeat for the other two variables as
well. You should finally end up with a table like this:

\includegraphics{imgs/final_lookup.png}

Now you have your numeric values for the indicators of the constrcut. As
this is a summative score, it just means that the score is the result of
the \textbf{sum} of the indicators. So to calculate the value for our
composite variable, we need to calculate the total score people
achieved, by summing their answers to all three questions. I can do this
by creating a new ``total'' column, and populating it with the sums
using the \texttt{SUM()} function:

\includegraphics{imgs/tot_sum_alpha.png}

And the values in this column represent the score that these people have
on the composite variable ``feelings of unsafety''. That's it, now
you've created this summative score. Woohoo! You can see that the person
with the highest score has achieved a score of 10 - this is the hightest
score on feeling unsafe in this mini sample. On the other hand, the
person with the lowest score are actually two people who achieved a
score of 3. They score low on feeling unsafe.

But to what extent do these measures all indicate the same concept? Is
someone who feels the same towards their safety going to answer
similarly on all onf these questions? We want to try to find this out,
in order to be able to speak about the our measure confidently. And as
mentioned earlier, you can put a number to the extent to which you can
rely on your measures, using Cronbach alpha.

\hypertarget{activity-6-testing-your-questions-cronbachs-alpha}{%
\subsection{Activity 6: Testing your questions: Cronbach's
alpha}\label{activity-6-testing-your-questions-cronbachs-alpha}}

Cronbach's alpha is a measure of internal consistency, that is, how
closely related a set of items are as a group. It is considered to be a
measure of scale reliability. Cronbach's alpha is a measure used to
assess the reliability, or internal consistency, of a set of scale or
test items. In other words, the reliability of any given measurement
refers to the extent to which it is a consistent measure of a concept,
and Cronbach's alpha is one way of measuring the strength of that
consistency.

Cronbach's alpha can be written as a function of the number of test
items and the average inter-correlation among the items.

\texttt{(number\ of\ items/(number\ of\ items-1))*(1\ -\ the\ variance\ associated\ with\ each\ item/\ the\ variance\ associated\ with\ the\ observed\ total\ scores)}

 With this in mind, we can apply this formula to excel, and use excel to
calculate our Cronbach alpha. As I mentioned, Cronbach alpha has to do
with variance across the different indicators for the same construct, in
this case the three questions about feeling unsafe, within each
individual person's answers. Remember that each row is one person, so in
each row you have the answers to the questions from the same individual.

So to do this, you first need some values:

\includegraphics{https://i0.wp.com/www.real-statistics.com/wp-content/uploads/2013/02/Picture11.png}

I'll go through each of these but, k is the number of indicators, sigma
(sum of) theta\^{}2 is the sum of the variance between people's answers,
and theta\^{}2 is the population variance. Alpha is the end result, that
you're calculating.

So you need:

\begin{itemize}
\tightlist
\item
  the number of indicators,
\item
  the sum of variance, and
\item
  the population variance.
\end{itemize}

The last value in there is the alpha, which we obtain at the end.

So, in cell B19, I put the count of the number of indicators for this
construct. As I know there are three, I could simply write in ``3''. If
you want, you can use the \texttt{COUNTA()} function as above. It just
counts the number of cells with text in them. But since you're
highlighting them anyway, it's not much of a time saver. Unless you had
a construct with many many many indicators. I guess then it would save
some time. But honestly, you should always know how many indicators your
construct has, as it should be based in theory and your expert domain
knowledge\ldots{}!

Now the next one is a bit more complex. In order to get the sum of the
variance for each question, I will need first to calculate the variance
for each question. Let's use the \texttt{VARP()} function to do this. I
can place these values as the bottom row to my data. Like so:

\includegraphics{imgs/varp_alpha.png}

Repeat these for the other two columns as well. Now I have my variance
for each indicator question, and I can add them up using the
\texttt{SUM()} function, in the cell B20:

\includegraphics{imgs/sum_var_alpha.png}

Now finally I need the variance between the total scores, between the
people. For this, in cell B21 you just have to calculate the variance of
these values, again using the \texttt{VARP()} function:

\includegraphics{imgs/pop_var_alpha.png}

So now you have absolutely everything to calculate your Cronbach alpha.
All you have to do, is calculate it using the values for number of
indicators, variation within people in their answers to the indicators,
and variation between people to their answers to the indicators. And you
do this with the following formula:

\texttt{=B19/(B19-1)*(1-B20/B21)}

Like so:

\includegraphics{imgs/final_calc_alpha.png}

So that leaves us a value of 0.79. So what does this mean? Well it's a
value between 0 and 1, so obviously 0 is the absolute words, and 1 is
the perfect score. But what sorts of scores are realistic? Well it
depends on the concept you're measuring most likely. But in terms of
acceptable thresholds, here is some guidance:

\includegraphics{http://www.statisticshowto.com/wp-content/uploads/2014/12/CA2.png}

Our value is acceptable, and so we can rest assured, that these three
indicators all tap in to a relatively similar thing, and we can use this
result as our justification for bringing them together as multi-item
indicators for feeling unsafe in one's neighbourhood.

Yay!

A note of caution though. There are some issues with Cronbach alphas.
For example, you might see quite high scored for scales with lots and
lots of question items. As one increases the numbers of items in a
scale, it is more likely that the alpha will be high, but score may be
meaningless because it does not represent an underlying construct or
worse - it represents multiple underlying constructs. Cronbach's Alpha
measures only the reliability of scale of measurement of responses of
the cases, in a Likert Scale. But it does not measure the reliability of
the respondents' opinion leading to the latent construct. In that case,
useful measurements are those of validity. Usually, for the sake of the
integrity of the research, it is recommended that the researcher run a
series of tests to test reliability and validity of questionnaires.
While the Cronbach's alpha is one of these tests, further tests include
principle components analysis, factor analysis, and other indicators.
Having an integrated set of tests adds more to the value of the research
in question. If you are interested, you can read up a bit more
\href{http://onlinelibrary.wiley.com.manchester.idm.oclc.org/doi/10.1002/9780470024522.ch5/pdf}{detail
on validation here}. But that is not something that we will worry about
now, instead we can move on to the final part of the day, recoding.

\hypertarget{recoding-some-more-transforming-your-variables-to-fit-with-your-concepts}{%
\section{Recoding some more: Transforming your variables to fit with
your
concepts}\label{recoding-some-more-transforming-your-variables-to-fit-with-your-concepts}}

\begin{quote}
Transforms are transformations on variables. One purpose of transforms
is to make statistical operations on variables appropriate and
meaningful. Another is to create new variables, aggregates, or other
types of summaries.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

So we've already covered some re-coding of our variables, transforming
them from text values into their rank orders, as they sit in the ordinal
variable above, using the \texttt{VLOOKUP()} function. The last thing
we'll cover today is addressing re-coding of the second variety
mentioned above by Leland Wilkindon, when you want to create aggregate
summaries of a variable. You might want to do this if you want to turn a
numeric variable into a categorical variable for example, or when you
want to turn a categorical variable with many possible values into a
\textbf{binary} variable, a variable with only two possible values.

Let's start with the second option, because I can demonstrate this on
our fear of crime table, that you should still have open, after having
just finished calculating your Cronbach's alpha. Let's say that we no
longer care about the distinction between all 4 types of worry, and
instead wanted to only distinguish between those who are worried or not
worried. We only want 2 possible categories. How can we achieve this?

Well one option is to use the \texttt{VLOOKUP()} function again. Create
another lookup table, but this time, change the column that contains the
items to change the values into from numbers, to the corresponding text
values. Like so:

\includegraphics{imgs/new_lookup_tab.png}

And then create a column for your new variable, for
\textbf{Walkdark\_binary}. Now in this new column, use the
\texttt{VLOOKUP()} function to change the values to the corresponding
binary options. Can you guess what the function will look like, based on
what the function looked like above, when translating to the numbers?

Remember you have to specify: - what you want to reference - your lookup
table - the number for the column where your new values are - that you
want \emph{exact} matches

So, got your formula yet? Well if not, I'll help you. It's this:

\texttt{=VLOOKUP(B2,H20:I23,2,FALSE)}

Like so:

\includegraphics{imgs/binary_vlookup.png}

Of course, remember to add your dollar signs if you want to copy and
paste that formula:

\includegraphics{imgs/add_dolla_lookup.png}

The result should be a new, binary variable, that re-coded your 4
possible values for the walkdark variable into 2: safe/ unsafe. Like so:

\includegraphics{imgs/final_binary_recode.png}

Now the last task is to re-code your numeric variables, into
categorical. You may remember from the first week's reading that you can
always go up the resolution, but not down, for levels of measurement for
variables. This means that you can change a numeric variable into
categorical, but you can't change categorical into numeric. Just
remember this!

\hypertarget{activity-7-recoding-variables}{%
\subsection{Activity 7: Recoding
Variables}\label{activity-7-recoding-variables}}

 OK so final thing, to turn a numeric variable into a categorical, go
back to your data of people's heights. Let's say we don't want any
sophisticated analysis with numbers, we just want to know whether people
are short or they are tall. We will decide this, by checking whether
they are taller than the group average, or shorter than the group
average.

To achieve this, we will return to our trusty \texttt{IF()} function. So
open up the data set with the heights again, and create a new column,
for a new variable called \textbf{tall\_or\_short}:

\includegraphics{imgs/tall_or_not_col.png}

Great, now this column, we want to populate with the word ``Tall'' if
the person is taller than average and ``Short'' if they are not. So what
are the elements of our statement? We need to check \emph{if} the person
height is \emph{greater than} the \emph{average height} and if it is,
type ``Tall'', else type ``Short''.

So we know that the \texttt{IF()} function takes 3 arguments. 1st the
logical statement, that has to be either true or false. In this case,
that statement will test whether person height is greater than the
average height. This would look something like this:
\texttt{cell\ value\ \textgreater{}\ average(column)}.

In the example of our first person, Samuel Carmona, that would be the
value of the cell \texttt{B2}, being checked against the average for the
whole column (\texttt{B:B}). So this part of the statement would look
like:

\texttt{B2\ \textgreater{}\ AVERAGE(B:B)}

Then the other 2 values that the \texttt{IF()} function needs is what to
do if the statement is true, and what to do if the statement is false.
These are simple. If true, write ``Tall'', and if false then write
``Short''.

So altogether, your function will look like this:

\texttt{=IF(B2\textgreater{}AVERAGE(B:B),"Tall","Short")}

Like so:

\includegraphics{imgs/ifelse_tallshort.png}

And finally, copy and paste for your whole column, and you will have a
range of values of tall and short. Since we used the average as our
divider, there should be about as many tall people as short people. You
can quickly make a pivot table to see if this is the case:

You can see that we are close enough:

\includegraphics{imgs/tallshort_pivot.png}

What could we do to split our data exactly in half? Well I'll leave this
question as extra credit. Raise your hand and tell one of us what the
solution is, and we'll come up with some reward.

\hypertarget{summary-3}{%
\section{Summary}\label{summary-3}}

In sum, you should now be able to begin to think about the concepts you
wish to study, and the way in which you can turn them into data to be
collected, to allow you to talk about htese concepts. You should be able
to discuss composite variables, and talk about validity and reliability
of measures and research. You should be comfortable with the following
terms:

\begin{itemize}
\tightlist
\item
  conceptualisation
\item
  measurement
\item
  empirical data
\item
  directly measurable concept
\item
  indirectly measurable concept
\item
  construct
\item
  composite variables
\item
  multi-item scale
\item
  single-item scale
\item
  summated scale
\item
  Likert scale
\item
  population
\item
  sample
\item
  sample statistics
\item
  population parameters
\item
  z-score
\item
  degrees of freedom
\item
  law of large numbers
\item
  central limit theorem
\item
  VLOOKUP()
\item
  reliability
\item
  validity
\item
  Cronbach's alpha
\item
  re-coding variables
\item
  binary variable
\end{itemize}

\#Sources -
\href{http://www.scielo.br/scielo.php?script=sci_arttext\&pid=S0080-62342014000100146}{Analysis
of variables that are not directly observable: influence on
decision-making during the research process} -
\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5459482/}{Composite
Variables: When and How} -
\href{http://onlinelibrary.wiley.com.manchester.idm.oclc.org/doi/10.1002/9780470024522.ch4/pdf}{Scores
and measurements: validity, reliability, sensitivity}

\hypertarget{week5}{%
\chapter{Week 5}\label{week5}}

\hypertarget{learning-outcomes-4}{%
\section{Learning outcomes}\label{learning-outcomes-4}}

This week we are interested in in taking a step back even further from
the analysis of data than last week. Last week we spoke about
conceptualisation and operationalisation, the steps that you take as a
researcher to turn your ideas and topics of interest into variables to
measure. But how do you then go about designing your research study? We
talked a bit about sampling, the ways that you select your sample to
collect information from, and how you ensure that this is done in a way
that reflects the population, about which you want to be drawing
conclusions. This week we take a step back, conceptually, to the highest
level of research oversight, to consider the process of research design,
and consider the various ways that you can go about collecting data.

Much of the data that we work with are collected as part of research.
There are many many different approaches to this process, and you will
have come across a good sample of them in your readings. Research design
can be described as a general plan about what you will do to answer your
research question. Research design is the overall plan for connecting
the conceptual research problems to the pertinent (and achievable)
empirical research. In other words, the research design articulates what
data will be required, what methods are going to be used to collect and
analyse the data, and how all of this is going to answer your research
question. Both data and methods, and the way in which these will be
configured in the research project, need to be the most effective in
producing the answers to the research question (taking into account
practical and other constraints of the study). Different design logics
are used for different types of study, and the best choice depends on
what sorts of reaserach questions you want to be able to answer.

Since your reading provides a comprehensive overview of different types
of research designs, I will not attempt to replicate this here. Instead
I will cover some general practical points, with focus on two different
study designs. These are experiments, and longitudinal studies. However
the skills we will practice today are relevant for data collected from
various research resigns. For example, missing data is something that is
an issue for cross sectional studies as well as longitudinal ones.
Similarly, the Gantt chart as a way to plan your study design can be
applied to any form of research design.

\hypertarget{terms-for-today-3}{%
\subsection{Terms for today}\label{terms-for-today-3}}

\begin{itemize}
\tightlist
\item
  Research Design
\item
  Evaluation \& Experiments

  \begin{itemize}
  \tightlist
  \item
    RCT
  \item
    Working with experimental data
  \item
    Meta-analysis (a note on the tip of the evidence-pyramid)
  \end{itemize}
\item
  Longitudinal Study Designs

  \begin{itemize}
  \tightlist
  \item
    The importance of time
  \item
    Linking data
  \end{itemize}
\end{itemize}

\hypertarget{research-design}{%
\section{Research Design}\label{research-design}}

When you're designing your research, and developing your research
design, you can imagine yourself as the architect of the research
project. You lay down the blueprints and identify all the tasks and
elements that will need to be realised in order for your research
project to be successful. The type of research design used in a crime
and justice study influences its conclusions. Studies suggest that
design does have a systematic effect on outcomes in criminal justice
studies. For example, when determining the effect of an intervention
(known as evaluation research, as you are \emph{evaluating} the effect
of the intervention), when comparing randomized studies with strong
quasi-experimental research designs, systematic and statistically
significant differences are observed
(\href{http://cebcp.org/wp-content/publications/Does\%20Research\%20Design\%20Affect\%20Study\%20Outcomes.pdf}{Weisburd
et al 2001}).

Those interested in the study of criminology and criminal justice have
at their disposal a wide range of research methods. Which of the
particular research methods to use is entirely contingent upon the
question being studied. Research questions typically fall into four
categories of research:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \tightlist
  \item
    \emph{descriptive} (define and describe the social phenomena),
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \emph{exploratory} (identify the underlying meaning behind the
    phenomena),
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    \emph{explanatory} (identify causes and effects of social
    phenomena), and
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    \emph{evaluative} (determine the effects of an intervention on an
    outcome).
  \end{enumerate}
\end{itemize}

Your readings will have gone through a lot of examples and details about
each one of these approaches, and what research design you can use to
answer which sort of categories of research questions. These books give
you a fantastic theoretical overview of the methods, and so I will not
reiterate those here. Instead I will try to focus on the practicalities
associated with some research designs. While I will use the example of
evaluative research here, you can use similar methods when appropriate
for all sorts of research design.

\hypertarget{experiments-and-evaluation}{%
\section{Experiments and evaluation}\label{experiments-and-evaluation}}

Experimental criminology is a family of research methods that involves
the controlled study of cause and effect. Research designs fall into two
broad classes: quasi-experimental and experimental. In experimental
criminology, samples of people, places, schools, prisons, police beats,
or other units of analysis are typically assigned (either randomly or
through statistical matching) to one of two groups: either a new,
innovative \textbf{treatment}, or an alternate intervention condition
(\textbf{control}). Any observed and measured differences between the
two groups across a set of ``outcome measures'' (such as crime rates,
self-reported delinquency, perceptions of disorder) can be attributed to
the differences in the treatment and control conditions. Exponential
growth in the field of experimental criminology began in the 1990s,
leading to the establishment of a number of key entities (such as the
Campbell Collaboration, the Academy of Experimental Criminology, the
Journal of Experimental Criminology, and the Division of Experimental
Criminology within the American Society of Criminology) that have
significantly advanced the field of experimental criminology into the
21st century. These initiatives have extended the use of experiments
(including randomized field experiments as well as quasi-experiments) to
answer key questions about the causes and effects of crime and the ways
criminal justice agencies might best prevent or control crime problems.
The use of experimental methods is very important for building a solid
evidence base for policymakers, and a number of advocacy organizations
(such as the Coalition for Evidence- Based Policy) argue for the use of
scientifically rigorous studies, such as randomized controlled trials,
to identify criminal justice programs and practices capable of improving
policy-relevant outcomes.

\begin{itemize}
\item
  \href{http://www.oxfordbibliographies.com/view/document/obo-9780195396607/obo-9780195396607-0085.xml}{Experimental
  Criminology by Lorraine Mazerolle, Sarah Bennett}
\item
  How much crime does prison prevent--or cause--for different kinds of
  offenders?
\item
  Does visible police patrol prevent crime everywhere or just in certain
  locations?
\item
  What is the best way for societies to prevent crime from an early age?
\item
  How can murder be prevented among high-risk groups of young men?
\end{itemize}

These and other urgent questions can be answered most clearly by the use
of a research design called the ``randomized controlled trial.'' This
method takes large samples of people--or places, or schools, prisons,
police beats or other units of analysis--who might become, or have
already been, involved in crimes, either as victims or offenders. It
then uses a statistical formula to select a portion of them for one
treatment, and (with equal likelihood) another portion to receive a
different treatment. Any difference, on average, in the two groups in
their subsequent rates of crime or other dimensions of life can then be
interpreted as having been caused by the randomly assigned difference in
the treatment. All other differences, on average, between the two groups
can usually be ruled out as potential causes of the difference in
outcome. That is because with large enough samples, random assignment
usually assures that there will be no other differences between the two
groups except the treatment being tested.

Watch this
\href{https://www.youtube.com/watch?v=IGDF1-B1Yjs\&feature=player_embedded}{6
minute video from Cambridge University to learn a bit more about
experimental crim}

And then
\href{http://www.cla.temple.edu/cj/center-for-security-and-crime-science/the-philadelphia-foot-patrol-experiment/}{this
video that describes the Philadelphia foot patrol experiment}. Pay
attention to how the blocks are assigned to the treatment (foot patrol)
and the control (normal, business as usual policing) groups.

Do take the time to watch the videos above, they will help you
understand how and why we can use experiments in criminological
research! Take time to get through these, it will help, but also I hope
that they are interesting examples of criminological research in action!
Quantitative criminology isn't all about sitting around playing with
spreadsheets and reading equations - we do get out sometimes, and get to
have an impact on things like policing :) So watch the above two films,
I'll wait here.

\includegraphics{https://media.giphy.com/media/pUeXcg80cO8I8/giphy.gif}

Evaluation of programmes is very important. As you heard the police
chiefs explain in the Philadelphia video above, it can make the
difference between investing in a helpful intervention (foot patrols in
this case) or not. By being able to quantify the crime reduction effect
that foot patrols had on particular areas, it became possible to support
and lobby for these to be implemented.

While it's great that evaluations can be used to build a case for
effective interventions, it is equally important to know whether
something doesn't work. Have you heard of the scared straight program?
The term ``scared straight'' dates to 1978, and a hit documentary by the
same name. The film featured hardened convicts who shared their prison
horror stories with juvenile offenders convicted of arson, assault, and
other crimes. The convicts screamed, yelled, and swore at the young
people. The concept behind the approach was that kids could be
frightened into avoiding criminal acts.

It is an idea that has also been tested in drivers' education classes
across the US. For several decades beginning in the 1960s, many
soon-to-be-drivers watched a graphic video of mangled bodies being
pulled from automobile wreckage -- described in one magazine review as a
``twenty-eight-minute gorefest'' meant to deter reckless driving.

You can see how people are immediately averse to this idea. It is, quite
obviously distressing to kids that are being subjected to these scared
straight programmes, through the yelling, the gruesome violence, and so
on. But if the end result is that this shock and horror deters kids from
offending, or young drivers from being careless on the roads, then it
might be considered a ``necessary evil'', right? It might be short-term
pain inflicted, but in the interest of long-term gain, whereby these
kids will avoid a life of offending, or ending up in a horrible road
traffic collision. But in order to make this argument, we need to be
able to tell - does this work?

To be able to answer such questions we can devise experiments, in the
form of research design of experimental and quasi-experimental research.
Some scholars believe that experimental research is the best type of
research to assess cause and effect (Sherman; Weisburd). True
experiments must have at least three features:

\begin{itemize}
\tightlist
\item
  at least two comparison groups (i.e., a treatment group and a control
  group),
\item
  variation in the independent variable before assessment of change in
  the dependent variable, and
\item
  random assignment to the groups.
\end{itemize}

\emph{What do each of these mean?}

Well first we need to establish our dependent and independent variables,
to determine what we expect is evoking a change in what.

What is a \textbf{dependent variable}? It's what you are looking to
explain. Remember in week 3, when we were talking about bivariate
analysis to assess the relationship between two variables? And we spoke
about one of them being a \textbf{response} variable and the other the
\textbf{predictor} variable, and how do we determine which variable is
which? In general, the explanatory variable attempts to explain, or
predict, the observed outcome. The response variable measures the
outcome of a study. One may even consider exploring whether one variable
causes the variation in another variable -- for example, a popular
research study is that taller people are more likely to receive higher
salaries. In this case, age at first arrest would be the explanatory
variable used to explain the variation in the response variable number
of arrests.

Well these variables can also be called \textbf{dependent} and
\textbf{independent} variables. \textbf{Dependent variables} are another
name for response variables. They \emph{depend} on the values of the
independent variable. It can also have a third name, and be called an
\emph{outcome variable}. The \textbf{independent variable} on the other
hand is another name for predictor variables.

For example, if you are trying to predict fear of crime using age (so
your research question might ask: \emph{are older people more worried
about crime?}) then in this case, you belueve that age will be
influencing fear, right? Old age will cause high fear, and young age
will cause low fear. In this case, since age is influencing fear, a
person's level of fear of crime \emph{depends} on their age. So your
\emph{dependent} variable is age. And since it's influenced by fear,
fear if the \emph{independent} variable.

So what about in the case of an experiment? Let's return to our example
with the scared straight programmes. We want to know whether scared
straight has an effect on future offending, correct? So what are our
variables here? Remember last week, when we were identifying concepts in
research questions in the feedback session? That might help. So we want
to know the effect of \emph{scared straight} on the people's
\emph{future offending}. Our variables are in italics; they are:

\begin{itemize}
\tightlist
\item
  exposure to scared straight programme
\item
  future offending behaviour.
\end{itemize}

Now which influences which one? Well the clue is always in our question.
And our question is whether kids who participate in scared straight
offend less than they would have if they didn't participate. So for this
we know that we have one variable (in this case offending) that
\emph{depends on} the other (participating in scared straight or not).
So what does this mean for which is dependent variable and which is the
independent?

Take a moment to try to answer this yourself. Think about which one is
\emph{predicting} which \emph{outcome} if the dependent/independent
division doesn't work for you. You should definitely have a go at trying
to guess here, because I will give you the answer later, and it will
help you check your understanding, and if you are unsure, then do ask
now. The dependent/independent variable distinction will be important
throughout your data analysis career, even if that is only this one
course.

So take a moment to think through, and decide which variable is
dependent or independent. Here is a corgi running around in circles to
separate the answer, so you don't accidentally read ahead:

\includegraphics{https://media.giphy.com/media/RrFZIzK3coIMg/giphy.gif}

Right so now you're hopefully read after your own consideration, and you
may have found that your \textbf{dependent variable} is the one that
depends on the intervention, which is - whether the young person exposed
to the scared straight programme offends or not. So your
\emph{dependent} variable is \emph{future offending}. This would have to
be conceptualised (how far in the future, what counts as offending, etc)
and then in turn operationalised, to be measured somehow.

Your independent variable on the other hand is the thing that you want
to see the effect of, in this case, it's participation in the scared
straight programme. Right? You are interested in whether participation
causes less offending in the future. So your \emph{independent} variable
is \emph{participating in scared straight}. Again you would have to
conceptualise scared straight participation (can they go to one event,
do they have to attend many? does it matter what happens to them?
whether they get yelled at or not? whether they go into prison or not,
etc) and then also operationalised in a way (do we just measure did this
young person ever take part as a yes/no categorical variable? Do we
count the number of times they took part?)

But no matter how you conceptualise and operationalise these variables,
you will still have to be able to determine the effect of one or the
other. And the \emph{research design} of your study will greatly affect
whether or not you can do that.

Experiments provide you with a research design that \emph{does} allow
you to make these cause and effect conclusions. If you design your study
as experiments, you will be able to control for certain variation, and
employ methods that give you certainty about what causes what, and which
event came first. This is a huge strength of experimental design. When
talking about causation, or in evaluations, experimental design has
features that mean that it is an optimal methodology for evaluation. Of
course this does \emph{not} mean it's the \emph{only} appropriate
design. But it's one approach that we will cover here in detail.

So for your research design to be considered an experiment the three
criteria listed above should be met. You need your dependent and
independent variables, but you also need variation in your independent
variable (usually between 2 groups). So the first thing you need is two
groups, between who you can compare the value of the dependent variable,
after having exposed them to some difference in the independent
variable. What you need from these two groups, is that they are
\emph{different} on this value of the independent variable. This is
because you want to compare them. As you will have picked up from your
readings, these two groups are often called your \textbf{treatment} and
your \textbf{control} groups, because one group is administered some
treatment (such as they took part in the intervention you are hoping to
evaluate) while the other one receives the business as usual approach.
Think back to the Philadelphia foot patrol video above: one group of
street segments (treatment) received foot patrols, right? That's where
beats were drawn up to, and where foot patrol officers went on their
shifts, to do their thing, and keep the streets safe. But there were
also another set of street segments, right? The ones that were assigned
to the control groups did \emph{not} receive foot patrol. Police still
responses to calls for service, and everything went on as usual, but
there was no patrol there - this is the control group.

So you have 2 groups - treatment and control - and you also have a
difference in the independent variable between the two - for example
where one group is given the treatment, and the other is not.

Watch \href{https://www.youtube.com/watch?v=igm0oanTFJI}{this video} to
make sure that you get this concept.

For another angle
\href{https://www.youtube.com/watch?v=aLesk8fujH8}{this video} is also
worth a watch (if for nothing else then for the beautiful MS Paint style
graphics)

Now the last point there is to do with random assignment. Our main goal
is to assess change in the dependent variable between the two groups,
after exposing them differently to the independent variable. But we want
to be entirely sure that one group isn't systematically different to the
other. One way to achieve this is to use random assignment into the
groups. Randomization is what makes the comparison group in a true
experiment a powerful approach for identifying the effects of the
treatment. Assigning groups randomly to the experimental and comparison
groups ensures that systematic bias does not affect the assignment of
subjects to groups. This is important if researchers wish to generalize
their findings regarding cause and effect among key variables within and
across groups. Remember all our discussion around validity and
reliability last week!

Random assignment is a criteria for experiment research. A research (or
evaluation) design is experimental if subjects are randomly assigned to
treatment groups and to control (comparison) groups. A research (or
evaluation) design is quasi-experimental if subjects are not randomly
assigned to the treatment or control conditions but rather if
statistical controls are used to study cause and effect. You will have
learned about quasi-experimental design in your readings, so here I will
focus on experimental designs.

\begin{quote}
Consider the following criminal justice example. Two police precincts
alike in all possible respects are chosen to participate in a study that
examines fear of crime in neighborhoods. Both precincts would be
pre-tested to obtain information on crime rates and citizen perceptions
of crime. The experimental precinct would receive a treatment (i.e.,
increase in police patrols), while the comparison precinct would not
receive a treatment. Then, twelve months later, both precincts would be
post-tested to determine changes in crime rates and citizen perceptions.
\end{quote}

-\href{http://law.jrank.org/pages/923/Criminology-Criminal-Justice-Research-Methods-Quantitative-research-methods.html}{Criminology
and Criminal Justice Research: Methods - Quantitative Research Methods}

You can read about these studies above for some examples of experiments
in criminal justice research. The Philadelphia foot patrol experiment
was just one of many. In particular, the Philadelphia foot patrol
experiment is an example of a \textbf{randomised control trial}. You
will have read about a few types of experimental design in your
textbooks, but here we will focus on this particular one. The randomised
controlled trial (RCT for short) is considered often as the most
rigorous method of determining whether a cause-effect relationship
exists between an intervention and outcome . The strength of the RCT
lies in the process of randomisation that is unique to this type of
research study design.

\hypertarget{rct}{%
\subsection{RCT}\label{rct}}

An RCT presents a study design that randomly assigns participants into
an experimental group or a control group. As the study is conducted, the
only expected difference between the control and experimental groups in
the RCT is the outcome variable being studied.

There are several advantages and disadvantages to RCTs:

Advantages

\begin{itemize}
\tightlist
\item
  Good randomization will ``wash out'' any population bias
\item
  Results can be analyzed with well known statistical tools
\item
  Populations of participating individuals are clearly identified
\end{itemize}

Disadvantages

\begin{itemize}
\tightlist
\item
  Expensive in terms of time and money
\item
  Volunteer biases: the population that participates may not be
  representative of the whole
\end{itemize}

Do engage with the reading around the advantages and disadvantages of
RCTs. They are often looked at as the gold standard for determining the
effect of a particular intervention, but that does not mean they are
without flaws, or are the only way forward, there are situations where
they may or may not be appropriate. But here we are getting practical,
so let's make the assumption, that for our evaluation of scared
straight, we have decided that we are going to evaluate using an RCT.

So the first step is to design our study.

\hypertarget{designing-an-rtc}{%
\subsection{Designing an RTC}\label{designing-an-rtc}}

When you are designing any research, you will have to map out all the
elements of the study in detail. One of the key issues with designing
your research that you have to keep in mind is that it is feasible to
carry out, given the resources which you have. These resources include
your time, funding, any available staff, and basically everything that
you need to be able to carry out your work. It can be write a large task
to try to estimate all elements of a study. In order to help you break
down your tasks into individual elements, and to be able to assign a
time element to each one, to be able to more accurately estimate the
time it will take you to carry out your study, you can use all sorts of
project planning tools. One of these is a Gantt chart. We will
illustrate the use of a Gantt chart here through planning an RCT, to
evaluate the effectiveness of the scared straight programme. But you can
use a Gantt chart to plan any sort of research project. You could even
use it to plan your dissertations next year!

\hypertarget{what-is-a-gantt-chart}{%
\subsubsection{What is a Gantt chart?!}\label{what-is-a-gantt-chart}}

A Gantt chart, commonly used in project management, is one of the most
popular and useful ways of showing activities (tasks or events)
displayed against time. On the left of the chart is a list of the
activities and along the top is a suitable time scale. Each activity is
represented by a bar; the position and length of the bar reflects the
start date, duration and end date of the activity. This allows you to
see at a glance:

\begin{itemize}
\tightlist
\item
  What the various activities are
\item
  When each activity begins and ends
\item
  How long each activity is scheduled to last
\item
  Where activities overlap with other activities, and by how much
\item
  The start and end date of the whole project
\end{itemize}

To summarize, a Gantt chart shows you what has to be done (the
activities) and when (the schedule). It looks like this:

\includegraphics{https://d2myx53yhj7u4b.cloudfront.net/sites/default/files/2excel-gantt-chart-temp1.jpg}

In order to be able to build a Gantt chart, you need to know the
following about your project:

\begin{itemize}
\tightlist
\item
  The tasks required to carry it out
\item
  The length of time available for the whole project
\item
  Approximate start and finish dates for the individual tasks
\end{itemize}

Gantt charts can help you make sure that your project is feasible in the
time you have. It can also be a way for you to think about all the tasks
that are involved with your project. The goal is for you to identify
overarching tasks (eg ``data collection'') and be able to sub-divide
into the individual elements that make up that task (eg ``identify
population'', ``plan sampling strategy'', ``recruit sample'', ``deploy
survey'', ``collect completed surveys'', ``input data into excel'').
Once you consider each element required for you to complete a particular
part of your project, you can start thinking about how long each will
take, and how feasible it is within your given set of resources
(including your time).

So how is this helpful in research design? Well let's consider the
example of designing an RCT to evaluate Scared Straight. In order to be
able to carry out this RCT, we need to be able to grab a basic outline
for one, and turn it into a Gantt chart. Then we can assess how long
this will take, what resources we will need, and whether or not this
would be feasible for us to carry out.

Let's start with the basic outline

\hypertarget{activity-1-research-planning-with-gantt-charts-design-an-rtc-example}{%
\subsection{Activity 1: Research Planning with Gantt Charts: Design an
RTC
example}\label{activity-1-research-planning-with-gantt-charts-design-an-rtc-example}}

The basic outline of the design of a randomised controlled trial will
vary from trial to trial. It depends on many many factors. The rough
outline for all research designs will follow something like this:

\begin{itemize}
\tightlist
\item
  Planning and conceptualisation
\item
  Operationalisation and data collection
\item
  Data analysis
\item
  Writing up of results
\end{itemize}

This is not necessarily always a linear process. It can be that after
data analysis you return for more data collection, or even go back to
the conceptualisation stage. But as I mentioned above, the aim of the
Gantt chart is to break down these tasks into the smallest possible
components. Why is this?

Well let's try to build a Gantt chart with just these elements first to
illustrate. As I said you need to know the approximate duration for your
project, the tasks, and how long they will take.

Let's say that we have 1.5 years to carry out our RCT for Scared
Straight. This is our approximate duration. We also have our list of
tasks, up there. But how long will each of these take? How long should
you budget for Planning and conceptualisation? What about for
Operationalisation and data collection or the Data analysis? Take a
moment to think about this.

Was it difficult to estimate? Why would you think that is? Do you think
it would be easier if you had more experience with research? Well
probably not by much. Each research project comes with its own
complexity and nuance, and to estimate how long something as vague as
``data analysis'' will take, would be an incredible tough task even for
the most seasoned researcher. Instead, the way to be able to better
guestimate the length for tasks is to break them into their components,
which can give you a better indicator of how long things will take.

 Let's try this for the Scared Straight RCT.

Let's start with our main overarching categories for above, but break
each one down into its components. Something like this:

\begin{itemize}
\tightlist
\item
  Planning and conceptualisation

  \begin{itemize}
  \tightlist
  \item
    Background/review of published literature
  \item
    Formulation of hypotheses
  \item
    Set the objectives of the trial
  \item
    Development of a comprehensive study protocol
  \item
    Ethical considerations
  \end{itemize}
\item
  Operationalisation and data collection

  \begin{itemize}
  \tightlist
  \item
    Sample size calculations
  \item
    Define reference population
  \item
    Define way variables will be measured
  \item
    Choose comparison treatment (what happens to control group)
  \item
    Selection of intervention and control groups, including source,
    inclusion and exclusion criteria, and methods of recruitment
  \item
    Informed consent procedures
  \item
    Collection of baseline measurements, including all variables
    considered or known to affect the outcome(s) of interest
  \item
    Random allocation of study participants to treatment groups
    (standard or placebo vs.~new)
  \item
    Follow-up of all treatment groups, with assessment of outcomes
    continuously or intermittently
  \end{itemize}
\item
  Data analysis

  \begin{itemize}
  \tightlist
  \item
    Descriptive analysis
  \item
    Comparison of treatment groups
  \end{itemize}
\item
  Writing up of results

  \begin{itemize}
  \tightlist
  \item
    Interpretation (assess the strength of effect, alternative
    explanations such as sampling variation, bias)
  \item
    First draft
  \item
    Get feedback \& make changes
  \item
    Final write-up of results
  \end{itemize}
\end{itemize}

So how do you come up with individual sub-elements? Well there is no
simple answer to this, the way that you come up with these categories is
by thinking about what it is that you need to do in each stage to
achieve your goals. What do you need to do to collect your data? What
are all the steps, all the actions that you need to take, to reach your
end goal of a set of data that you can analyse in order to be able to
talk about the difference between your control and treatment groups?
What do you need to do when you write up? What are the stages of writing
up? How long do each one of these normally take you? There are some
people who can write a rough 1st draft quickly, and send it to a
colleague for comments and feedback. Others need to be very comfortable
with their draft first, and spend more time on it, before they can show
someone else to receive comments. Because of this, not only does each
project have its nuances and differences when building a Gantt chart,
but each person will as well.

This exercise is aimed to help you get thinking about projects in this
way, but also to illustrate, once you have the above information for a
project, how you can draw it up into a visual timeline that can help you
plan your research project, and make sure that it runs on time. So let's
build a Gantt chart for our RCT of the Scared Straight programme, using
the tasks above to guide us.

First you will have to open a new Excel spreadsheet. Just a blank one.
We'll be building our Gantt chart from scratch.

So once you have your blank Excel sheet, create 4 columns:

\begin{itemize}
\tightlist
\item
  Task
\item
  Start date
\item
  End date
\item
  Duration
\end{itemize}

Something like this:

\includegraphics{imgs/gantt_headers.png}

The \emph{Task} column refers to each individual activity, that are the
detailed steps that we have to take, in order to be able to complete our
project. These are all the activities that we've broken the tasks into.
If we were considering this Gantt chart table as a data set, you can now
begin to guess, that our unit of analysis is the \emph{task}. Each one
activity we have to do is one row. These include the bigger group that
each sub-task it belongs to as well as the sub tasks themselves. This
will be meaningful later on.

In the other columns, \emph{start date}, \emph{end date}, and
\emph{duration} we will record the temporal information around each task
- that is - when will it start? when will it end? how long will it take?

So as a first step, let's populate the \emph{task} column. You can copy
and paste from the list above. Should look something like this:

\includegraphics{imgs/task_subtask_pop.png}

Great, now how will we populate the start date and end date columns.
This is where the Gantt chart is very much a tool for you to plan your
research project. How can we know how long will something take? The
short answer is: we can't. We don't have a crystal ball. But we can
\emph{guess}. We can start with the project start date (or if you work
better counting backwards, you can start with the project end date if
it's a known hard deadline), and then just try to estimate how long each
phase will take to complete, and when we can start the next.

Tasks can run simultaneously. You don't always have to wait for one task
to finish before you start the next. Sometimes one task needs to finish
for the next to start, for example: you cannot begin data analysis until
you have finished data collection. On the other hand, you can (and
should) begin writing while still continuing your data analysis. So you
can have temporal overlap - or be working on multiple projects at once.

Now let's say that we are starting our Scared Straight evaluation quite
soon, we have a start date of the 1st of November. Using some very rough
guessing, this is the relative timeline that I've come up with:

\begin{itemize}
\tightlist
\item
  Background/review of published literature: 01/11/17 to 01/12/17
\item
  Formulation of hypotheses 01/12/17 07/12/17
\item
  Set the objectives of the trial 02/12/17 08/12/17
\item
  Development of a comprehensive study protocol 08/12/17 to 18/12/17
\item
  Ethical considerations 08/12/17 to 18/12/17
\item
  Sample size calculations 18/12/17 to 19/12/17
\item
  Define reference population 18/12/17 to 28/12/17
\item
  Define way variables will be measured 28/12/17 to 07/01/18
\item
  Choose comparison treatment (what happens to control group) 01/01/18
  to 07/01/18
\item
  Selection of intervention and control groups, including source,
  inclusion and exclusion criteria, and methods of recruitment 18/01/18
  to 21/01/18
\item
  Informed consent procedures 22/01/18 to 23/01/18
\item
  Collection of baseline measurements, including all variables
  considered or known to affect the outcome(s) of interest 23/01/18 to
  23/01/18
\item
  Random allocation of study participants to treatment groups (standard
  or placebo vs.~new) 21/01/18 to 23/01/18
\item
  Follow-up of all treatment groups, with assessment of outcomes
  continuously or intermittently 24/01/18 to 23/01/19
\item
  Descriptive analysis 24/01/19 to 24/02/19
\item
  Comparison of treatment groups 30/01/19 to 24/02/19
\item
  Interpretation (assess the strength of effect, alternative
  explanations such as sampling variation, bias) 20/02/19 to 20/03/19
\item
  First draft 20/03/19 to 25/03/19
\item
  Get feedback \& make changes 25/03/19 to 05/04/19
\item
  Final write-up of results 05/04/19 to 30/04/19
\end{itemize}

You should be able to copy this over into Excel if you roughly agree
with these time scales. If not, you can do this yourself, and come up
with how long you think each stage would take you.

Notice that I did not give a start or end time to the \emph{overarching
categories} of planning and conceptualisation , operationalisation and
data collection , data analysis , and writing up of results? We
mentioned earlier, that these categories are so vague, that it becomes a
very difficult task indeed to be able to guess their duration. Instead
we break them down into smaller tasks, and calculate those. Well to get
the start and end date for the overarching tasks, you just need the
first start date for the first task, and the last end date for the last
task that belongs to this overarching group. How can we find this? Well
we can use the \texttt{=MIN()} and the \texttt{=MAX()} functions in
Excel.

\includegraphics{imgs/ gantt_min.png}

\includegraphics{imgs/gantt_max.png}

Make sure that you only select the sub-tasks that belong to each
individual overarching task. So for example, for Planning and
conceptualisation, only select up until ``Ethical considerations'' and
\emph{do not} also include the ``Operationalisation and data
collection'' stage!

Once you have all your start and end dates, your table should look
something like this:

\includegraphics{imgs/gantt_start_end.png}

You might find that Excel changes the formatting of your dates in some
of the cells, like mine did on the first 8 rows. You can see in those,
the date is formatted as 01-Dec for example, whereas later it is
formatted the way we entered it, such as 22/01/19. This is because Excel
\emph{knows} that the value you are entering here is a \emph{date}. It
doesn't just think you're entering some weird words, it deduces that if
what you are entering follows this rough format of 2 digits/ 2 digits /
2 digits, then it's likely to be a date. This is really handy, because
you can do calculations on these cells now, that you would not be able
to do, if Excel just thought that these were weird words. We will take
advantage of this to calculate the duration column here. Do you really
want to count out how many days are between the start and end date? Well
in some cases it might be easy. It could be that you thought, ``well I
will start my random allocation of study participants into treatment and
control groups on the 21st of January, and I think it will take about 2
days to do this, so I will end on the 23rd of January'', but often you
will have deadlines, that you need to work towards, or you might want to
double check that you are counting correctly. In any case, to get the
last, \emph{duration} column, you can simply use an equation, where you
subtract the start date from the end date, and you get the number of
days that are inbetween. Isn't that neat? You can simply apply some
simple maths notations to your date data in Excel, and you get
meaningful results, such as the number of days that exist between two
dates! We will play around more with dates and such in week 7 of this
course, so you can see some more date-related tricks then!

Right, back to our dates, so remember, all formulas start with the
\texttt{=} equation, and here all we are doing is subtracting the value
of one cell (start date) from another cell (end date). Like so:

\includegraphics{imgs/calc_dur.png}

Copy and paste the formatting all the way down, and ta-daa you have your
column for the duration of each task:

\includegraphics{imgs/all_cols_gantt.png}

Now you have all the columns you need, to be able to build your Gantt
chart.

To do this, click on an empty cell, anywhere \emph{outside} your table,
and select Charts \textgreater{} Stacked Bar Chart:

\includegraphics{imgs/gantt_select_bar.png}

An empty chart will appear like so:

\includegraphics{imgs/gantt_empty_chart.png}

Right-click anywhere in the empty chart space, and choose the option
``Select Data\ldots{}'':

\includegraphics{imgs/gantt_select_data.png}

This will open a dialogue box. This dialogue box might look different if
you're on PC or on Mac (and even on Mac your version will be newer than
mine, so might look slightly different) but the basic commands should be
the same. If you cannot find anything, let us know!

So on this popup window, select the option to \emph{Add} data.

On PC:

\includegraphics{https://www.officetimeline.com/Content/images/articles/gantt-chart/3_Select-Data-Add.png}

On Mac:

\includegraphics{imgs/mac_add_data.png}

When you select this option, you have to supply 2 values to this series.
First you have to tell it the name. This is simply the cell that
contains your column header. Then you have to tell it the values. These
are all your start dates. You can select by clicking in the blank box
for each value, and then clicking/ drag dropping on the spreadsheet.
Like so:

\includegraphics{imgs/gantt_select_data_1.gif}

When you're done, click ``OK''.

On a PC, when you click the ``Add'' button it will open a new window,
but again, all you have to enter in that new window is the name and the
values, the exact same way. Here is an illustrative example of what this
will look like on PC:

\includegraphics{https://www.officetimeline.com/Content/images/articles/gantt-chart/3_Select-Data-Start-Date-Added.png}

Now click OK, and you will see some bars appear, reaching to each start
date that we have. Now we have to add the \emph{duration}. To do this,
just repeat the steps, for adding a series, but with the duration column
this time.

\includegraphics{imgs/gantt_select_data_2.gif}

The part that appears in red above represents the duration of each task.
The blue part now is actually redundant. We just needed it there, so
that the red part (the actual task) begins at the correct point. So just
like we did when making that box plot a few weeks ago, we clear the
fill, we clear the line, and in case there is a shadow there, we clear
that as well, so that we don't confuse ourselves by having that blue
part there.

Just to show something new, I will show you a different way of going
about this here, buy you can just as easily follow the same steps you
took when making things invisible in your boxplot.

But the other things you can do is to click on the blue section of the
graph to select it:

\includegraphics{imgs/gantt_select_blue.png}

And then right click and choose `Format Data Series\ldots{}':

\includegraphics{imgs/gantt_format.png}

Then on the side tabs, you can go through and select fill, and line, and
even shadow, and make sure that they are all set to no fill/ no line/ no
shadow:

\includegraphics{imgs/gant_no_fill.png}

\includegraphics{imgs/gantt_no_line.png}

\includegraphics{imgs/gantt_no_snadow.png}

Then click OK, and you will see only the red parts of your graph, which
represent each task and the duration it lasts:

\includegraphics{imgs/gantt_blue_cleared.png}

But what is each task? I just see numbers? Well we need to add the
labels, from the task column of your data, to give it some sort of
meaning. To do this, once again right click anywhere on your chart area,
and select ``Select Data\ldots{}'', and this time, for where it asks for
axis labels, click in that box, and then highlight the column with the
tasks in it:

\includegraphics{imgs/axis_labels_gantt.png}

On a PC you will have to click the ``Edit'' button under the
``Horizontal category axis labels'' box. This will look something like
this:

\includegraphics{https://www.officetimeline.com/Content/images/articles/gantt-chart/5_Select-Data-Source.png}

When you then select the column with the tasks in it and click OK, it
should label your tasks properly now:

\includegraphics{imgs/gantt_labelled.png}

Now your Gantt chart is basically ready. It can be that yours looks sort
of in a different order? If this is the case, go back to your data, and
sort your data on the ``End date'' column. If you do this, then you will
have your fist tasks at your top, and the last tasks at the bottom. This
should help you plan your tasks. You can take some time to edit your
Gantt chart formatting to make it look the way that you would find it
the most helpful. For example, here's mine:

\includegraphics{imgs/final_gantt.png}

So what's going to take the longest? As you can see there, the longest
duration is for the operationalisation and data collection tab. It's an
overarching category, but there are not any sub-tasks associated with it
for the majority of its duration. Well, the thing is, even though we are
not actively collecting data in that longer period, we are still in the
data collection phase. Can you think why?

Well we want to know if scared straight works on reducing offending
right? So for this, we need to recruit our people, assign them into a
control and a treatment group, and then \emph{wait for a pre-determined
amount of time} before we can collect the follow-up data - or the
\emph{after} data.

Remember back ton conceptualisation. How do we conceptualise
reoffending? Well in this case, we conceptualise it as if the person has
offended in the 12 months following taking part in the scared straight
programme. Because of this, we have to wait 12 months until we collect
our ``after'' data.

Planning is a very important part of the research project, and the
research design which you pick will greatly affect your research plan.
Think about if we considered instead a one-time survey? We could ask
people - ``have you ever taken part in a scared straight programme?''
And then ask them ``have you offended in the last 12 months?''. But this
is a \textbf{cross-sectional} study design, in which we only take
measurement at one point in time. This is a very different study design,
and the advantages of an RCT over a cross-sectional survey in terms of
determining the effect of an intervention are widely discussed in your
readings. However you can also imagine how it would be an easier study
to carry out, right? The data collection part of your Gantt chart there
would reduce significantly, from over 12 months, to something much
shorter, just the length of time it takes to conduct one survey. Maybe a
month.

Hopefully you are beginning to get an idea into the nuances of finding
your optimal research design. To a certain extent this is dictated by
your research question. If you want to know whether scared straight
works or not, and you want to design a study to assess this, an RCT is
your ideal way forward. However your hands may be tied. If you wanted to
do this as part of an undergraduate dissertation project for example,
you cannot just wait 12 months to follow up people's offending, as your
deadlines will have all passed by then.

The Gantt chart is a handy tool for research planning, and considering
all the tasks that you need to carry out, as dictated by your research
design. We illustrated here with the example of planning an RCT. To get
the steps for these we can consult the reading, we can look at previous
studies, or we can browse around for outlines others have used. For
example, the RCT one is based on
\href{https://www.healthknowledge.org.uk/e-learning/epidemiology/practitioners/introduction-study-design-is-rct}{this
proposed outline} for RCTs. Your readings around each research design
will give you an indication of what tasks are involved with each one,
and you should be able to produce such an outline for any kind of
project. It is a good way to keep yourself on track, to make sure that
you do not take on too many tasks, and also to show your supervisor/
employer/ person marking your project proposal, in case that is the
stream you choose for your dissertation, that you are thinking about
feasibility of your tasks.

Once you have planned your project, you need to carry out each task.
Something like a literature review you will be familiar with from your
work on other modules. You have to have a read through the relevant
literature, in order to be able to identify whether your research
question has already been answered, to look into what other people are
researching in your topic area, to consider the approached to
conceptualisation and operationalisation that others before you have
taken, and to engage with what is currently happening in this topic
you're interested in. This will lead you to your research question, and
your hypotheses. Hypotheses are the testable versions of your questions.

For example if our research question is: does scared straight deter
reoffending in young people? your hypothesis would look something like
this: those who did not receive scared straight program intervention
will offend more than those who did.

Once you have this you're almost good to go, except you will have to
make sure that you are addressing any ethical considerations with the
research. We'll return to this a bit later, in its own section.

But fast forward to the point where you have your sample. Let's say
you've carried out all your planning, all your calculations, and you now
have a set of people who you want to assign to your control and your
treatment groups. The distinguishing feature of an RCT is the random
assignment of people in your sample to either group. How does this work?
Well the next section will explore just this.

\hypertarget{activity-2-random-assignment-into-control-and-treatment}{%
\subsection{Activity 2: Random assignment into control and
treatment}\label{activity-2-random-assignment-into-control-and-treatment}}

So how does random assignment work? Well we could achieve this by going
old-school, and writing everyone's name on a piece of paper, and drawing
the names out of a hat. But here we will use Excel's ability to
programmatically assign people into treatment or control. You are all
Excel pros by now, with your formulas, and your lookups and pivot
tables. So might as well hone these skills some more.

\begin{figure}
\centering
\includegraphics{http://resources.infosecinstitute.com/wp-content/uploads/121411_1611_SecureRando1.png}
\caption{\url{http://dilbert.com/strip/2001-10-25}}
\end{figure}

So to have random assignment to a group, each member of your sample has
to have the same probability of being selected for each group. Let's say
we want to assign two groups. We want to assign people to a
\textbf{control group} and a \textbf{treatment group}. Remember that the
control and the treatment must be must be coming from the same sample,
so that they are similar in all characteristics \emph{except} in the
particular thing you are interested in finding out the effect of. By
random assignment, each person has the same probability of being in the
treatment or the control group, and so there is no chance for systematic
bias, as there would be for example if you were asking people to
\emph{self-select} into treatment or control. If people were given the
chance to volunteer (\emph{self-select}) it would leave open a
possibility that people with certain traits are more likely to
volunteer, and there might be some systematic differences between your
two groups.

 So let's say we have our sample of people who will be assigned either
to the treatment group, of recieving the scared straigh treatment, or
the control group, who do not have to go through this treatment. Well
let's say we have our class here. We've got a total of 58 students
enrolled in the class. Go ahead and download this list, from Blackboard.
You can find it under week 5 \textgreater{} data \textgreater{}
student.xlsx

Once you have downloaded the data, open it with excel, and have a look
at it. You can see that we have 58 rows, one for each student. You can
also see that we have 3 variables: ID, Name, and Program.

To assign people randomly to a group, we can use Excel's
\texttt{=RAND()} function. The Excel RAND function returns a random
number between 0 and 1. For example, \texttt{=RAND()} will generate a
number like 0.422245717. RAND recalculates when a worksheet is opened or
changed. This will become important later.

So give it a go. Create a new column called ``random number'' on our
data. Like so:

\includegraphics{imgs/rand_col.png}

Now simply type \texttt{=RAND()} into the first cell and press Enter:

\includegraphics{imgs/type_rand.png}

When you hit enter, the formula will generate a random number, between 0
and 1. Something like this:

\includegraphics{imgs/first_rand.png}

Did you get a different number for Julia there, than I did? Chances are,
you did. This \texttt{RAND()} function generates a random number each
time used. If I did this again (and try this on your data), it will give
a different number. Try. Go into the cell, where you've typed =RAND(),
and instead of exiting out of the formula with the tick mark next to the
formula bar, just press enter again. You should, now, have another
random number appear. Now copy and paste the formula (and make sure its
the formula you're copying, and not the value) to every single student
in our sample. You will end up with a whole range of values, between 0
and 1, randomly assigned to everyone. Something like this:

\includegraphics{imgs/rand_nums.png}

Now we can use these values, which have been assigned \emph{randomly} to
assign students to a control or treatment groups. Remember last week
when we were doing some re-coding? Remember when we were recoding the
numeric variable into a categorical one? And I left a little bonus
question at the end? Well the bonus question was asking essentially,
what value can you use, as a cut-off point, to make sure that 50\% of
your data get put into one group, and 50\% of your data into the other
group? This question should already be familir to you, but let me
re-phrase: what measure of central tendency cuts your data right in half
based on considering the values on a numeric variable?

Are you thinking \textbf{median}??

\includegraphics{https://media.giphy.com/media/YcMs3OGd89Pxu/giphy.gif}

Nice work! Indeed the median is the value that divides our data right
smack in half. Now are you starting to realise where I'm going with
this? Basically, if you want to assign each student, based on this
randomly assigned score, to either control or treatment groups, and we
want to make sure that equal amounts of students go to either group,
then what we can do is use the \texttt{IF()} statements we were using
last week to re-code data!

How? Well remember what we did to assign people into tall or short
categorical variable values, based on the numeric value for height? We
decided that if a person is taller than average, they will be labelled
``Tall'' and ``Short'' if they are not. So what are the elements of our
statement? We need to check \emph{if} the person height is \emph{greater
than} the \emph{average height} and if it is, type ``Tall'', else type
``Short''. Remember now?

We can apply this again here. So to recap, we know that the
\texttt{IF()} function takes 3 arguments. 1st the logical statement,
that has to be either true or false. Then the other 2 values that the
\texttt{IF()} function needs is what to do if the statement is true, and
what to do if the statement is false. So altogether, your function will
look like this:

\texttt{=IF(condition,\ do\ if\ true,\ do\ if\ false)}

What is our condition here. Well we are using this time the median to
divide our randomly assigned numbers into 2. So we want to assign people
into treatment or control, if they are above or below the median in
their random number, that was allocated to them randomly. This ensures
the \emph{random} element of the randomized control trial, and ensures
that people have equal probability of ending up in either group.

Let's say anyone with a random number above the median will be in the
treatment group, and anyone with a random number below the median will
be in the control group. What is our condition in this case? What is it
that we are testing each number against?

Well in this case we are testing people against whether their random
number is greater than the median of all the random numbers. If their
random number is greater than the median of all random numbers, then
they are part of the treatment group (what happens if condition is
true). On the other hand, if their random number is \textbf{not} greater
than the median of all the random numbers, then they are assigned to the
control group (what happens if condition is false). If any of this is
unclear for you at the moment, then please raise your hand, and make one
of us explain this.

Translated into Excel language, our formula is:

\texttt{=IF(random\ number\ value\ \textgreater{}\ median(all\ random\ numbers),\ "Treatment",\ "Control")}

For example, for our first person in the list there, the formula will
look like:

\texttt{=IF(D2\ \textgreater{}\ MEDIAN(D:D),\ "Treatment",\ "Control")}

As such :

\includegraphics{imgs/first_rand_assig.png}

Hit Enter, and then copy the formatting all the way down, so that
everyone in the class has been assigned to the control and treatment
groups.

You should have something like this:

\includegraphics{imgs/students_assigned.png}

Don't worry if you don't get the same values, as I said this is
\emph{random} and so you should not get consistently the same answers.
In fact you should get different ones to your friends next to you as
well.

One way that you can sense check your results is to have a look at your
treatment and control groups. Let's see how many students we have in
each. You can use a pivot table to do this, and create a
\emph{univariate} \emph{frequency table} to look at this new ``Group''
variable.

You've built enough of these by now that you should be OK making this
pivot table on your own, without guidance. If you get stuck on something
though, let us know!

If all goes well, your pivot table should let you know that you have 29
students in your control group, and 29 students in your treatment group:

\includegraphics{imgs/freq_groups.png}

So, how did it go? Who's in your treatment and who's in your control
group? Where did you end up? Find your name in your sample. Are you in
the treatment or the control group? What about in the spreadsheet of the
person next to you?

If you were assigned to the treatment group, as treatment
\href{https://www.youtube.com/watch?v=sw1vm_PO8ss}{watch this Saturday
Night Live skit on Scared Straight}. Let's see what it does for your
offending\ldots{}

If you were assigned to the control group, you can instead
\href{https://www.youtube.com/watch?v=-T_pjMr7-n0}{watch this Saturday
Night Live skit on Star Wars auditions}.

Now you've been exposed to either the treatment or the control
condition. I'll be in touch in 12 months time to follow up, and find out
about your offending behaviour. I'll report the results back in a paper.
Science\ldots{}!

\hypertarget{activity-3-stratified-randomisation}{%
\subsection{Activity 3: Stratified
randomisation}\label{activity-3-stratified-randomisation}}

Randomization is important because it is almost the only way to assign
all the other variables equally except for the factor (A and B) in which
we are interested. However, some very important confounding variables
can often be assigned unequally to the two groups. This possibility
increases when the number of samples is smaller, and we can stratify the
variables and assign the two groups equally in this case.

For example, if the smoking status is very important, what will you do?
First, we have our method of randomization that we learned previously.
Just to put a name to this, the approach we had followed was
\textbf{simple randomisation} There are two randomly assigned separate
sequences for smokers and non-smokers. Smokers are assigned to the
smoker's sequences, and non-smokers are assigned to the non-smoker's
sequences. Therefore, both smokers and non-smokers groups will be placed
equally with the same numbers.

So we can use `simple randomization with/without stratification'.
However, if there are multiple stratified variables, it is difficult to
place samples in both groups equally with the same numbers. Usually two
or fewer stratified variables are recommended.

 What does that look like? Well let's illustrate again with assigning
you, the class, into control and treatment groups for the scared
straight programme. Let's say that we knew that there was a different
effect of Scared Straight possibly on those in BA Criminology and BA
Social Sciences programme participants. This means that we've identified
the \emph{program} variable to be a \textbf{confounding variable}. A
confounding variable is an outside influence that changes the effect of
a dependent and independent variable. To account for this, we want the
same number of BA Crim and BA Social Science students in both groups.
Because if we didn't control for this, it could be that through the
random assignment alone, one group would have only BA Crim students, and
the other have all the BA Social Science students. Now if we think that
program is a \textbf{confounding variable}, in other words it's
something that might influence our dependent variable, then we want to
account for this, because otherwise, even if we find a difference
between the two groups, we'll be unable to say whether it's due to the
Scared Straigh intervention, or this variable of degree program!

So how can we make sure that both BA Criminology and BA Social Sciences
groups will be placed equally with the same numbers?

Well to do this, you would go back to your random numbers that we
created with the \texttt{=RAND()} function. The important thing here is,
that you consider how we assigned people into control/treatment groups.
We decided that everyone with a random number above the total group
median gets treatment, and everyone below gets control. The total group
median was used to divide the total group into two. So what do you think
can be done to do this for 2 groups? You guessed it, \emph{you have to
assign each group based on their own group median!}.

There are two ways of doing this, the long manual hard way, or the easy
lazy code way. Let's start with the code way.

Just like before, we use an IF statement. Except this time the condition
will change. Instead of saying, if this person's random score is above
the median score for all students, we have to change it to say that ``if
this person's random number is above the median \emph{for his/her
group}'' then assign him/her to the treatment. Else assign him/her to
the group. The important extra addition there is the \emph{for his/her
group}. This means that we are changing what data to calculate the
median from.

We've calculated conditional medians before. Do you remember? This was
in the third week, when we were carrying out bivariate analysis between
a categorical and a numeric variable, and you had to calculate the
median for each possible value of the categorical variable.

Here we apply the exact same logic, but we select for each person to
calculate the median for all the others who match his/her value for the
program variable.

The outer IF statement remains exactly the same procedure as it was for
the one group, but now you are doing it for the two. This means that
only your \emph{condition} in your ``IF'' statement changes. Remember
our IF statement for assigning based on the total median?

\texttt{=IF(D2\textgreater{}MEDIAN(D:D),\ "Treatment",\ "Control")}

To change, we change only the condition. The condition here is the
\texttt{D2\textgreater{}MEDIAN(D:D)} part. That part assesses whether
the person's random number was above the total median. Well now we need
to introduce another IF into this equation, to make sure that we are
calculating the appropriate group median, for the program. So if they
student's value for the program variable is BA Crim, we should calculate
the crim median, and if it's BA Soc Sci, then we should calculate the
soc sci median. Sounds familiar?

So if total median is \texttt{MEDIAN(D:D)}, then our conditional mean
calculation is\ldots{}:

\texttt{MEDIAN(IF(C2=C2:C59,\ D2:D59))}

Where you are saying, if the cell value of C2 (so the program for the
person in row 2) is equal to the values in the program column (c
column), then include those random numbers in the calculation of the
median. So if you just replace the median statement from the initial IF
with this, creating a new condition, like so:

\texttt{D2\ \textgreater{}\ MEDIAN(IF(C2=C2:C59,\ D2:D59))}

And now, all you need to do, is plug in this new condition into your
outer IF statement:

\texttt{=IF(D2\ \textgreater{}\ MEDIAN(IF(C2=C2:C59,\ D2:D59)),\ "Treatment",\ "Control")}

Don't forget to add your dollar signs, for copy and pasting the formula:

\texttt{=IF(D2\ \textgreater{}\ MEDIAN(IF(C2=\$C\$2:\$C\$59,\ \$D\$2:\$D\$59)),\ "Treatment",\ "Control")}

And also don't forget to hit \texttt{Ctrl} + \texttt{Shift} +
\texttt{Enter} instead of \emph{just} Enter.

You should get, for each person, an evaluation against the median of the
particular group that this person belongs to. Again you can check
whether or not this has worked, in terms of assigning half the people to
treatment and the other half to control, by producing a pivot table to
look at the frequency of the stratified group variable, that you've now
created.

You might notice that your pivot table shows a no longer even
distribution between the groups:

\includegraphics{imgs/uneven_pivot.png}

Take a moment to think about why this might be? Maybe consider the
number of people in each group. Is this an even or an odd number? In the
event that there is an odd number, then when you divide the data into
half, since we are splitting people, you will end up with one group
larger than the other (by 1 person). Unless someone volunteers to be
split in half, and half of them being put in one group, and the other
half in the other, this can happen. So what can you do? Well I'll leave
this here as something for you to think about. If you have an idea, or
if you have no idea, and want to discuss, raise your hand now, we will
come around to talk through it.

But create a bivariate frequency table though, using your stratified
group variable and the program variable:

\includegraphics{imgs/biv_prog_tc.png}

You can see that both BA Crim and BA Soc Sci are split about half and
half, assigned into treatment and control.

What about using the other treatment/control grouping, the one where we
\textbf{did not} stratify by programme?

You will all have different results here, since it's randomly assigned,
but it's possible that for some of you BA Soc Sci might be
overrepresented in one group versus the other. If this were the case,
and we knew (or suspected) that this variable of what program you're
enrolled in had an effect on offending behaviour, then this might
influence the validity and generalisability from your study.

\hypertarget{freeze}{%
\subsection{Freeze!}\label{freeze}}

Now before you're done here there is one last thing to note. Remember
when describing the \texttt{=RAND()} formula, I mentioned that RAND
recalculates when a worksheet is opened or changed? Now let's say we've
assigned everyone to treatment or control conditions, we make you all
watch your appropriate videos, and then, we save our worksheet, to
return to it 12 months later. In 12 months, we may not wholly remember
who was assigned to which group. You yourself might not remember either.
But if upon reopening, the random numbers are changed, then the assigned
groups will also change! So since we don't want this, we want to somehow
``freeze the values''.

You can do this by highlighting the column and going to Formulas
\textgreater{} Settings \textgreater{} and selecting ``Calculate
Manually'':

\includegraphics{imgs/calc_man_pc.png}

On mac:

\includegraphics{imgs/calc_man.png}

And on PC:

\includegraphics{https://www.extendoffice.com/images/stories/doc-excel/keep-random-number/xdoc-keep-random-number-2.png.pagespeed.ic.7AQei1qtqU.webp}

This way you will be able to keep track of who was assigned to control
and who was assigned to the treatment groups.

\hypertarget{a-final-note-on-evidence-meta-analysis}{%
\section{A final note on evidence: Meta
Analysis}\label{a-final-note-on-evidence-meta-analysis}}

So let's say we ran our RCT on Straight and we found some sort of effect
between those exposed to it and those not. While we make all
arrangements possible to ensure the reliability, validity, and
generalisability of our study, we are only human, and we can make
mistakes. Even if we don't make mistakes, the way that inferential
statistics works, is that one out of every 20 studies will be wrong,
just probabilistically. This will make more sense if you move on to
study inferential statistics, but basically we, as social scientists,
resign ourselves to work with a 95\% confidence rate, so on the whole,
we are admitting to be wrong about 5\% of the time (1 out of 20). So
yes, while RCTs are strong, robust evidence for or against a programme,
often they can still be disputed.

This is very well illustrated in reception of actual evaluations of
Straight programmes. And there are many. This is mostly because studies
either find \textbf{no effect} or a \textbf{negative effect} of the
programme on at-risk youth. This means that it either doesn't stop their
offending, OR MAKES IT WORSE, compared with control groups. Hold on a
second?!? Why are we implementing programmes that \emph{at best} don't
work, and at worst, make offending worse? Well like all businesses,
Scared Straight programmes will tug at the credibility of individual
studies, and if you are up against something that is making a lot of
money for someone, you will need to produce quite robust evidence to
bring it down.

You may have come across something called the ``evidence pyramid''. This
is a visual representation of the hierarchy of evidence, so basically
the credibility of a study in evaluating an intervention, based on its
study design. The most robust evidence is at the top, while the least is
at the bottom.

Here it is:

\includegraphics{http://integratedtreatmentservices.co.uk/wp-content/uploads/2014/12/Screen-Shot-2014-12-18-at-10.20.50.png}

Well here is \emph{a} version. There are many versions of this out
there. But you can see, for all we've been praising RCTs here, there is
actually a step above. This is \emph{systematic reviews}. I'm not going
to hugely go into this, but essentially these represent the \emph{study
of studies}. So what a systematic review, and in particular, it's
subset, the \textbf{meta-analysis} do, is consider \emph{all previous
studies} and evaluate their results. That way, you can draw conclusions
and say that many studies are all finding that Scared Straight has no or
negative effect, and it becomes much harder to criticise!

Meta-analysis is the quantitative analysis of findings from multiple
studies. At its core, meta-analysis involves researchers pulling
together the results of several studies and making summary, empirical
statements about some cause and effect relationship. A classic example
of meta-analysis in criminology was performed by Wells and Rankin and
concerned the relationship between broken homes and delinquency.

After observing a series of findings showing that the
broken-homes-causes-delinquency hypothesis was inconclusive, Wells and
Rankin identified fifty studies that tested this hypothesis. After
coding the key characteristics of the studies, such as the population
sampled, age range, measures (both independent and dependent) used, the
authors found that the average effect of broken homes across the studies
was to increase the probability of delinquency by about 10 to 15
percent. Perhaps more importantly, they found that the different methods
used across the studies accounted for much of the variation in
estimating the effect of broken homes. For example, the effect of broken
homes on delinquency tended to be greater in studies using official
records rather than self-report surveys.

Although the research community has not spoken with one voice regarding
the usefulness of meta-analysis, one thing is clear: meta-analysis makes
the research community aware that it is inappropriate to base
conclusions on the findings of one study. It is because of this
important lesson that meta-analysis has become a popular technique in
criminological and criminal justice research.

If you are still interested in the outcomes of Scared Straight, you can
read a
\href{http://onlinelibrary.wiley.com/doi/10.1002/14651858.CD002796.pub2/full}{meta
analysis here}. TL;DR: It doesn't work

\hypertarget{longitudinal-data}{%
\section{Longitudinal data}\label{longitudinal-data}}

There are two commonly used longitudinal research designs,** panel
\textbf{and }cohort** studies. Both study the same group over a period
of time and are generally concerned with assessing within- and
between-group change. Panel studies follow the same group or sample over
time, while cohort studies examine more specific populations (i.e.,
cohorts) as they change over time. Panel studies typically interview the
same set of people at two or more periods of time.

For example, the 1970 British Cohort Study (BCS70) follows the lives of
more than 17,000 people born in England, Scotland and Wales in a single
week of 1970. Over the course of cohort members' lives, the BCS70 has
broadened from a strictly medical focus at birth to collect information
on health, physical, educational and social development, and economic
circumstances among other factors.

The Millennium Cohort Study (MCS), which began in 2000, is conducted by
the Centre for Longitudinal Studies (CLS). It aims to chart the
conditions of social, economic and health advantages and disadvantages
facing children born at the start of the 21st century.

Our Future (formerly the Longitudinal Study of Young People in England
(LSYPE2)), is a major longitudinal study of young people that began in
2013. It aims to track a sample of over 13,000 young people from the age
of 13/14 annually through to the age of 20 (seven waves).

These are some examples from the UK Data Service
(\href{https://www.ukdataservice.ac.uk/get-data/key-data/cohort-and-longitudinal-studies}{see
those and more here})

The main advantage of longitudinal studies is that you can track change
over time, and you meet the temporal criteria for causality. You collect
data from people across multiple \textbf{waves}. Waves refer to the
times of data collection in your data. For example, if you follow a
cohort from birth until their 30th birthday, and you take measurements
every 10 years, once at point of birth, once at age 10, once at age 20,
and finally at age 30, then you will have 4 waves in this longitudinal
data about these people you're following.

\hypertarget{what-does-longitudinal-data-look-like}{%
\subsection{What does longitudinal data look
like?}\label{what-does-longitudinal-data-look-like}}

So far we've only shown you cross-sectional data. Each row was one
observation, each column as one variable, and they were collected at a
single point in time. So what do longitudinal data look like? A
longitudinal study generally yields multiple or ``repeated''
measurements on each subject. So you will have many, repeated measure,
from the same person, or neighbourhood, or whatever it is that you are
studying (most likely people though\ldots{} when you take repeated
observations about places it's more likely to be a time-series designs.
Time-series designs typically involve variations of multiple
observations of the same group (i.e., person, city, area, etc.) over
time or at successive points in time. Typically, they analyze a single
variable (such as the crime rate) at successive time periods, and are
especially useful for studies of the impact of new laws or social
programs. An example of a time-series design would be to examine the
burglary rate across the boroughs of Greater Manchester over the last
five years. We'll be dealing with time series in week 7.)

Okay so what do these data actually look like? Well have a look at the
description for the
\href{https://discover.ukdataservice.ac.uk/series/?sn=2000030}{Next
Steps (formerly the Longitudinal Study of Young People in England
(LSYPE1))}. Briefly mentioned above, the Next Steps (formerly the
Longitudinal Study of Young People in England (LSYPE1)) is a major
longitudinal study that follows the lives of around 16,000 people born
in 1989-90 in England. The first seven sweeps of the study (2004-2010)
were funded and managed by the Department for Education (DfE) and mainly
focused on the educational and early labour market experiences of young
people.

The study began in 2004 and included young people in Year 9 who attended
state and independent schools in England. Following the initial survey
at age 13-14, the cohort members were interviewed every year until 2010.
The survey data have also been linked to the National Pupil Database
(NPD) records, including cohort members' individual scores at Key Stage
2, 3 and 4.

In 2013 the management of Next Steps was transferred to the Centre for
Longitudinal Studies (CLS) at the UCL Institute of Education and in 2015
Next Steps was restarted, under the management of CLS, to find out how
the lives of the cohort members had turned out at age 25. It maintained
the strong focus on education, but the content was broadened to become a
more multi-disciplinary research resource.

There are now two separate studies that began under the LSYPE programme.
The second study, Our Future (formerly LSYPE2), began in 2013 and will
track a sample of over 13,000 young people from the age of 13/14
annually through to the age of 20 (seven waves).

There are a lot of interesting variables in there for those interested
in young people and delinquent behaviour. There is some data about drug
and alcohol, some about offending such as graffiti, vandalism,
shoplifting, as well as social control factors such as family
relationship, bullying, and so on. If you are interested in the data,
you can always have a browse through the
\href{https://discover.ukdataservice.ac.uk/Catalogue/?sn=5545\&type=Data\%20catalogue\&lt}{site
here} and
\href{http://doc.ukdataservice.ac.uk/doc/5545/mrdoc/pdf/5545age_25_survey_questionnaire.pdf}{have
a read of one of the questionnaires as well}.

In any case, we should get back to our question, what does this data
look like. And it looks exactly as you would imagine, it looks like the
results of survey questionnaires, completed by people, but over time. If
you were to download the next steps data for example, you will end up
with a separate file for each wave. But in each wave, you would have
\textbf{repeat measures} of the same variables, from the same people. So
each wave you see the exact same variables, and the exact same people
making up the rows of answers, but you know that time has passed.

The benefit of a longitudinal study is that researchers are able to
detect developments or changes in the characteristics of the target
population at both the group and the individual level. The key here is
that longitudinal studies extend beyond a single moment in time. As a
result, they can establish sequences of events. It is generally admitted
that causes precede their effects in time. This usually justifies the
preference for longitudinal studies over cross-sectional ones, because
the former allow the modelling of the dynamic process generating the
outcome, while the latter cannot. Supporters of the longitudinal view
make two interrelated claims: (i) causal inference requires following
the same individuals over time, and (ii) no causal inference can be
drawn from cross-sectional data.

Anyway have a look at a small subset of 3 waves of the data. There are 4
variables in each wave. The first one, ``NSID'' is the unique identifier
for each person. Then there are three variables that contain the answers
that each person gave to some questions. * W1canntryYP* is the answer to
whether the young person ever tried Cannabis. \emph{W1alceverYP} is the
answer to whether the young person ever had proper alcoholic drink, and
\emph{W1cignowYP} is the answer to whether the young person ever smoked
cigarettes.

So you can download these three waves of young people being surveyed
from blackboard. You can find them labelled wave\_1.xlsx, wave\_2.xlsx,
and wave\_3.xlsx in the data folder for this week on BB. Download all
three onto your computer, and open them up in excel.

\hypertarget{activity-4-linking-data}{%
\subsection{Activity 4: Linking data}\label{activity-4-linking-data}}

So hopefully if I've taught you anything about the structure of data, is
that you have \emph{all} your observations in your rows and \emph{all}
your variables in your columns. So if you want to be able to look at
changes in people's responses over time, for example, you will need to
be able to link these data sets together into one spreadsheet.

So how do we do this? Well what you can do is to link one data set with
another. Data linking is used to bring together information from
different sources in order to create a new, richer dataset. This
involves identifying and combining information from corresponding
records on each of the different source datasets. The records in the
resulting linked dataset contain some data from each of the source
datasets. Most linking techniques combine records from different
datasets if they refer to the same entity. (An entity may be a person,
organisation, household or even a geographic region.)

You can merge (combine) rows from one table into another just by pasting
them in the first empty cells below the target table---the table grows
in size to include the new rows. And if the rows in both tables match
up, you can merge columns from one table with another by pasting them in
the first empty cells to the right of the table---again, the table
grows, this time to include the new columns.

Merging rows is pretty straightforward, but merging columns can be
tricky if the rows of one table don't always line up with the rows in
the other table. By using \texttt{VLOOKUP()}, you can avoid some of the
alignment problems.

To merge tables, you can use the VLOOKUP function to lookup and retrieve
data from one table to the other. To use \texttt{VLOOKUP()} this way,
both tables must share a common id or key.

This is a standard ``exact match'' \texttt{VLOOKUP()} formula (remember
that means you have to set the last parameter to `FALSE' for an exact
match).

So first things first, open up all three waves in three separate excel
spreadsheets. Have a look at them all. You can see the first one has the
following columns:

\begin{itemize}
\tightlist
\item
  \emph{NSID}: the unique ID
\item
  \emph{W1canntryYP}: ever tried cannabis\\
\item
  \emph{W1cignowYP}: ever smoked
\item
  \emph{W1alceverYP}: ever had alcohol
\end{itemize}

Then you can have a look at wave 2. You will see in wave two that the
unique ID column stays the same (\emph{NSID}: the unique ID), but the
other three columns are named slightly different:

\begin{itemize}
\tightlist
\item
  \emph{W2canntryYP}: ever tried cannabis\\
\item
  \emph{W2cignowYP}: ever smoked
\item
  \emph{W2alceverYP}: ever had alcohol
\end{itemize}

It might be a subtle difference, but the first two characters in the
variable name actually refer to the wave in which this variable was
collected. This is very handy, because if you imagine that they were
just called ``canntryYP'' and ``cignowYP'' and ``alceverYP'', once they
would be joined together into one data set, then how would you be able
to tell which one came from which wave? You could rename them yourself
(which is what you would do in this case) but it's very nice that these
data were already collected with this joining in mind, and so the
variable naming was addressed for us in this way.

If you're still curious, have a look at wave 3, where you will see the
familiar NSID column, as well as these three:

\begin{itemize}
\tightlist
\item
  \emph{W3canntryYP}: ever tried cannabis\\
\item
  \emph{W3cignowYP}: ever smoked
\item
  \emph{W3alceverYP}: ever had alcohol
\end{itemize}

So why doesn't the NSID column change? Well this is the same value for
all participants all throughout. This is so that we can identify each
one. Due to ethics and the data protection act, we cannot share data
that contains personally identifiable information, especially in cases
where it refers to some pretty sensitive stuff, such as someone's drug
use, alcohol use, or some delinquent behaviour. Instead each person is
given a unique code. This code can be used to track them, over time,
without identifying them personally.

\includegraphics{imgs/fn2187.png}

You need a unique identifier to be present for each row in all the data
sets that you wish to join. This is how Excel knows what values belong
to what row! What you are doing is matching each value from one table to
the next, using this unique identified column, that exists in both
tables. For example, let's say we have two data sets from some people in
Hawkins, Indiana. In one data set we collected information about their
age. In another one, we collected information about their hair colour.
If we collected some information that is unique to each observation, and
this is the \emph{same} in both sets of data, for example their names,
then we can link them up, based on this information. Something like
this:

\includegraphics{imgs/merge_logic_1.png}

And by doing so, we produce a final table that contains all values,
lined up \emph{correctly} for each individual observation, like this:

\includegraphics{imgs/merge_logic_2.png}

This is all we are doing, when merging tables, is we are making use that
we line up the correct value for all the variables, for all our
observations.

So let's do this with our young people. Let's say we want to look at the
extent of cannabis, alcohol, and cigarette trying in each wave of our
cohort, as they age. To do this, we need to link all the waves in to one
data set. We have established that the variable \emph{NSID} is an
anonymous identifier, that is unique to each person, so we can use that
to link their answers in each wave.

So remember the parameters you need to pass to the \texttt{VLOOKUP()}
function, from last week? You need to tell it:

\begin{itemize}
\tightlist
\item
  first \emph{what value to match},
\item
  then \emph{where the lookup table is},
\item
  then \emph{which column of this table you want},
\item
  and finally \emph{whether or not you want exact match}.
\end{itemize}

In this case, we want to match the unique identifier, found for each
person in the \emph{NSID} column. This is the value to match. Then our
lookup table is now \emph{the other data set} which we want to link. The
column will be the matching column to what we are copying over, and the
exact match parameter we will set to ``FALSE'' (meaning we \emph{do}
want exact matches only).

So what does this look like in practice?

Well let's open up our wave 1 (well technically you have them all open,
so just bring wave 1 to the front). Now we don't want to overwrite this
file, so save it as something new. Do this by selecting File
\textgreater{} Save As\ldots{} and choosing where to save it, and giving
it a name. Here I will save it in the same folder where I've saved the
individual waves data, and call it ``next\_stepsw1-3.xlsx'' as it will
contain waves one through three of the next steps longitudinal survey:

\includegraphics{imgs/rename.png}

Now that you have this as a new file (which already contains the data
from wave 1), you can get ready to merge in the data from waves 2 and 3.
First, lets create column headers for the variables we will copy over.
You can do this by simply copying over the column headers from the other
data sets (waves 2 and 3). Like so:

\includegraphics{imgs/merge_copy_headers.png}

Notice that I'm not copying over the NSID column. This is because it
would be exactly the same. It's enough to have this once, there is no
need for three replications of the same exact column. If you are
participant NS23533L, you will always have this value for the NSID
column. This is used to match all your answers, and to copy over into
this sheet, but it is \emph{not} itself copied over. If this is
confusing as to why, just raise your hand now, and we will come around
to talk through it.

Right so now we have all our column headers, let's copy over the column
contents. When you type in the \texttt{VLOOKUP()} function into Excel,
it gives you a handy reminder of all the elements you need to complete:

\includegraphics{imgs/vlookup_hints.png}

\begin{itemize}
\tightlist
\item
  \emph{lookup value} - what value to match,
\item
  \emph{table array} - where the lookup table is,
\item
  \emph{col index num} - which column of this table you want,
\item
  and finally \emph{range lookup} - whether or not you want exact match.
\end{itemize}

Our lookup value will be the NSID for this particular person. Here we
find this in cell A2:

\includegraphics{imgs/lookup_val_nsid.png}

Now the table array is the lookup table. Where can we find the values
for ``W2canntryYP''? Well this is in the data set for wave 2. We've
grabbed data from other sheets before, but never from a totally
different file\ldots{}! However the process is exactly the same. All you
need to do, is find the data set, and select the range that represents
your lookup table, which is all the data in this sheet!

Something like this:

\includegraphics{imgs/select_from_w2.gif}

You can see that by going to the sheet and highlighting the appropriate
columns, your formula bar there, in your original file, is populated
with the code to refer to those columns in that file! Just like we did
when grabbing data from a different sheet within the same file.

Remember the reference of a cell is
\texttt{column\ letter\ +\ row\ number}? The reference for a cell from a
sheet is \texttt{sheet\ name\ +\ !\ +\ column\ letter\ +\ row\ number}?
Well the reference for a cell from a sheet on an entirely different file
is
\texttt{{[}\ +\ file\ name\ +\ {]}\ +\ sheet\ name\ +\ !\ +\ column\ letter\ +\ row\ number}.

So you can see that by clicking and highlighting, excel has
automatically populated with the reference, which in my case is:

\texttt{{[}wave\_2.xlsx{]}wave\_2.csv!\$A:\$D} - which means I want from
wave\_2.xlsx file, the wave\_2.csv sheet, columns A through D (static,
because I've included the dollar signs there).

If any of this is unclear flag us down to talk through these now. You've
been slowly building up to this though. We have gradually made formulas
more and more complex, so all these are, are formulas you've learned
before, but all patched together, to be making some new formulas.

So now we still have two parameters to define, the column index number,
and the range lookup. Column index number asks you, which column, from
your reference table do you want to grab. Since we've copied over our
headings in order, we know that the first heading will be column number
2 (column 1 contains the reference IDs in the NSID variable. Remember
the reference tables you made for recoding last week? Same concept, the
values \emph{to match} are in the first column). Then the second one
will be column number 3, and the third, column number 4. So in this
case, our column index number is 2, and the range lookup is FALSE,
because we want an \emph{exact match}.

So our final formula looks like this:

\texttt{=VLOOKUP(A2,{[}wave\_2.xlsx{]}wave\_2.csv!\$A:\$D,2,FALSE)}

\includegraphics{imgs/merge_first_form.png}

\textbf{NOTE} it's possible that for some of you (likely those on PCs)
there will be quotes around the sheet reference in this formula,
something like this:

\texttt{=VLOOKUP(A2,\textquotesingle{}{[}wave\_2(1).xlsx{]}wave\_2.csv\textquotesingle{}!\$A:\$D,4,FALSE)}

see the \texttt{\textquotesingle{}} around the
\texttt{{[}wave\_2(1).xlsx{]}wave\_2.csv}? You \emph{might} see this in
your version. But still achieves the same thing.

Double click on the little blue square on the bottom right hand of the
blue frame around this cell to copy the formula all the way to the
bottom.

To get the other two columns from the same sheet, you use the
\emph{exact same formula} except you change the column index number.
Does it make sense why you do this? Because you're grabbing a
\emph{different column}. You're always grabbing the one that corresponds
to your header, which you've copied over. If this is unclear make sure
to raise your hand, so we can go through this. It's worth going through
it even if you feel like you get it, as you will be doing this again in
your task, and it's something much easier explained in person!

So now, copy the formula for the next two columns, change the column
index number to 3 and to 4 as appropriate:

\includegraphics{imgs/VLOOKUP.jpg}

and you will see the values for each person from both wave 1 and wave 2
in there:

\includegraphics{imgs/nas_present.png}

You can see that for some rows, in wave 2 we have a value of
\texttt{\#N/A}. This is because, that NSID is \emph{not} found in the
wave 2 data.

One issue with longitudinal studies is something called
\textbf{attrition}. Attrition occurs when cases are lost from a sample
over time or over a series of sequential processes. One form of sample
attrition occurs in longitudinal research when the subjects studied drop
out of the research for a variety of reasons, which can include:
unwillingness of subjects to continue to participate in research,
difficulties in tracing original respondents for follow-up (for example,
because of change of address) and nonavailability for other reasons (for
example, death, serious illness). A survey of major longitudinal studies
in the United States found that the average attrition rate was 17 per
cent (Capaldi and Patterson, 1987, cited in Sapsford and Jupp, 1996).
Therefore, quite a lot of cases may be lost. Attrition is one of the
major methodological problems in longitudinal studies. It can
deteriorate generalizability of findings if participants who stay in a
study differ from those who drop out.

What you are seeing here are the presence of people who took part in
wave 1, but not in wave 2. Attrition rates are important to know and
mention in analysis of longitudinal data, to be able to discuss the
issues which it may cause, as described above and in your readings.

Attrition is only one cause of \textbf{missing data}. Sooner or later
(usually sooner), anyone who does statistical analysis runs into
problems with missing data. In a typical data set, information is
missing for some variables for some cases. In surveys that ask people to
report their income, for example, a sizable fraction of the respondents
typically refuse to answer. Outright refusals are only one cause of
missing data. In self-administered surveys, people often overlook or
forget to answer some of the questions. Even trained interviewers
occasionally may neglect to ask some questions. Sometimes respondents
say that they just do not know the answer or do not have the information
available to them. Sometimes the question is inapplicable to some
respondents, such as asking unmarried people to rate the quality of
their marriage. In longitudinal studies, people who are interviewed in
one wave may die or move away before the next wave. When data are
collated from multiple administrative records, some records may have
become inadvertently lost.

You can see our second person with NAs there, NS15760C, also has NA
values for the first wave. This means that while NS15760C was
interviewed in the 1st wave (and potentially in waves 2 and 3 as well),
they did not answer these questions! This could be because they put one
of the answers that were coded as ``NA'', such as Refused to answer,
wrote ``Not applicable'', or responses ``Don't know'', or because they
were unable to complete or refused this whole section. Missing data is
important, and it's important to know \emph{why} your data is missing.
When people refuse to answer something, it might be motivated by very
different things than when people say ``don't know'' to something. If
people with certain characteristics are more likely to not respond, then
there might be systematic biases introduced through missing data. This
is important to keep in mind.

 OK so let's copy over wave 3 as well, and then we can have a look at
our attrition and so on rates.

Have a go at doing this on your own, following the steps from when we
copied over wave 2, but this time from the wave 3 file.

If you need a little nudge, the formula which I ended up with was:
\texttt{=VLOOKUP(A2,{[}wave\_3.xls{]}wave\_3.csv!\$A:\$D,2,FALSE)}.
Yours might look something similar.

Then, when that is done, you will see a final data set with all the
three waves of these questions present in your data:

\includegraphics{imgs/merged_data_final.png}

We picked up in the \#N/A last time, there is another issue with
longitudinal, and generally self-report data collection methods,
available to spot here. Remember that all of our questions are asking
people whether they have \emph{ever} smoked, or \emph{ever} tried
cannabis, or \emph{ever} had an alcoholic drink. So surely, once someone
answers yes, you would expect them to keep answering yes, correct? Well,
have a look at respondent number ``NS23533L''. While in wave 2, they
admit to trying all cannabis and cigarettes and alcohol, in wave three
they seem to have forgotten this experience, and report that they have
not ever tried either cannabis or cigarettes. The issue here is called
\textbf{response bias}.

Response bias is a general term for anything that influences the
responses of participants away from an accurate or truthful response.
These biases are most prevalent in the types of studies and research
that involve participant self-report, such as structured interviews or
surveys. It can be caused by a variety of factors, for example the
phrasing of questions in surveys, the demeanour of the researcher, the
way the experiment is conducted, or the desires of the participant to be
a good experimental subject and to provide socially desirable responses
may affect the response in some way. All of these " artefact" of survey
and self-report research may have the potential to damage the validity
of a measure or study. Because of response bias, it is possible that
some study results are due to a systematic response bias rather than the
hypothesized effect, which can have a profound effect on psychological
and other types of research using questionnaires or surveys. It is
therefore important for researchers to be aware of response bias and the
effect it can have on their research so that they can attempt to prevent
it from impacting their findings in a negative manner. Response biases
can have a large impact on the validity of questionnaires or surveys.

There isn't much that we can do (at the stage of data analysis) to
control for this. If you suspect that you will encounter response bias,
you should consider this in your research design, and build measures
into the data collection phase, that try to account for or at least
identify sources of response bias in your survey.

\hypertarget{complete-cases-an-approach-to-missing-data}{%
\section{Complete cases (an approach to missing
data)}\label{complete-cases-an-approach-to-missing-data}}

There are a vast range of statistical techniques for accommodating
missing data (see \url{www.missingdata.org.uk}). Perhaps the most
commonly adopted is to simply exclude those participants in our dataset
who have any data missing (in those variables we are concerned with)
from our analysis. This is what is commonly known as a `\textbf{complete
case analysis}' or `listwise deletion' - we analyse only the complete
cases. This approach simply says ``we will not deal with any of the
missing data'', and instead subsets the analysis to the sample where
participants have answered every question - in other words, only use the
rows which do not have missing data.

If data are missing complete randomly, meaning that the chance of data
being missing is unrelated to any of the variables involved in our
analysis, a complete case analysis is unbiased. This is because the
subset of complete cases represent a random (albeit smaller than
intended) sample from the population. In general, if the complete cases
are systematically different from the sample as a whole (i.e.~different
to the incomplete cases), i.e.~the data are not missing completely
randomly, analysing only the complete cases will lead to biased
estimates.

For example, suppose we are interested in estimating the median income
of the some population. We send out an email asking a questionnaire to
be completed, amongst which participants are asked to say how much they
earn. But only a proportion of the target sample return the
questionnaire, and so we have missing incomes for the remaining people.
If those that returned an answer to the income question have
systematically higher or lower incomes than those who did not return an
answer, the median income of the complete cases will be biased. This is
something to keep in mind when choosing the route of complete case
analysis.

\hypertarget{activity-5-selecting-complete-cases}{%
\subsection{Activity 5: Selecting Complete
Cases}\label{activity-5-selecting-complete-cases}}

But how can we include only the compete cases? Well for this you can use
the filter function of Excel. Remember the little funnel icon? Well if
you go to the Data tab, you will see it:

\includegraphics{imgs/cc_filter.png}

If you click on the Filter icon, you will see small downwards arrows
appear on the column headers for your data, like so:

\includegraphics{imgs/cc_arrows.png}

If you click on these arrows, you can see all the possible values that
the variable can take, and you can see little check boxes next to these
values. If you click in them you can toggle the tick/untick of these
boxes, which means that you can hide the values which are not ticked. So
in the first column, untick any value that is not ``Yes'' or ``No'',
like so:

\includegraphics{imgs/untick_not_yn.png}

You can see that any rows that had an NA value have been hidden. For
example in the above image you can see that row 18 is gone, and instead
we see row 17 followed by row 19. You can repeat this for all your rows,
and you will end up with only cases where the person has answered
``Yes'' or ``No'' to these questions across all three waves of the
study. You now have only the \emph{complete cases} for your analysis.

Filtering only hides these rows though, and they could still show up in
your analysis. You don't really want to delete data, because that's
never a good idea in case you make a mistake, or want to go back and
re-do some analysis this time including some missing variables as well.
Instead, one thing that you could do is to copy the complete cases only,
over to a new sheet called ``complete cases''. To do this, create a new
sheet (remember, plus sign at bottom of spreadsheet) and call it
``complete cases''.

\includegraphics{imgs/cc_new_sheet.png}

Then go back to your wave\_1 sheet and copy all the columns, and then go
back to your new, complete cases sheet, and paste in the values. You now
have a new sheet, in your excel workbook, that has only the complete
cases.

So what do these complete cases look like? I can tell you, that there
were originally 1000 people in this sample that I've subset for you. So
now, after you've removed all the NAs, and have only complete cases, how
many complete cases do you have?

If you did the same thing to me, you should have 622 cases left. Pretty
big attrition rate, eh? Something to think about\ldots{}!

Now, finally, since we've worked so hard on this data, let's have a look
at it. Can you tell me, whether the percent of those who answered all
questions in all three waves who \emph{have} tried cannabis increases
from wave to wave?

If your gut reaction to this question was to do with fear and confusion,
one thing you could do is think back to all the skills that we've
learned so far, and which one of these you would need to draw on, to be
able to answer this question. First, if you want to know if the percent
of people who have tried cannabis (that is - answered yes to the
question about trying cannabis) is greater for each wave than the wave
before, you need to find out: what percent of people tried cannabis in
wave 1, what about wave 2, and what about wave 3? In our newly created,
complete cases data set, we know that we have one variable for cannabis
trying in each wave. These are W1canntryYP for wave 1, W2canntryYP for
wave 2, and W3canntryYP for wave 3. How can you find out the \% who said
yes for each one of these? Well remember our univariate analysis of a
categorical variable, where we can find out the count of values for each
variable with a pivot table? And then how we can translate those into
percentages?

If not, then refer back to your notes from the 2nd week on univariate
analysis. If yes, you now know you need to make 3 pivot tables, and save
the answers from each. You can then combine those into a \textbf{complex
table}. Remember complex tables from the feedback session after the
bivariate analysis labs, on week 3?

So you can do something like this:

\includegraphics{imgs/cc_build_ct.gif}

In the end you will end up with a table like this:

\includegraphics{imgs/final_cc_ct.png}

And you can even get fancy and plot this change over time, to emphasise
that indeed, there is an increase in the percent of respondents who
answer yes to trying cannabis wave on wave:

\includegraphics{imgs/incr_yes_can.png}

But we'll do more plotting and visualisation after reading week, when we
have our data viz session.

\hypertarget{ethics}{%
\section{Ethics}\label{ethics}}

I wanted to leave you with a final note on research ethics. It's an
essential part of your research design that you consider the ethical
implication of your study, both on your participants, on your
researchers (including yourself), and on the wider community.

Watch \href{https://www.youtube.com/watch?v=Zbi7nIbAuMQ}{this 7 minute
video that gives a good introduction to research ethics}, and pay
particular attention to the concepts of:

\begin{itemize}
\tightlist
\item
  informed consent
\item
  beneficence (benefits and harms to society)
\item
  justice
\end{itemize}

The ``IRB'' section is specific to the USA, however, we also have our
own code of ethics, and all research needs to undergo an ethical review.
Internally, the University of Manchester has created an ethics decision
tool. You can navigate through this tool to determine whether or not
your research requires ethical approval. You can access the tool, and
read more about the university's ethics procedures
\href{https://www.manchester.ac.uk/research/environment/governance/ethics/}{here}

\hypertarget{summary-4}{%
\section{Summary}\label{summary-4}}

In sum, you should now be able to think about the research design that
either you need to create to collect data, or that someone else has
created in order to collect the data that you are working with. There
are many decisions that go into designing a research study, and there
are pros and cons associated with each approach. When you use randomised
control trials, you have to consider the random assignment, when you use
longitudinal data, you will have to join the data sets collected at
different points in time, and think about things like attrition. The
study design has implications for what research questions the data you
collect will allow you to answer, and what analysis you'll be able to
carry out. You should be comfortable with the following terms:

\begin{itemize}
\tightlist
\item
  dependent (response/ outcome) variable
\item
  independent (predictor) variable
\item
  treatment
\item
  control
\item
  evaluation
\item
  simple randomization
\item
  stratified randomisation
\item
  confounding variable
\item
  waves
\item
  attrition
\item
  missing data
\item
  response bias
\item
  complete cases
\item
  ethics

  \begin{itemize}
  \tightlist
  \item
    informed consent
  \item
    beneficence (benefits and harms to society)
  \item
    justice
  \end{itemize}
\end{itemize}

\hypertarget{week6}{%
\chapter{Week 6}\label{week6}}

\hypertarget{learning-outcomes-5}{%
\section{Learning outcomes}\label{learning-outcomes-5}}

This week is the most fun week in all of data analysis - the week where
we learn about the principles of data visualisation. The visual display
of your data is so important because it gives you a chance to
communicate what is interesting in pictures. Simple, neat graphs can
tell you in one glance, what you might have to read many paragraphs of
text to otherwise learn. Visualisation is an art and a science, and this
week we will explore the work in this area, and hopefully you will only
produce beautiful, and meaningful visualisations of your data from now
on.

Here are some terms that we will cover today:

\begin{itemize}
\tightlist
\item
  Visualising data
\item
  Principles of good data visualisation

  \begin{itemize}
  \tightlist
  \item
    Ink to data ratio
  \end{itemize}
\item
  Grammar of graphics
\item
  Exploratory viz
\item
  Communicating results
\end{itemize}

\hypertarget{visualising-data}{%
\section{Visualising data}\label{visualising-data}}

A picture is worth a thousand words; when presenting and interpreting
data this basic idea also applies. There has been, indeed, a growing
shift in data analysis toward more visual approaches to both
interpretation and dissemination of numerical analysis. Part of the new
data revolution consists in the mixing of ideas from visualisation of
statistical analysis and visual design. Indeed data visualisation is one
of the most interesting areas of development in the field.

Good graphics not only help researchers to make their data easier to
understand by the general public. They are also a useful way for
understanding the data ourselves. In many ways it is very often a more
intuitive way to understand patterns in our data than trying to look at
numerical results presented in a tabular form.

Recent research has revealed that papers which have good graphics are
perceived as overall more clear and more interesting, and their authors
perceived as smarter (see \href{https://vimeo.com/181771433}{this
presentation})

\hypertarget{why-visualise-data}{%
\section{Why visualise data?}\label{why-visualise-data}}

New insights. Visualising data can give you new insights into your own
data (exploratory data visualisation) as well as effectively communicate
the results from your studies to your audiences.

Interactive data visualisation is something that has the power to really
engage people with a topic. Instead of just passively telling people
numbers in a table, visualisations can help engage your reader. But
don't take my word for it, give it a go yourself! Have a look at
\href{https://www.nytimes.com/interactive/2017/04/14/upshot/drug-overdose-epidemic-you-draw-it.html}{this
article in The Upshot (NYT's data-driven venture, focused on politics,
policy and economic analysis)}. It should be an interesting little
activity - you can draw the graph that represents the trends you think
are happening, and then compare with the actual figures. Seriously try
it out, it should be fun!

So we won't quite be learning how to make these kinds of interactive
visualisations, but we will learn some basic principles behind effective
data visualization. By the end of today you should also have a practical
sense for why some graphs and figures work well, while others may fail
to inform or actively mislead. You will know how to create a wide range
of plots in Excel as well as how to refine plots for effective
presentation.

\hypertarget{anatomy-of-a-plot---the-grammar-of-graphics}{%
\section{Anatomy of a plot - the Grammar of
Graphics}\label{anatomy-of-a-plot---the-grammar-of-graphics}}

\begin{quote}
The grammar of graphics takes us beyond a limited set of charts (words)
to an almost unlimited world of graphical forms (statements). The rules
of graphics grammar are sometimes mathematical and sometimes aesthetic.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

The grammar of graphics is about creating graphs mathematically.
Essentially the philosophy behind this as that all graphics are made up
of layers, the idea that you can build every graph from the same few
components: a data set, a set of geoms---visual marks that represent
data points, and a coordinate system.

Take this example (taken from \emph{Wickham, H. (2010). A layered
grammar of graphics. Journal of Computational and Graphical Statistics,
19(1), 3-28.})

You have a table such as:

\includegraphics{/Users/reka/Desktop/course-incubator/images/table.png}

You then want to plot this. To do so, you want to create a plot that
combines the following layers:

\includegraphics{/Users/reka/Desktop/course-incubator/images/layers.png}

This will result in a final plot:

\includegraphics{/Users/reka/Desktop/course-incubator/images/combined.png}

\begin{quote}
We often call graphics charts (from or Latin charta, a leaf of paper or
papyrus). There are pie charts, bar charts, line charts, and so on.
{[}The Grammar of Graphics{]} shuns chart typologies. For one thing,
charts are usually instances of much more general objects. Once we
understand that a pie is a divided bar in polar coordinates, we can
construct other polar graphics that are less well known. We will also
come to realize why a histogram is not a bar chart and why many other
graphics that look similar nevertheless have different grammars.
(\ldots{}) The concept of a graphic is so general that we need
organizing principles to create instances of graphics. We may not want
to put a pie chart in a catalog, but we need to give users some simple
way to produce one.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

\hypertarget{principles-of-good-data-visualisation}{%
\section{Principles of good data
visualisation}\label{principles-of-good-data-visualisation}}

There is a vast amount of research into what works in displaying
quantitative information. The classic book is
\href{https://www.edwardtufte.com/tufte/books_vdqi}{The Visual Dispay of
Quantitative Information by Edward Tufte}, but since him there are many
other researchers as well who focus on approaches to displaying data.
Your reading for this week will provide you a crash course into data
viz.

The \textbf{Data-Ink ratio} is a concept introduced by Edward Tufte, the
expert whose work has contributed significantly to designing effective
data presentations. In his 1983 book, The Visual Display of Quantitative
Data, he stated the goal is to ``Above all else show the data''.

\begin{quote}
A large share of ink on a graphic should present data-information, the
ink changing as the data change. Data-ink is the non-erasable core of a
graphic, the non-redundant ink arranged in response to variation in the
numbers represented
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{https://www.edwardtufte.com/tufte/books_vdqi}{Tufte, 1983}
\end{itemize}

Tufte refers to data-ink as the non-erasable ink used for the
presentation of data. If data-ink would be removed from the image, the
graphic would lose the content. Non-Data-Ink is accordingly the ink that
does not transport the information but it is used for scales, labels and
edges. The data-ink ratio is the proportion of Ink that is used to
present actual data compared to the total amount of ink (or pixels) used
in the entire display. (Ratio of Data-Ink to non-Data-Ink).

\includegraphics{http://www.infovis-wiki.net/images/thumb/5/55/DIR.jpg/600px-DIR.jpg}

Good graphics should include only data-Ink. Non-Data-Ink is to be
deleted everywhere where possible. The reason for this is to avoid
drawing the attention of viewers of the data presentation to irrelevant
elements. The goal is to design a display with the highest possible
data-ink ratio (that is, as close to the total of 1.0), without
eliminating something that is necessary for effective communication.

\#\#\#~An example:

This is an example of a graph with a low Data-Ink Ratio:

\includegraphics{http://www.infovis-wiki.net/images/thumb/2/2e/Dir1.png/400px-Dir1.png}

The border around the graph, the background color and the grid lines are
all unnecessary data ink.

Now an example of a graph with a high Data-Ink Ratio:

\includegraphics{http://www.infovis-wiki.net/images/thumb/1/1b/Dir2.png/400px-Dir2.png}

We have deleted the border around the graph, the background color and
the grid lines and have thus drawn the viewer's attention to horizontal
scales that are data-ink. There is nothing else to distract and the key
features of the data stand out clearly.

\hypertarget{criticisms}{%
\subsection{Criticisms}\label{criticisms}}

Inbar, et al, evaluated in 2007 the people's acceptance of the
minimalist approach to visualize information. They asked 87 students to
rate their preference for two different graphs displaying identical
information - a standard bar-graph and a minimalist version
\href{http://portal.acm.org/citation.cfm?id=1362587}{Inbar, 2007}. The
results showed that the majority students did not like Tufte's
minimalist design of bar-graphs - instead they seem to prefer
``chartjunk''.
\href{http://portal.acm.org/citation.cfm?id=1362587}{Inbar, 2007}.

In the example shown above, increasing the data-ink ratio made it harder
to read most of the data. For example, removing the top border of the
chart removed an implied 20\% line. It also made it harder to see how
much the graph lies (in that it does not show a range from 0\% to 100\%,
and/or does not show the domain from January through December).
\href{https://www.librarysearch.manchester.ac.uk/primo-explore/fulldisplay?docid=44MAN_ALMA_DS21134626320001631\&context=L\&vid=MU_NUI\&search_scope=BLENDED\&isFrbr=true\&tab=local\&lang=en_US}{How
to Lie with Statistics} discusses this flaw in the example charts.

\hypertarget{what-makes-a-bad-graph-bad}{%
\section{What makes a bad graph bad?}\label{what-makes-a-bad-graph-bad}}

Whether or not you subscribe to Tufte's school of minimalism, you should
be able to recognise \emph{bad} graphs. What makes a bad graph bad
though? The generic overview answer to this is that bad graphs are the
ones where it becomes difficult for your audience to interpret the
meaning you are trying to convey.

On the other hand, your audience might have some sort of expectations
for what they require from you. You will have to manage a route between
what you might be told that you need to produce, for example by a boss,
and what is the best method for visualising your data in a way that
communicates your results effectively.

A good example of this dilemma is the pie chart.

\begin{quote}
A pie chart is perhaps the most ubiquitous of modern graphics. It has
been reviled by statisticians (unjustifiably) and adored by managers
(unjustifiably). It may be the most concrete chart, in the sense that it
is a pie. A five-year-old can look at a slice and be a fairly good judge
of proportion. (To prevent bias, give the child the knife and someone
else the first choice of slices.) The pie is so popular nowadays that
graphical operating systems include a primitive function for drawing a
pie slice.
\end{quote}

\begin{itemize}
\tightlist
\item
  Leland Wilkinson (2005) \emph{The Grammar of Graphics}
\end{itemize}

Indeed, a lot of people shun the pie chart (see for example this blog
entry titled
\href{http://www.storytellingwithdata.com/blog/2011/07/death-to-pie-charts}{death
to pie charts}) or this story from Business Insider titled
\href{http://www.businessinsider.com/pie-charts-are-the-worst-2013-6?IR=T}{pie
charts are the words}, but managers have a particular affinity towards
it. While these seem emotional and unfair, there is actually
justification for these. People are actually \emph{not} that great in
telling proportions from pie charts. If you are interested, have a look
at
\href{https://eagereyes.org/blog/2016/a-reanalysis-of-a-study-about-square-pie-charts-from-2009}{this
study}, where researchers found that a square pie chart performs the
best, when people have to guess the proportion that it represents.

There are other charts as well, which are less popular to hate, but in
certain situations may obscure important information. In some cases bar
plots can hide important features of your data, and might not be the
most appropriate means for comparison. See the below image for example,
where the same data about 2 groups, green and purple, are visualised
using 3 different methods, a histogram, which shows the green group
following a normal distribution, and the purple group following a
heavily skewed distribution (remember week 5), a boxplot that shows the
same, and finally a bar plot, which makes the green and purple group
appear identical:

\includegraphics{https://pagepiccinini.files.wordpress.com/2016/02/barplot_psa1.jpg}

Now potentially, the
\href{https://www.kickstarter.com/projects/1474588473/barbarplots/description}{kickstarter
campaign} around actually banning bar plots might be a bit of an extreme
leap, but it is important to keep in mind that the kind of visualisation
that you choose might greatly impact the conclusions that people will
draw about your data, and the story that you are able to tell.

There are some recommendations on what to use (and not use) in certain
contexts, which can help you avoid making a bad graph. For example, most
data visualisation experts agree that you should not use 3D graphics
unless there is a meaning to the third dimension. So using 3D graphics
just for decoration, as in
\href{https://mir-s3-cdn-cf.behance.net/project_modules/disp/2505dd10837923.56030acd2ef20.jpg}{this
case} is normally frowned upon. However there are cases when including a
third dimension is vital to communicating your findings. See this
\href{http://www.visualisingdata.com/2015/03/when-3d-works/}{example}.

You can see also an example in the graph below:

\includegraphics{http://socviz.co/assets/ch-02-chartjunk-life-expectancy.png}

Because of the angle and the 3D you cannot really read off the extent of
differences between regions.

We want to create pictures of data that people, including ourselves, can
look at and learn from.

But it is not always enough for you to know the perfect visualisation,
it is important that you also know that your audience is comfortable
interpreting these types of visualisations. Before I became a lecturer
at Manchester, I worked as a crime analyst. I love data, and I was just
coming out of my education, so I felt very comfortable with stats and
data analysis, however my lack of real-world experience was made very
evident when I entered my first ever briefing, with 3 Chief Inspectors
from the Met Police to present them the work I'd done analysing
confidence in police. My second slide was a set of boxplots, comparing
the scores on a public attitudes survey between their sectors. It might
have looked something like this (the data is fictitious by the way):

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-25-1.pdf}

It was useless. I spent basically all my alloted time trying to talk
through the graph, and it achieved the opposite effect of clearly
displaying information, and telling the story of the different levels of
confidence in each Sector. I did not take my audience into account, and
that made my visualisation ineffective. It was not a great moment, but
at least I got an embarrassing story to tell when teaching people about
data visualisation.

\hypertarget{what-graph-should-i-use}{%
\section{What graph should I use?}\label{what-graph-should-i-use}}

There are a lot of points to consider when you are choosing what graph
to use to visually represent your data. There are some best practice
guidelines, but at the end of the day, you need to consider what is best
for your data. What do you want to show? What graph will best
communicate your message? Is it a comparison between groups? Is it the
frequency distribution of 1 variable?

As some guidance, you can use the below
\href{https://flowingdata.com/2009/01/15/flow-chart-shows-you-what-chart-to-use/}{cheatsheet,
taken from Nathan Yau's blog Flowingdata}:

\includegraphics{https://i1.wp.com/flowingdata.com/wp-content/uploads/2009/01/chart-chart1.jpg}

However, keep in mind that this is more of a guideline, aimed to nudge
you in the right direction. There are many ways to visualise the same
data, and sometimes you might want to experiment with some of these, see
what the differences are. You can also consider some inspiration
\href{http://datavizproject.com/}{here}.

Channels for mapping ordered data (continuous or other quantitative
measures), arranged top-to-bottom from more to less effective, after
Munzer (2014, 102) created by \href{http://socviz.co/}{Kieran Healy}:

\includegraphics{http://socviz.co/assets/ch-02-channels-for-cont-data-vertical.png}

Channels for mapping unordered categorical data, arranged top-to-bottom
from more to less effective, after Munzer (2014, 102) created by
\href{http://socviz.co/}{Kieran Healy}:

\includegraphics{http://socviz.co/assets/ch-02-channels-for-cat-data-vertical.png}

\hypertarget{edges-contrasts-and-colors}{%
\section{Edges, Contrasts and Colors}\label{edges-contrasts-and-colors}}

Looking at pictures of data means looking at lines, shapes, and colors.
Our visual system works in a way that makes some things easier for us to
see than others. I am speaking in slightly vague terms here because the
underlying details are the remit of vision science, and the exact
mechanisms responsible are often the subject of ongoing research. I will
not pretend to summarize or evaluate this material. In any case,
independent of detailed explanation, the existence of the perceptual
phenomena themselves can often be directly demonstrated through visual
effects or ``optical illusions'' of various kinds. These effects
demonstrate that, perception is not a simple matter of direct visual
inputs producing straightforward mental representations of their
content. Rather, our visual system is tuned to accomplish some tasks
very well, and this comes at a cost in other ways.

The active nature of perception has long been recognized. The Hermann
grid effect, shown in the figure below, was discovered in 1870. Ghostly
blobs seem to appear at the intersections in the grid but only as long
as one is not looking at them directly.

\includegraphics{http://socviz.co/assets/wk-02-perception-hermann-grid-effect.jpg}

A related effect is shown below. These are Mach bands. When the gray
bars share a boundary, the apparent contrast between them appears to
increase. Speaking loosely, we can say that our visual system is trying
to construct a representation of what it is looking at based more on
relative differences in the luminance (or brightness) of the bars,
rather than their absolute value.

\includegraphics{http://socviz.co/assets/ch-02-mach-bands-horizontal.png}

Similarly, the ghostly blobs in the Hermann grid effect can be thought
of as a side-effect of the visual system being tuned for a different
task.

These sorts of effects extend to the role of background contrasts. The
same shade of gray will be perceived very differently depending on
whether it is against a darker background our a lighter one. Our ability
to distinguish shades of brightness is not uniform, either. We are
better at distinguishing darker shades than we are at distinguishing
lighter ones. And the effects interact, too. We will do better at
distinguishing very light shades of gray when they are set against a
light background. When set against a dark background, differences in the
middle-range of the light-to-dark spectrum are easier to distinguish.

Our visual system is attracted to edges, and we assess contrast and
brightness in terms of relative rather than absolute values. Some of the
more spectacular visual effects exploit our mostly successful efforts to
construct representations of surfaces, shapes, and objects based on what
we are seeing. Edward Adelson's checkershadow illusion, shown below, is
a good example.

\includegraphics{http://socviz.co/assets/ch-02-perception-adelson-checkershow.jpg}

\hypertarget{colour}{%
\section{Colour}\label{colour}}

When choosing color schemes, we will want mappings from data to color
that are not just numerically but also perceptually uniform. The goal in
each case is to generate a perceptually uniform scheme, where hops from
one level to the next are seen as having the same magnitude. Excel will
take care of these for you, by offering colour palettes in the
``Design'' section of the Chart Layout tab:

\includegraphics{http://www.java2s.com/Tutorial/Microsoft-Office-Excel-2007Images/Apply_Chart_Style___Click_More_List_Arrow_In_Chart_Styl.PNG}

Gradients or sequential scales from low to high are one of three sorts
of color palettes. When we are representing a scale with a neutral
mid-point (as when we are showing temperatures, for instance, or
variance in either direction from a zero point or a mean value), we want
a diverging scale, where the steps away from the midpoint are
perceptually even in both directions. The blue-to-red palette in the
design layouts above is one example. Finally, perceptual uniformity
matters for unordered categorical variables as well. We often use color
to represent data for different countries, or political parties, or
types of people, and so on. In those cases we want the colors in our
qualitative palette to be easily distinguishable, but also have the same
valence for the viewer. Unless we are doing it deliberately, we do not
want one color to perceptually dominate the others.

The main message here is that you should generally not put together your
color palettes in an ad hoc way. It is too easy to go astray. In
addition to the considerations we have been discussing, there we might
also want to avoid producing plots that confuse people who are colour
blind, for example, and color blindness comes in a variety of forms.
Fortunately for us, almost all of the work has been done for us already.
Different color spaces have been defined and standardized in ways that
account for these uneven or nonlinear aspects of human color perception.

A good resource website is \href{http://colorbrewer2.org/}{colorbrewer}.
This site offers many colour schemes that you can use in your graphs if
you wanted to introduce manual colours. The site looks somewhat like
this:

\includegraphics{http://pic.accessify.com/thumbnails/777x423/c/colorbrewer2.org.png}

Go to the site, and select a colour scheme that you like. When you do
you can see that you can adjust the number of categories that you need
to create a colour scheme for:

\includegraphics{imgs/choose_col_num.png}

You will set these to the number of values in your categorical variable
for example.

Then you can also select the colur scheme that is most appropriate for
your variable. If you have ordinal categories for example, then you
could use a sequential scale - going from dark blue to light blue. On
the other hand, if you have nominal variable, then a sequential colour
scale would not make sense. In this case you would use a qualitative
scale. If you are unsure why this is the case, raise your hand now, and
we can help explain.

\includegraphics{imgs/choose_col_vartyp.png}

You might be thinking that that's real nice, but how do we get these
colours into our excel graphs? Well you may notice the code next to the
colours:

\includegraphics{imgs/how_to_copy_cols.png}

These are ways for the computer to be able to understand what the value
is for that colour. There are a few options. Here we see HEX values for
each colour.

A hex triplet is a six-digit, three-byte hexadecimal number used in
HTML, CSS, SVG, and other computing applications to represent colors.
The bytes represent the red, green and blue components of the color. One
byte represents a number in the range 00 to FF (in hexadecimal
notation), or 0 to 255 in decimal notation. This represents the least
(0) to the most (255) intensity of each of the color components. Thus
web colors specify colors in the True Color (24-bit RGB) color scheme.
The hex triplet is formed by concatenating three bytes in hexadecimal
notation, in the following order:

\begin{itemize}
\tightlist
\item
  Byte 1: red value (color type red)
\item
  Byte 2: green value (color type green)
\item
  Byte 3: blue value (color type blue)
\end{itemize}

Not sure if you might still be the generation that had any interaction
with MySpace, but that was an excellent venue to learn about hex colours
and html customisation. I guess a potentially more relevant venue would
be tumlbr, if any of you use this and want to customise your pager,
\href{https://www.tumblr.com/docs/en/custom_themes}{you can use html to
do this}. Now you don't need to do this at all, but you should now know,
that if you want to change the colour of something, you need to know the
hex code for this colour. And this is what colourbrewer is telling you
above.

If you want the three colours you see there you have to use the codes
\texttt{\#ffeda0}, \texttt{\#feb24c}, \texttt{\#f03b20}.

You can also change the display, from HEX to RGB or CMYK codes, which
are just used in different contexts. You can ask colourbrewer to display
these codes instead by changing the value in the dropdown menu:

\includegraphics{imgs/choose_col_code.png}

If this is something you're interested in I'm happy to chat about these,
raise your hand now.

Right, but how do we get these colours into our excel graph? Well let's
give it a go.

\hypertarget{activity-1-custom-colours-in-excel-graphs.}{%
\subsection{Activity 1: Custom colours in Excel
graphs.}\label{activity-1-custom-colours-in-excel-graphs.}}

 Download the FBI crime statistics data from Blackboard under the Week 6
materials in course content.

Open the data up in excel, and have a look at it. You can see that it
include the number of crimes for various crime types for each year from
1994 to 2013, as well as some columns for crime rate. Crime rate is
important because it normalises the \emph{number} of crimes by the
population at risk. Why is this important?

Well think about this - where do you expect more pickpocketing
incidents, outside Piccadilly station, or in Platt Fields Park? Why?

My guess is that you said Piccadilly station, and because there are more
people close together. There are \textbf{more possible targets}. That's
what you are accounting for when calculating crime rate. In this case,
they are accounting for changes in the population between the years, to
make sure that you can compare the crimes between the years. It might
not make a lot of sense to say that a particular crime is increasing if
the population is also increasing, because as a percentage, the crime
might not actually be increasing at all. So instead we consider the
population.

But is the population increasing? Well lets create a column graph of the
population.

To do this, select the \emph{Population} column, and choose the
clustered column graph option:

\includegraphics{imgs/desc_viz_1.png}

You can see however that the category axis labels are not very
meaningful. They are only numbers, from 1 to 20, and do not help you
answer whether population changes between the years.

\includegraphics{imgs/pop_blue.png}

To add axis labels, right click anywhere on the chart area, and select
the ``Select Data\ldots{}'' option:

\includegraphics{imgs/pb2.png}

Then, click into the text box next to the \emph{Category (X) axis
labels}, and then select the values in the \emph{Year} column. Make sure
to select only the values, and not also the column header:

\includegraphics{imgs/pb3.png}

Now we can see the differences. However what we wanted to demonstrate
here is how to change the colour of your graphs to what you wanted,
potentially some colours from colourbrewer.

Well to do this, you can double click on any of the bars, which should
open up a popup window.

\includegraphics{imgs/manual_fill_1.png}

On this, select the ``Fill'' option. On this you can see there is a
dropdown menu, where you can select the fill.

If you're on a PC, if you double click on the bars to change the fill,
the pop up window doesn't have the color option immediately like what is
shown above (for mac). Instead you will have to first select the option
for ``Solid fill'', an then the colour option will appear:

\includegraphics{imgs/pc_solid_fill.png}

Click on the option for ``More Colours\ldots{}''

\includegraphics{imgs/manual_fill_2.png}

This opens up a new set of options. You can see it's set to RGB sliders.
If you wanted to, you could use the RGB code, and set the red, green,
and blue levels in a way that gets you your colour. If you are using a
PC you will have to use the RGB code for the rest of the exercise (we'll
get to this in a second). If you're using a mac, you can also just paste
your hex code in the box below the sliders, that says ``Hex Color \#''.

\includegraphics{imgs/manual_fill_4.png}

So let's say that we want to change the colour to the middle value from
the colourbrewer scale above. Well we can see that the hex code for that
is \texttt{\#feb24c}. So what we need to do is change the code in the
text box above, where it says ``FFFFFF'' to ``feb24c'':

\includegraphics{imgs/manual_fill_5.png}

If you're on a PC, then you will have to change the drop down selection
menu on the colorbrewer2.org:

\includegraphics{imgs/choose_col_code.png}

and set it to ``RGB'' scale. This will give you the values for the red,
green, and blue sliders. In this case, that will be:

\begin{itemize}
\tightlist
\item
  R: 254
\item
  G: 178
\item
  B: 76
\end{itemize}

Enter those values in the pop-up window that appears:

\includegraphics{imgs/pc_rbg.png}

So set each colour to this value, and click ``OK'', and ta-daaa, your
graphs will appear with manual colour:

\includegraphics{imgs/manual_fill_6.png}

In this case we don't have a stacked bar, we just have the one variable,
so it's not hugely useful, but I wanted to demonstrate how you can
insert your own colours there. You might want to do this later, in the
more complex graphs. Your decisions about color will focus more on when
and how it should be used. Colour is a powerful channel for picking out
visual elements of interest, and can really make a difference to your
graph. You are very welcome to use the excel default colours in your
graphs within this course, but if you're interested in learning more
about colour
\href{https://lisacharlotterost.github.io/2016/04/22/Colors-for-DataVis/}{have
a look at this page}.

\hypertarget{reading-between-the-lines-points}{%
\section{Reading between the lines
(points)}\label{reading-between-the-lines-points}}

What sorts of relationships are inferred, and under what circumstances?
In general we want to identify groupings, classifications, or entities
than can be treated as the same thing or part of the same thing:

\begin{itemize}
\tightlist
\item
  \textbf{Proximity:} Things that are spatially near to one another are
  related.
\item
  \textbf{Similarity:} Things that look alike are related.
\item
  \textbf{Connection:} Things that are visually tied to one another are
  related.
\item
  \textbf{Continuity:} Partially hidden objects are completed into
  familiar shapes.
\item
  \textbf{Closure:} Incomplete shapes are perceived as complete.
\item
  \textbf{Figure/Ground:} Visual elements are either in the foreground
  or the background.
\item
  \textbf{Common Fate:} Elements sharing a direction of movement are
  perceived as a unit.
\end{itemize}

\hypertarget{exploratory-data-visualisation}{%
\section{Exploratory data
visualisation}\label{exploratory-data-visualisation}}

Data visualisation helps you unlock the hidden meaning in your data. It
is the first tool of the data analyst. When you are given a heap of
data, the only way to start getting some insight into it is to start
making some visualisations. If you remember in weeks 2 and 3, when we
did our univariate and bivariate analyses, we always started with
graphing our data. And in some instances, visualising data itself can
lead to surprising insights.

I suggest that in your own time, you listen to
\href{http://datastori.es/66-iquantnyc/}{this episode of the datastories
podcast} which consists of an interview with Ben Wellington, the author
of the blog \href{http://iquantny.tumblr.com/}{I Quant NY}. I mentioned
him in the opening lecture, he's the guy who found the
\href{http://iquantny.tumblr.com/post/87573867759/success-how-nyc-open-data-and-reddit-saved-new}{fire
hydrant that earns \$30,000 a year}. It's a good interview, so
definitely bookmark it for later!

Exploratory data visualisation is a way for you to get to know your
data. It is a way to explore patterns and trends that you might not
immediately see. We've covered some of this when we were performing
univariate and bivariate analyses, but we will focus on how data
visualisation can help you answer questions about your data, and also
spend some time on making sure these graphs are in line with good
practice.

\hypertarget{comparing-categories}{%
\subsection{Comparing categories}\label{comparing-categories}}

 Let's practice answering questions with data. Go back to the FBI data
you downloaded from Blackboard.

So firstly, let's say we want to know the answer to the following
question:

\begin{itemize}
\tightlist
\item
  \textbf{What year had the highest violent crime rate?}
\end{itemize}

You can see that the appropriate data is in the column labelled
\emph{Violent Crime Rate}. So what do you think is the best way to
visualise this? You could look at your chart selection thought starter
above, and see that one option for comparisons is to use column graphs.
So let's give that a go:

Select the column that contains the data for violent crime rate.
Highlight it. When you have highlighted that column then go to your
chart selector, and select \emph{Column} \textgreater{} \emph{2-D
Column} \textgreater{} \emph{Clustered Column}. Like so:

\includegraphics{imgs/desc_viz_1.png}

Once you select this, a default column chart should appear. You can see
however that the category axis labels are not very meaningful. They are
only numbers, from 1 to 20, and do not help you answer your question,
\emph{What year had the highest violent crime rate?}.

\includegraphics{imgs/desc_viz_2.png}

To add axis labels, right click anywhere on the chart area, and select
the ``Select Data\ldots{}'' option:

\includegraphics{imgs/desc_viz_3.png}

Then, click into the text box next to the \emph{Category (X) axis
labels}, and then select the values in the \emph{Year} column. Make sure
to select only the values, and not also the column header:

\includegraphics{imgs/desc_viz_4.png}

Once you've done that, click on ``OK'', and you will see the labels
updated:

\includegraphics{imgs/desc_viz_5.png}

Now you can see that your x-axis labels are meaningful, and essentially
from this point on, you can use this graph to answer your question:
\emph{What year had the highest violent crime rate?}.

We can see that 1994 had the highest violent crime rate.

But this graph is not necessarily in line with the best practice around
data ink, and around all of the visuals introduced having meaning.

Firstly, we are talking about number of crimes per 100,000 population.
Since we are talking about \textbf{number of crimes} we know that we
will be talking about whole numbers. Even if we end up with some decimal
points after dividing by population to get the rate, it would not
necessarily be meaningful to report fractions of crimes. So we can
remove the .0 from the y-axis labels, as it adds no value to our graph.
To do this, double click on the text in the y-axis labels. It will bring
up a window where you can see some choices to select what to edit, in
the left hand side of the window.

It might automatically be selected to ``Scale'':

\includegraphics{imgs/desc_viz_7.png}

Click on the next one down, the one that says ``Number''. In the options
that come up, \emph{untick} the box next to ``Linked to source''. Once
you untick this box you should have available some options for you to
make changes. You can then edit the text box next to where it says
``Decimal places'', and set it to 0, like so:

\includegraphics{imgs/desc_viz_8.png}

You can now also go to the other options, and make changes there. For
example, you might decide that you want to increase the font size, to
make your graph easier to read, and also that you might want to change
your font to something easier to read, such as Arial:

\includegraphics{imgs/desc_viz_9.png}

On a PC, the ``format axis'' window has different options. There is no
font or text box options for example. However you can do those things on
the main screen of Excel (using the toolbar at the top). Just select the
axis and change the fonts from there. Or you can change the font with
this menu as well:

\includegraphics{imgs/pc_change_font.jpg}

Click ``OK'' when you are finished.

If you change your font, and also your font size, you should make sure
to apply to all of your axis labels. To do this, just double click on
the x-axis labels (the years) and the same pop-up window will appear.
Make sure to set the font to the same font, and the font size to the
same size for both axes, to ensure consistency:

\includegraphics{imgs/desc_viz_10.png}

The other bit of information you're providing here that doesn't really
have much meaning attached to it here is colour. What does the blue in
the bars mean? And why is there a gradient? Why are the bars lighter
blue on the top, and darker blue on the bottom? These are questions that
people looking at this chart might ask, and we, the people who made the
chart, do not actually have any good answers to! (actually on the PC
there might not be a gradient, but on a mac there is, and just in case,
it's good to note this).

So to get around this, let's set our bars to solid black, to get around
any possible issues with introducing colour. To do this, this time
double click on any of the bars, to open up a popup window. Again there
will be options on the left hand side. First select the option for
``Line'' and make sure that you set this to ``No Line'':

\includegraphics{imgs/desc_viz_11.png}

Then click on ``Shadow'' and make sure to just untick the box next to
where it says ``Shadow''. Again there is no point to introduce a shadow
here, as it would give you no additional information, it represents no
data, and therefore can only introduce confusion into your chart.

\includegraphics{imgs/desc_viz_12.png}

Finally, select fill, and set your fill colour to black:

\includegraphics{imgs/desc_viz_13.png}

Once you are done with all this, you can click on ``OK'' and you will
have your final bar graph ready to be inserted into a report:

\includegraphics{imgs/final_bar.png}

Ta-daaa. So is this the best graph to answer our question? Well you
could explore a few more, and see what happens.

\hypertarget{activity-2-exploring-trends}{%
\subsection{Activity 2: Exploring
trends}\label{activity-2-exploring-trends}}

We'll be talking about changes over time and trends and such next week,
but it's interesting to note here how it would (or at least should)
influence your chart choice if the question we ask is slightly
different. Let's say we are still interested in trends in violent crime
rate, but instead of asking which year had the highest violent crime
rate, instead this time we are interested in a new question:

\begin{itemize}
\tightlist
\item
  \textbf{Is violent crime rate going up, going down, or staying the
  same?}
\end{itemize}

Now we could, in theory, answer this question by looking at our bar
graphs above. But the slight difference is, bar graphs are good at
comparing \textbf{categories}. But what they dont do, is they do
\emph{not} link up these categories. And rightly so. Such charts are for
visualising categorical variables, which should not hugely link.

However, those of you still on top of your \emph{levels of measurement}
will have noticed, that year can actually be considered a continuous
variable. And so it might lend itself to better \emph{trend}
visualisation by being presented through a \emph{line graph}.

 To select this, once again highlight the column with the data of
interest (violent crime rate) and then this time select Line
\textgreater{} Line for chart type:

\includegraphics{imgs/desc_line_1.png}

Once again you will see that your x-axis labels are not very helpful,
they are numbersfrom 1 to 20, rather than labelled with the years that
each data point represents. To address this, once again right-click
anywhere in the chart, and select the ``Select Data\ldots{}'' option:

\includegraphics{imgs/desc_line_2.png}

In the popup, once again click into the text box next to the
\emph{Category (X) axis labels}, and then select the values in the
\emph{Year} column. Make sure to select only the values, and not also
the column header:

\includegraphics{imgs/desc_viz_4.png}

When you click OK, the labels should appear on your chart:

\includegraphics{imgs/desc_line_3.png}

Now you don't always have to manually change the bits of the graph, you
can also use some pre-created layouts that exist. You can find this in
the top bar of excel, under a label called ``Chart Quick Layouts'':

\includegraphics{imgs/desc_line_layout.png}

Again, a difference on the PC: When editing the line graph, there is not
a heading called ``Chart Quick Layouts''. Instead, you can find the
options for different layouts under the \textbf{``chart tools''} section
and the tab called \textbf{``design''}.

\includegraphics{imgs/pc_chart_tools.png}

You can browse through and select one that looks like something you
would like.

Once you pick a quick chart format, you can still make edits to your
graphs. For example, you can double click on the line to make another
popup window appear.

On this new window, you can click on ``Line'' option, and choose the
colour you would like for the line.

\includegraphics{imgs/desc_line_edit.png}

You can also see 3 options along the topic there, where you can choose
between whether you want to edit the line etc, under the ``Solid''
option, but also change gradients under the ``Gradient'' option, and
also edit the style of the line using the ``Weights \& Arrows'' option.
Click on this, and have a play with this as well:

\includegraphics{imgs/desc_line_type.png}

Again these might be slightly different on your version, or on a PC to a
mac. For editing the line, all the options there are different as well.
The main point here is for you to just play around and see what is
available to change in the graph, so have a look, but here is the
equivalent PC screen shot to show you:

\includegraphics{imgs/pc_line_2.png}

\includegraphics{imgs/pc_edit_line.png}

You can also add point markers to the line, this can help add clarity to
your graph. Select the ``Marker Style'' option and you can choose a
marker.

\includegraphics{imgs/desc_line_markers.png}

On PC:

\includegraphics{imgs/pc_line_1.png}

\includegraphics{imgs/pc_edit_marker.png}

Make any changes you would like, and finally click ``OK'' to produce
your final graph.

\includegraphics{imgs/desc_line_final.png}

\hypertarget{activity-3-making-a-chart-template}{%
\subsection{Activity 3: Making a chart
template}\label{activity-3-making-a-chart-template}}

 Now you will probably find your own style for a graph, and you might
not want to change these settings every time you create a graph. Let's
say we want to create a separate graph for the rates of various crime
types in this data. To make our lives easier, once we've gotten one
graph to look just how we would like, we can save this as a template.

To do this, click anywhere in the graph, and select ``Save as
Template\ldots{}''.

\includegraphics{imgs/save_as_template.png}

On a PC you won't have this option appear from the right-clicked menu,
but you can find the ``Save As Template'' on the Chart tools
\textgreater{} Design tab on the top menu in Excel:

\includegraphics{imgs/pc_save_template.png}

Choose a location to save, give it a name you might remember (here I
call this ``bw'', short for black and white):

\includegraphics{imgs/name_template.png}

On PC:

\includegraphics{imgs/pc_save_template_3.png}

Now, next time you build a graph you can use this template. Let's try,
let's create another line graph this time for the rate of murder and
nonnegligent manslaughter. So as you would, select the column with the
data for rate of murder and nonnegligent manslaughter:

\includegraphics{imgs/select_mm_col.png}

but instead of selecting Line graph, click on the ``Other'' option for
graphs, and scroll to the bottom. You should see your template appear as
one of the possible selections!

\includegraphics{imgs/choose_template.png}

Again on the PC this is slightly different, you first create your graph
with the line graph, and then when it's created, you right click
anywhere in the chart and choose:

\includegraphics{imgs/pc_apply_template.png}

Then go to templates, and choose ``My templates'':

\includegraphics{imgs/pc_my_templates.png}

Click on the template, then click OK, and you should have your graph be
updated to the new template.

You can now see that the default chart that appears follows the
formatting that you have carefully devised as your ideal format. You
will still have to add any sort of data addition (for example specify
where to find the years to populate the Category (X) axis labels). But
it definitely saves you some time in terms of formatting.

\hypertarget{activity-4-saving-your-graph}{%
\subsection{Activity 4: Saving your
graph}\label{activity-4-saving-your-graph}}

 When you are making your reports, there are two ways you can include
your graph.

One approach is just to save your graphs as pictures. To do this, you
can just right click anywhere in the graph area, and select the ``Save
as picture\ldots{}'' option.

\includegraphics{imgs/save_as_pic.png}

Navigate to a folder where you collect your results, and save the image
there.

\includegraphics{imgs/save_pic_2.png}

Of course, because nothing is easy, this option does not exist on PC.
Instead, perhaps the best option may just be copying the graph and
pasting it into a word doc and saving it there.

Then when you are writing your essay, you can insert an image using the
Insert \textgreater{} Picture from file option:

\includegraphics{imgs/find_pic.png}

This will open up a popup window you can use to navigate to the picture
you just saved, and selecting it. When you are done, click ``OK'', and
your graph will appear :

\includegraphics{imgs/inserted_graph.png}

It is important that you label your graphs. You should include a short
and to-the-point caption for the graph. You should also refer to the
graph in your writing. Something like this:

\includegraphics{imgs/ref_and_caption.png}

\hypertarget{acvitity-5-more-complex-graphs}{%
\section{Acvitity 5: More complex
graphs}\label{acvitity-5-more-complex-graphs}}

While we only covered descriptive analysis for univariate and bivariate
analysis, when it comes to making sense of your data graphically, you
can include further variables.

 Let's show an example now.

In the FBI data, you can see that we have quite a few columns
(variables) for crime rates for various crime types. We looked at rates
of violent crime and also murder and nonnegligent manslaughter, but
there is also robbery, Aggravated assault, burglary, and some more.
Let's say that we want to compare the trajectories of all these
variables over the years, over time.

You can start with a simple line graph for one of the variables. Let's
select the violent crime rate, and build a line chart for this variable.
You should be able to do this by now without guidance, but if you need
assistance, just scroll up to where we did this earlier.

Now once you've created this graph, right-click anywhere on the chart
itself, and select the ``Select Data\ldots{}'' option:

\includegraphics{imgs/comp_c_1.png}

In the popup that appears, under the series box, click on the ``Add''
button:

\includegraphics{imgs/comp_c_2.png}

Once you clicked on the ``Add'' button, the field for ``Name'' and ``Y
values'' should appear empty. Click in the text box next to name, and
then click on the column header (variable name) for the next variable.
Select ``Robbery Rate'':

\includegraphics{imgs/comp_c_3.png}

Then, click in the text box next to the ``Y values:'' and then select
the values in the robbery rate column:

\includegraphics{imgs/comp_c_4.png}

Then repeat this for every variable you want to add. For every variable,
click on ``Add'', then for ``Name'' select the column header, and for
``Y values'' select the values for that variable.

Repeat this for ``Aggravated Assault Rate'' and again for ``Burglary
Rate'':

\includegraphics{imgs/comp_c_5.png}

Finally, when you have added all these variables, click on OK, make any
changes you'd like to, to the graph, and then ta-daa you will see all 4
variables on one graph:

\includegraphics{imgs/comp_c_6.png}

You could also create a stacked column chart, that shows you the
cumulative crime rate in each year, while separating out for categories,
using all the same variables.

Again, start with building a column chart for just one of the variables.
Let's build this for rate of violent crimes:

\includegraphics{imgs/stackedbar_1.png}

So that will create this column chart:

\includegraphics{imgs/stackedbar_2.png}

Now, just as we did with the line charts, just right-click anywhere on
the chart area, and again select ``Select Data\ldots{}''.

\includegraphics{imgs/stackedbar_3.png}

And again this will bring up a popup. Here, once again you can add
variables with the ``Add'' button:

\includegraphics{imgs/stackedbar_4.png}

Once you clicked on the ``Add'' button, the field for ``Name'' and ``Y
values'' should appear empty. Click in the text box next to ``Name'',
and then click on the column header (variable name) for the next
variable. Select ``Robbery Rate'' column header. Then click in the text
box next to ``Y values'', and then select the values for robbery rate:

\includegraphics{imgs/stackedbar_5.png}

Then repeat this for every variable you want to add. For every variable,
click on ``Add'', then for ``Name'' select the column header, and for
``Y values'' select the values for that variable. So in this case, again
as we did for the line graphs, repeat this for ``Aggravated Assault
Rate'' and again for ``Burglary Rate'':

\includegraphics{imgs/comp_c_5.png}

Then finally you should end up with a stacked bar chart of all these
variables, that allows you to compare the rate of each crime type
between years, but also allows you to compare a cumulative crime rate,
if you consider the crimes of Violent crimes, Robbery, Assault, and
Burglary together:

\includegraphics{imgs/stackedbar_final.png}

So you can see that you can use data visualisation as a way of putting
together many variables into one graph.

\hypertarget{more-guidance-on-chart-design}{%
\section{More guidance on chart
design}\label{more-guidance-on-chart-design}}

Read through this list on
\href{https://guides.library.duke.edu/datavis/topten}{``Dos and Don'ts
of Charts and Graphs''} and have a look at some of
\href{http://datajournalismhandbook.org/1.0/en/introduction_3.html}{these
examples} as well for some further inspiration.

\hypertarget{communicating-results}{%
\section{Communicating results}\label{communicating-results}}

We have been covering exploratory data analysis, where you take your
data, and produce visualisations from that, but sometimes you can also
compliment your results by visualising your tables and other outputs. An
example would be the conditional formatting, which we covered in week 3.

The most important thing when communicating your results is that you
know your audience. Know what they understand, what they don't, and what
you can tell them in one graph. Your graph has to tell a story. Think
about why you are making it? What is the message that you want to
convey? What is the story this graph is telling? You need to be clear
with this to yourself, to make sure that your graph accomplishes it's
mission in telling an interesting story about your data.

\begin{quote}
You should look at your data. Graphs and charts let you explore and
learn about the structure of the information you collect. Good data
visualizations also make it easier to communicate your ideas and
findings to other people. Beyond that, producing effective plots from
your own data is the best way to develop a good eye for reading and
understanding graphs---good and bad---made by others, whether presented
in research articles, business slide decks, public policy advocacy, or
media reports.
\end{quote}

\begin{itemize}
\tightlist
\item
  \href{http://socviz.co/}{Kieran Healy}
\end{itemize}

\hypertarget{activity-6-interpreting-results}{%
\subsection{Activity 6: Interpreting
results}\label{activity-6-interpreting-results}}

You should, by becoming a maker of good graphs, become literate about
graphs as well. If you are interested in something like a role as a
crime analyst, part of your interview process might include something
like a numeracy test, which will test your ability to interpret trends
and data from graphs (amongst some other things).

 Here's an example:

\hypertarget{question-1}{%
\subsubsection*{Question 1}\label{question-1}}
\addcontentsline{toc}{subsubsection}{Question 1}

\includegraphics{imgs/numtest_1.png}

Then the test question could be something like this:

Tick all the true statements:

\begin{itemize}
\tightlist
\item
  68\% of observed lessons were graded `Good' or `Outstanding' in 2011.
\item
  In 2011 the percentage of lessons that were graded `Good' or
  `Outstanding' was twice the percentage of lessons that were graded
  `Good' or `Outstanding' in 2007.
\item
  The percentage of lessons graded `Inadequate' was halved between 2007
  and 2011.
\end{itemize}

So, which one of these do you think are true statements?

OK try again:

\hypertarget{question-2}{%
\subsubsection*{Question 2}\label{question-2}}
\addcontentsline{toc}{subsubsection}{Question 2}

\includegraphics{imgs/numtest_2.png}

Tick all the true statements:

\begin{itemize}
\tightlist
\item
  The range of marks in Test A was greater than in Test B.
\item
  The median mark in Test B was approximately 10 percentage points
  higher than the median mark in Test A.
\item
  In Test B one-quarter of the pupils achieved 75\% or more.
\end{itemize}

And again:

\hypertarget{question-3}{%
\subsubsection*{Question 3}\label{question-3}}
\addcontentsline{toc}{subsubsection}{Question 3}

\includegraphics{imgs/numtest_3.png}

Tick all the true statements:

\begin{itemize}
\tightlist
\item
  Two-thirds of the pupils spent 15 minutes or less on planning.
\item
  The range of time used for planning was 22 minutes.
\item
  The pupil with the median planning time achieved a final mark of 54.
\end{itemize}

And finally:

\hypertarget{question-4}{%
\subsubsection*{Question 4}\label{question-4}}
\addcontentsline{toc}{subsubsection}{Question 4}

\includegraphics{imgs/numtest_4.png}

Tick all the true statements:

\begin{itemize}
\tightlist
\item
  All the pupils completed the test within the maximum time allowed.
\item
  The median time taken was 40 minutes.
\item
  No pupils recorded a time less than 29 minutes.
\end{itemize}

Alright. Make a note of all of your answers before reading the results
ahead.

\hypertarget{results}{%
\subsubsection*{Results}\label{results}}
\addcontentsline{toc}{subsubsection}{Results}

OK I have the answers for you now:

\begin{itemize}
\tightlist
\item
  For question 1 the correct answers were Options A and C
\item
  For question 2 the correct answers were Option B
\item
  For question 3 the correct answers were Options A, B and C
\item
  For question 4 the correct answers were Options A and B
\end{itemize}

So, how did you do? If you are unsure about any of the answers, ask us
now!

\hypertarget{summary-5}{%
\section{Summary}\label{summary-5}}

In sum, you should now be able to select a graph that represents your
data in a meaningful way, that helps you describe your data as well as
communicate your results to your audiences. You should be familiar with
the following terms:

\begin{itemize}
\tightlist
\item
  bar chart
\item
  column chart
\item
  line chart
\item
  data ink ratio
\item
  stacked bar chart
\item
  conditional formatting
\item
  producing summary statistics by groups
\item
  if statements in excel
\item
  box plots in excel
\item
  association/ direction of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    positive relationship
  \item
    negative relationship
  \end{itemize}
\item
  form of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    linear relationship
  \item
    non-linear relationship
  \end{itemize}
\item
  strength of a relationship between numeric variables

  \begin{itemize}
  \tightlist
  \item
    weak
  \item
    moderate
  \item
    strong
  \end{itemize}
\item
  scatterplot
\item
  trendline
\item
  correlation
\item
  correlation does not mean causation
\end{itemize}

\hypertarget{week7}{%
\chapter{Week 7}\label{week7}}

\hypertarget{learning-outcomes-6}{%
\section{Learning outcomes}\label{learning-outcomes-6}}

This week we consider another important factor that is present in our
data that we don't always talk about, and that is the importance of
\emph{time}. The importance of place in criminology and crime analysis
is widely discussed. We know certain areas can be crime hotspots, and we
know that whether you come from a well of or depreived area you have
different access to resources, and therefore your outcomes in terms of
involvement with criminal justica system also differs. However time is
just as important as place. We often hear that crime is ``going up'' or
``going down'' over time. It is very important, that as well-rounded
criminologists, you are able to talk about these concepts with
appropriate knowledge and understanding.

When violence increases bewteen March and August, is that because we are
seeing an increase in crime and offending? Or is it possible that the
time of year has something to do with this? How much must crime increase
and over how long of a time, in order to be able to confidently say that
crime is on the increase? These are important, and not always easy
questions to answer, and this week we will begin to think about this.

Here are some terms that we will cover today:

\begin{itemize}
\tightlist
\item
  Crime trends
\item
  Temporal crime analysis
\item
  Seasonality
\item
  Time series data analysis

  \begin{itemize}
  \tightlist
  \item
    Moving averages
  \item
    Smoothing techniques
  \item
    Seasonal decomposition
  \end{itemize}
\item
  Signal vs noise
\end{itemize}

\hypertarget{crime-and-incident-trend-identification}{%
\section{Crime and incident trend
identification}\label{crime-and-incident-trend-identification}}

All crimes occur at a specific date and time, however such definite
temporal information is only available when victims or witnesses are
present, alarms are triggered, etc., at the time of occurrence. This
specific temporal data is most often collected in crimes against
persons. In these cases, cross-tabulations or histogram6 of weekday and
hour by count will suffice. The great majority of reported events are
crimes against property. In these cases, there are seldom victims or
witnesses present. These events present the analyst with `ranged'
temporal data, that is, an event reported as occurring over a range of
hours or even days. In the case of ranged temporal data, analysis is
possible through use of equal chance or probability methods. If an event
was reported as having occurred from Monday to Tuesday, in the absence
of evidence to the contrary, it is assumed the event had an equal chance
or probability of occurring on each of the two days, or .5 (\%50). In
the same manner, if an event was reported as having occurred over a 10
hour span there is a 10\% chance the event occurred during any one of
the hours. This technique requires a reasonable number of events in the
data set to be effective. The resulting probabilities are totalled in
each category and graphed or cross-tabulated. This produces a comparison
of relative frequency, by weekday or hour
\href{http://cradpdf.drdc-rddc.gc.ca/PDFS/unc76/p530054.pdf}{source}.

\textbf{Temporal crime analysis} looks at trends in crime or incidents.
A crime or incident trend is a broad direction or pattern that specific
types or general crime and/or incidents are following.

Three types of trend can be identified:

\begin{itemize}
\tightlist
\item
  overall trend -- highlights if the problem is getting worse, better or
  staying the same over a period of time
\item
  seasonal, monthly, weekly or daily cycles of offences -- identified by
  comparing previous time periods with the same period being analysed
\item
  random fluctuations -- caused by a large number of minor influences,
  or a one-off event, and can include displacement of crime from
  neighbouring areas due to partnership activity or crime initiatives.
\end{itemize}

\hypertarget{activity-1-extracting-temporal-variables-from-dates}{%
\subsection{Activity 1: Extracting temporal variables from
dates}\label{activity-1-extracting-temporal-variables-from-dates}}

This week we will be looking at crime data from the USA. As you saw, the
data from police.uk is aggregated by months. We do not know when the
offences happened, only the month, but nothing more granular than that.
American police data on the other hand is much more granular.

 Cities release their own data. Here, we will be looking at crimes from
Dallas, Texas.
\href{https://www.dallasopendata.com/Public-Safety/All-Crime/p9zb-d4n6/about}{You
can see more about these data, and find the download link to the data
dictionary here}. You can download a subset of Dallas crime data from
blackboard. Go to course content \textgreater{} week 7 \textgreater{}
Data for week 7, and the file is \emph{dallas\_burg.xlsx}. Download this
and open it up in excel.

When you open the excel spreadsheet, you will see that there is a column
for date called \textbf{Date.of.Occurrence}. The date is in the format
dd/mm/yyyy. So the first date on there you can see is 16/11/2016.

But what if I asked you the question: which year had the most
residential burglaries? Or what if I want to know if residential
burglaries happen more in the weekday, when people are at work, or in
the weekends, maybe when people are away for a holiday? You have the
date, so you should be able to answer these questions, right?

Well you need to be able to have the right variables to answer these
questions. To know what year saw the most residential burglaries, you
need to have a variable for year. To know what day of the week has the
most burglaries, you need to have a variable for day of the week. So how
can we extract these variables from your date column? Well luckily excel
can help us do this.

So first you want to make sure that your date column is, in fact, a
date. To be sure, you can right click on the column, and select ``Format
cells\ldots{}'':

\includegraphics{imgs/format_date_1.png}

Under the ``Category'' sidebar select ``Date'', and pick a format that
matches your date layout (for example, in this case it's
date-month-year).

\includegraphics{imgs/format_date_2.png}

This way you make sure that Excel knows that your date column is a date.

\hypertarget{activity-2-extract-year-from-date-column}{%
\subsection{Activity 2: Extract year from date
column}\label{activity-2-extract-year-from-date-column}}

 Now, let's start by answering our first question: which year had the
most burglaries. To answer this, we first need a variable for year.
Let's extract this from the date column. We can do this with the
\texttt{=year()} function. Inside the brackets, you just have to put the
date from which to extract the year.

First, let's create a new column, called ``Year'', like so:

\includegraphics{imgs/create_year_col.png}

Then for the first cell, enter the formula \texttt{=year()}, and inside
the brackets, put the date value for the first row, in this case, cell
\texttt{C2}:

\includegraphics{imgs/extract_year_formula.png}

You will see the cell be populated with the value for year, in this case
2016. Copy the formatting down to populate the whole ``Year'' column:

\includegraphics{imgs/year_col_pop.png}

Ta-daa! You now have a column of years! Now how do you find out which
year has the most number of burglaries? Well it's simple univariate
analysis that we should be so familiar with by now! Yay!

So go ahead, build a pivot table to count the frequency of each variable
for year. Try to do this now with no further guidance. If you do need
some help, have a look at your notes from the univariate analysis lab,
in week 2. But try to have a go.

So which year had the most residential burglaries?

\includegraphics{https://media.giphy.com/media/8nZnnnvX5A4so/giphy.gif}

TO answer this question, hopefully you built a pivot table with the year
variable, and your table look should like this:

\includegraphics{imgs/count_of_yr.png}

If not raise your hand now, and we will come around and help!

If it does, nice work! You can now identify that the year with the
highest number of residential burglaries in Dallas was 2015. You should
however note that since 2017 is not yet over, you have incomplete data
for this year, and so you're not comparing like for like. Always think
about how to interpret your findings, and keep in mind any possible
limitations and issues associated with your data.

\hypertarget{activity-3-extract-day-of-week-from-date-column}{%
\subsection{Activity 3: Extract day of week from date
column}\label{activity-3-extract-day-of-week-from-date-column}}

Now let's go back to our 2nd question - do residential burglaries happen
more in the weekday, when people are at work, or in the weekends, maybe
when people are away for a holiday?

 To answer this question, we need a variable for day of the week. First,
create a new column call it Day.of.Week:

\includegraphics{imgs/create_day_of_wk_col.png}

Then populate this column with the day of the week. To do this, you can
use the \texttt{=text()} function. You have to pass this function two
parameters. The first is the value of the date column again, as was the
case with the year function, and the second is the format that you want
the text to take.

If you ever forget this, excel will remind you:

\includegraphics{imgs/text_params.png}

The value parameter, just like with the year function, is the cell in
the \emph{Date.of.Occurrence} column. In the case of our first row here,
it's C2. The second value is the format parameter. Depending on what you
pass here, a different value will be returned by the \texttt{text()}
function. Here is a list of values, and their results:

\begin{itemize}
\tightlist
\item
  \emph{d:} 9
\item
  \emph{dd:} 09
\item
  \emph{ddd:} Mon
\item
  \emph{dddd:} Monday
\item
  \emph{m:} 1
\item
  \emph{mm:} 01
\item
  \emph{mmm:} Jan
\item
  \emph{mmmm:} January
\item
  \emph{mmmmm}: J
\item
  \emph{yy:} 12
\item
  \emph{yyyy:} 2012
\item
  \emph{mm/dd/yyyy:} 01/09/2012
\item
  \emph{m/d/y:} 1/9/12
\item
  \emph{ddd, mmm d:} Mon, Jan 9
\item
  \emph{mm/dd/yyyy h:mm AM/PM:} 01/09/2012 5:15 PM
\item
  \emph{dd/mm/yyyy hh:mm:ss:} 09/01/2012 17:15:00
\end{itemize}

Let's use the ``dddd'' option here to extract the full name of each
weekday. So to do this, your formula should be:

\texttt{=year(C2,\ "dddd")}

Like so:

\includegraphics{imgs/c2_dddd.png}

Now copy this for each row, and you will now find out the day of the
week that each one of these burglaries falls on:

\includegraphics{imgs/dow_col.png}

Can you make a pivot table that answers our question about weekends and
weekdays yet? Well, not quite just yet. You would need a column for
weekday/weekend. How can we do this?

Well think back to when we did re-coding in week 4. Remember the
\texttt{VLOOKUP()} function? Remember the \texttt{VLOOKUP()} function
takes 4 parameters. You have to tell it the \textbf{lookup\_value} - the
value to search for. You then have to tell it the \textbf{table\_array}
- which is your lookup table, with two columns of data. The VLOOKUP
function \emph{always} searches for the lookup value in the first column
of table\_array. Your table\_array may contain various values such as
text, dates, numbers, or logical values.You then have to tell it the
\textbf{col\_index\_num} - the column number in table\_array from which
the value in the corresponding row should be returned. Finally, you
still have to specify the \textbf{range\_lookup}. This determines
whether you are looking for an exact match (when you set to FALSE) or
approximate match (when you set to TRUE or you omit it).

So the first thing you have to create is your lookup table. For each
value of day of the week, you should assign a value of Weekday or
Weekend. Something like this:

\includegraphics{imgs/wkday_lookup.png}

Now let's try to apply the formula.

\begin{itemize}
\tightlist
\item
  \textbf{lookup\_value} is the day of the week
\item
  \textbf{table\_array} is this table we just created
\item
  \textbf{col\_index\_num} is the column which contains the values to
  translate into
\item
  \textbf{range\_lookup} set this to FALSE, so we match on exact matched
  only
\end{itemize}

So

\includegraphics{imgs/vlookup_formula_wkday.png}

Make sure to add the dollar signs, to ensure that you can copy the
formatting!

Now, finally you have a variable that tells you whether each burglary
took place on a weekday or a weekend:

\includegraphics{imgs/wkday_var_created.png}

Now you can use this variable to create a pivot table, and see the
number of burglaries on weekdays or weekends. Let's create this pivot
table:

\includegraphics{imgs/wkday_pivot.png}

Unsurprisingly, there are a lot more burglaries on weekdays than
weekends. Why do you think that is? Take a moment to chat to the person
next to you and discuss.

\includegraphics{https://media.giphy.com/media/xT5LMPQWYPMrZOYjug/giphy.gif}

So what did you discuss? I am hoping that you mentioned that there were
a lot more weekdays than weekend-days in our data, and in fact, in all
weeks. There are 2.5 times as many weekdays than weekends in a week. I
know, it's a sad truth, we work a lot more than we get to rest. But
another thing that happens because of this, is that simply looking at
the number of burglaries in weekdays and in weekdays might not be a very
meaningful measure. Remember earlier, when we spoke about comparing like
for like? Or last week, when we talked about the crime rate (per 100,000
population) versus the number of crimes? Well again here, we should
calculate a rate; to truly be able to compare, we should look at a rate
such as the number of burglaries \emph{per day} for weekdays, and the
number of burglaries \emph{per day} for weekend-days.

How do you calculate the rate? Well you do this simply by dividing the
numerator (number of burglaries) by an appopriate denominator. What is
the best denominator? Well it depends on the question you're looking to
answer. Usually it's what comes after the \emph{per}. If we are looking
for number of crimes \emph{per population} then we will be dividing by
the population. If we are looking at number of burglaries \emph{per
household} we will be dividing by the number of households in an area.
In this case, we were talking about the number of burglaries \emph{per
day} to compare between weekends and weekdays. So, your denominator will
be the number of days for each group.

So, to get the burglary rate (per day), we simply take our total number
of burglaries from our pivot table, and divide by the number of days for
each. As we know, there are 5 weekdays (boo) and 2 weekends (yaay). So
let's divide accordingly:

\includegraphics{imgs/burg_1.png}

And copy also for the weekends, and voila we have our answer to the
question, are there more burglaries on weekdays or weekends:

\includegraphics{imgs/burg_2.png}

Now you can discuss again why you think this might be. For example,
during the week, people are away from their homes for work, for the
majority of each day, which leaves their home unprotected. I've
mentioned the \href{}{crime triangle} before. If the resident is not
home, then there is an absence of a capable guardian, and there is an
increased risk for a crime (such as burglary) to occur!

There are many things that peak on certain days of the week. If you're
interested in some more examples,
\href{https://www.theguardian.com/lifeandstyle/2013/may/29/most-dangerous-day-of-week}{read
this article in the Guardian about the most dangerous days of the week}.

\hypertarget{aggregating-to-simple-intervals}{%
\section{Aggregating to simple
intervals}\label{aggregating-to-simple-intervals}}

Above the activities have you the ability to extract certain types of
date categories from the date column. If we want to compare year on year
increase or decrease, this is one approach. Or if we want to compare day
of week, month of year, and so on. We did this by creating a new
variable, and then using a pivot table. We could also look at the date
as it is, we could have a look at the number of crimes each day, but
often this can be noisy. Instead, sometimes we want to aggregate (group)
these into simpler time intervals.

First, we've not had a look at our time variable yet, so we could start
with that.

\hypertarget{activity-4-aggregate-to-hour}{%
\subsection{Activity 4: Aggregate to
hour}\label{activity-4-aggregate-to-hour}}

 First create a new column for `Hour':

\includegraphics{imgs/hour_1.png}

Then, use the \texttt{=HOUR()} function to extract the hour, just like
we did to get the year using the \texttt{=YEAR()} function. Except, this
time we are extracting hour from the ``Time.of.Occurrence'' variable,
rather than the ``Date.of.Occurrence'' variable. Like so:

\includegraphics{imgs/hour_2.png}

And finally, copy your formatting to all the rows:

\includegraphics{imgs/hour_3.png}

Now you have a column for the hour that the offence was committed in!

You could also extract something else, for example the month and year
from the date. To do this, you simply use this, you again create a new
column, and this time use a formula that extracts text from the date:

\texttt{=TEXT(*cell\ reference*,"mmm-yyyy")}

In where it says cell reference, just put in the reference to the column
from the date (in this case it's C) and the cell number (2 for the first
row).

\texttt{=TEXT(C2,"mmm-yyyy")}

You can then copy this formula down the whole data set.

NOTE: Now there is also a new feature in Excel 2016 that allows you to
do automatic grouping by time. You can have a look through
\href{https://blogs.office.com/en-us/2015/10/13/time-grouping-enhancements-in-excel-2016/?eu=true}{this
tutorial} to show you how you can use this new feature.

So what can we do with all this great temporal data? Well let's start by
visualising!

\hypertarget{visualising-time}{%
\section{Visualising time}\label{visualising-time}}

Depending on what you're visualising, you can use either a linear
approach, or a cyclical approach. A linear approach makes sense if
you're plotting something against passing time. If you are looking at
whether a trend is increasing or decreasing, this is something you would
look at over time, because it would be moving forward.

On the other hand, many measures of time are cyclical. For example,
remember in the very first week, when we spoke about \emph{levels of
measurement} of variables, and we had a variable for time, and mentioned
that this was \emph{not} a numeric variable, instead it was
categorical-ordinal, because it loops back around. After the 23rd hour
of a day comes the 0 hour of the next! So representing this on a linear
graph may mask some important variation. Let me show you an example:

\includegraphics{imgs/time_line_ex.png}

The above is some made-up data, of something, over the hours of a day.
What do the peaks there look like to you? If I were looking at this, I
would say we have 3 peaks. There is a peak in the early hours of the
morning, like 1-2am, then again a peak midday, and again another peak in
the evening around 9pm.

\includegraphics{imgs/time_radar_ex.png}

This is the same data, but visualised in a cyclical way. In this
visualisation, it actually appears that there are two main peaks. The
peaks that were previously identified as two separate, possibly
independent peaks have now bridged into 1, and we can see that instead
there might be a continuum, and an ongoing event from 9pm to 2am. It is
important to consider the cyclical nature of some of your temporal
variables, and use the appropriate means to visualise them.

It might be worth to have a look at some examples of temporal data being
visualised, as well as making our own visualisations of time. Here I
will focus on 3 approaches, \textbf{line graphs}, \textbf{radar graphs},
and \textbf{heatmaps}.

\hypertarget{activity-5-line-graphs}{%
\subsection{Activity 5: Line graphs}\label{activity-5-line-graphs}}

Let's start with viewing some examples of time visualised using a
timeline, of continuous time passing:

The world of sports is rich with data, but that data isn't always
presented effectively (or accurately, for that matter). The folks over
at FiveThirtyEight do it particularly well, though. In this interactive
visualization below, they calculated what's called an ``Elo rating'' --
a simple measure of strength based on game-by-game results -- for every
game in the history of the National Football League. That's over 30,000
ratings in total. Viewers can compare each team's Elo to see how each
team performed across decades of play.
\href{http://projects.fivethirtyeight.com/complete-history-of-the-nfl/\#ari}{See
here}

\includegraphics{https://blog.hubspot.com/hs-fs/hubfs/history-of-football-teams.png?t=1510614058976\&width=669\&height=475\&name=history-of-football-teams.png}

Another example of a continuous timeline is the ``what is warming the
world'' visualisation. Ever heard a version of the advice, ``Don't
simply show the data; tell a story with it''? That's exactly what this
visualization from Bloomberg Business does -- and it's the interactive
part that makes the story move along from beginning to end.

The point of the visualization is to disprove theories that claim that
natural causes explain global warming. The first thing you'll see is the
observed temperature as it's risen from 1880 to present day. As you
scroll down, the visualization takes you through exactly how much
different factors contribute to global warming in comparison to what's
been observed, adding a richer layer of storytelling. The conclusion the
authors want viewers to draw is made very clear.
\href{http://www.bloomberg.com/graphics/2015-whats-warming-the-world/}{See
here}

\includegraphics{https://blog.hubspot.com/hs-fs/hubfs/bloomberg-climate-change.png?t=1510614058976\&width=669\&height=277\&name=bloomberg-climate-change.png}

 To make a line graph, we need to think about what we will represent on
our horizontal (x) axis, and what we will represent on our vertical (y)
axis. Let's say that on the Y axis we want to represent \emph{number of
burglaries}. OK, so what should we have on our X axis? That will be
whatever we use to fill in the blank in the sentence: number of
burglaries per \_\_\_\_\_\_\_\_\_\_\_\_\_. Let's say we want to know any
changes in the number of burglaries \emph{per day}. Well to do this we
will need to \emph{count the number of times that each date appears in
the data}. Remember what this means?

That's right! Univariate analysis time. Go ahead and use a pivot table
to build a frequency table of the ``Date.of.Occurrence'' variable. You
will eventually end up with something like this:

\includegraphics{imgs/burg_day_pivot.png}

Now to turn this into a line graph. We've made quite a few line graphs
before, including last week as well, but never with this many data
points. However, the motions are still the same. You highlight your
data, you select your appropriate chart (line graph), and then you also
add the labels, using ``Select Data'' and populating the `Category (X)
axis labels' section in the popup window that appears. If any of this is
confusing to you, re-visit last week's lab notes about data
visualisation.

When all said and done, you should end up with a chart that looks like
this:

\includegraphics{imgs/line_burgs.png}

So is residential burglary going up or down in the city of Dallas? It's
not easy to tell from this, is it? The thing with data at these
intervals is that there is a lot of day-to-day variation, and what we
would refer to as the \textbf{noise}. And in this \textbf{noise} we can
loose sight of the \textbf{signal}.

Signal-to-noise ratio (abbreviated SNR or S/N) is a measure used in
science and engineering that compares the level of a desired signal to
the level of background noise. While SNR is commonly quoted for
electrical signals, it can be applied to any form of signal, and is
sometimes used metaphorically to refer to the ratio of useful
information to false or irrelevant data.

One of my favourite books I've read in recent years is called
\href{https://www.theguardian.com/books/2012/nov/09/signal-and-noise-nate-silver-review}{The
Signal and the Noise by Nate Silver} and if you enjoyed anything about
this class, or you enjoyed reading the Tiger that Isn't text, I would
highly recommend it! It can get a bit more technical in places, but it's
got loads of very neat examples, and is an entertaining read overall. In
this book, the author talks about the need to see past all the noise
(the random variation in the data) in order to be able to make
predictions (using the signal).

So in this case of our burglaries, we would need to be able to somehow
look past this variation in day-to-day changes, to be able to look at
overall trends. For example, we could start to \textbf{smooth} this
data, and instead of looking at the number of burglaries each day, begin
to consider the \emph{average number of burglaries every 10 days}, which
would be something called the \textbf{10-point moving average}. This
would be one approach to smooth our data. I will return to this later
today.

\hypertarget{activity-6-radar-graphs}{%
\subsection{Activity 6: Radar graphs}\label{activity-6-radar-graphs}}

There are also multiple cyclical ways to visualise your data. One
approach is to use radar graphs The below graphs all represent the same
data, of page views for a website across different times of day. Here's
an example by Purna Duggirala that is essentailly a bubble chart that
uses two clocks side by side:

\includegraphics{http://dougmccune.com/blog/wp-content/uploads/2011/04/two_clocks-300x163.png}

The biggest problem with the chart is the incorrect continuity. A single
clock on its own isn't a continuous range, it's really only half a
range. So the clock on the left is showing 12am -- 12pm, but when you
reach the end of the circle the data doesn't continue on like the
representation shows. Instead you need to jump over to the second clock
and continue on around. It's difficult to see the ranges right around
both 12pm and 12am, since you lose context in one direction or another
(and worse, you get the incorrect context from the bordering bubbles).

In a great show of Internet collaboration, the double clock chart
spurred some other experimentation. Jorge Camos came up with a polar
chart that plots the data on a single ``clock face,'' showing two 12
hour charts overlaid on top of each other.

\includegraphics{http://dougmccune.com/blog/wp-content/uploads/2011/04/polar_chart2-300x256.png}

Given that the 12-hour clock is a difficult metaphor to use for a
visualization, many people choose to use a 24-hour circle. 24-hour
circular charts typically start with midnight at the top of the chart
and then proceed clockwise, showing all 24 hours in one 360-degree
range. The benefit of a 24-hour circular chart is that the cyclical
nature of the data is represented and the viewer can easily read the
continuity at any point on the chart.

A simple example of a 24-hour circle comes from Stamen Design`s
Crimespotting. This isn't a data-heavy visualization chart, since it
doesn't actually show any data other than sunrise and sunset times
(instead it's main purpose is as a filtering control). But it's a good
example of the general layout of 24-hour charts, and it's very clean and
well-labeled. You can read about the thinking that went into designing
this ``time of pie'' on Stamen's blog.

\includegraphics{http://dougmccune.com/blog/wp-content/uploads/2011/04/crimespotting_time_pie.png}

The inspiration for this time selector, which is documented in Tom
Carden's blog post, was the real-life timer control used for automating
lights in your house.

\includegraphics{http://dougmccune.com/blog/wp-content/uploads/2011/04/real_life_time_pie-150x150.png}

This above is known as a radial chart. We will now learn how to build
one in Excel. But before we do, I wanted to tell you guys the story of
Florence Nightingale, the original data visualisation pioneer. I know, I
know, we covered data visualisation last week, can I please get over it.
But it's one of the most fun aspects of data analysis so no, I cannot,
and now I will tell you about why Florence Nightingale wasn't just a
famour nurse, she was a famous data pioneer!

This is Florence Nightingale's `coxcomb' diagram on mortality in the
army:

\includegraphics{http://bigbangdata.somersethouse.org.uk/wp-content/uploads/2015/12/FlorenceData900.jpg}

We all have an image of Nightingale - who died 100 years ago today - as
a nurse, lady with the lamp, medical reformer and campaigner for
soldiers' health. But she was also a datajournalist. After the disasters
of the Crimean war, Florence Nightingale returned to become a passionate
campaigner for improvements in the health of the British army. She
developed the visual presentation of information, including the pie
chart, first developed by William Playfair in 1801. Nightingale also
used statistical graphics in reports to Parliament, realising this was
the most effective way of bringing data to life.

Her report on mortality of the British Army, published in 1858, was
packed with diagrams and tables of data. The coxcomb chart demonstrates
the ratio of British soldiers who died during the Crimean War of
sickness, rather than of wounds or other causes. Her work changed the
course of history, enabling pro-active change through data collection.
In 1858 Nightingale presented her visualisations to the health
department demonstrating the conditions of the hospitals, and whose
actions resulted in deaths being cut by two thirds.

 So, how do we make these in Excel. Well let's return to our within-day
variation in residential burglaries. Let's visualise when they take
place.

To do this, we will first need to create a frequency table of the hour
variable. Remember we are graphing UNIVARIATE ANALYSIS. We are
describing one variable in our data, which is the hour variable. To know
the frequency of each value of this variable (the number of burglaries
in each hour), we must build a pivot table.

So do this, create a pivot table that tells you the frequency of each
hour:

\includegraphics{imgs/hr_freq.png}

Now highlight the ``Total'' column, as it represents the number of
burglaries in each hour, and this is what you want to graph. Then for
charts select ``Other'', and then ``Radar'':

\includegraphics{imgs/hr_radar_1.png}

A radar graph will appear, something like this:

\includegraphics{imgs/wrong_radar.png}

Now there is something wrong with this graph. Can you spot it? Take a
moment now to look\ldots{} I'll wait here.

\includegraphics{https://media.giphy.com/media/3ohhwIbKJAT3eUz38s/giphy.gif}

Did you notice? If not, look again, this time, look at the graph, and
tell me which hour has the most burglaries. Great, now look at the pivot
table. Is it the same hour? (hint: no)

Why is this? Well our axes are not actually labelled! Remember, to label
your axes, do as you did in the line graph, right click on the graph,
``Select Data\ldots{}'' and populate the `Category (X) axis labels'
section in the popup window that appears with the hours. Now your graph
should make sense:

\includegraphics{imgs/correct_hr_1.png}

Much better. Clearly residential burglaries are happening in the
morning, and in the day, and much less so in the night. But, is this any
different between weekend and weekday?

Now we are introducing a \emph{second} variable. By now you should know
that this means to answer this question, we will need \textbf{bivariate}
analysis. So, create a bivariate table, by dragging the weekday variable
which we created earlier into the ``Column Labels'' box in the pivot
table options. This will create a crosstab for you:

\includegraphics{imgs/bi_radar_1.png}

Now select the values for the ``Weekday'' column, and again for charts
choose Other \textgreater{} Radial:

\includegraphics{imgs/hr_r_20.png}

A chart will appear, showing you only the weekday burglaries. Now, right
click this chart and select ``Select Data\ldots{}'' option:

\includegraphics{imgs/hr_r_21.png}

First, label your current data. Do this by clicking in the textbox next
to the ``Name:'' label, and then clicking on the column header (cell
B4):

\includegraphics{imgs/hr_r_22.png}

Then add the correct labels, but clicking on the box next to ``Category
(X) axis labels'', and selecting the hours in the ``Row Labels'' column.

\includegraphics{imgs/hr_r_23.png}

Now, add the weekend burglaries. Do this the same way we added multiple
lines last week. You click on the ``Add'' button, and you put the name
in the textbox next to ``Name:'', and the values in the text box next to
the ``Y-values'' box. This time you can leave ``Category (X) axis
labels'' empty, as it's already populated by the previous series:

\includegraphics{imgs/hr_r_24.png}

Ta-daa here is your 2-variable radial graph.

\includegraphics{imgs/hr_r_25.png}

One issue though. Can you think of it?

No? Well look how tiny the circle for the weekend is. This is because of
the problem that we discussed earlier, there are simply many more
burglaries in the weekday. So if we want to look at how within-day
patterns differ between weekend and weekday, this graph isn't
necessarily the best approach. Instead, we might want to look at
\emph{percentage} distribution, instead of count.

To do this, change the pivot table to display percent instead of count.
A reminder how to do this: you click on the little i (or arrow on a PC)
to bring up a popup window, on which you select options:

\includegraphics{imgs/hr_r_26.png}

This brings up a menu where you can select what to ``show data as''.
Choose \% of column (because we want to know the percent of weekday
crimes in each hour, and percent of weekend crimes in each hour, so our
100\% are our columns, so we are looking at column percentages):

\includegraphics{imgs/hr_r_27.png}

For doing this on a PC, to get the percentages for weekday/weekend
burglaries, the options are slightly different. Need to go to the tab
called ``Show Values As'' and then select \% of Column:

\includegraphics{imgs/pc_radial.png}

And finally, ta-daa you have a radial graph that helps you compare the
within-day temporal patterns of residential burglary between weekdays
and weekends.

\includegraphics{imgs/hr_r_28.png}

\hypertarget{activity-7-heatmaps}{%
\section{Activity 7: Heatmaps}\label{activity-7-heatmaps}}

Heatmaps visualise data through variations in colouring. When applied to
a tabular format, Heatmaps are useful for cross-examining multivariate
data, through placing variables in the rows and columns and colouring
the cells within the table. Heatmaps are good for showing variance
across multiple variables, revealing any patterns, displaying whether
any variables are similar to each other, and for detecting if any
correlations exist in-between them.

Typically, all the rows are one category (labels displayed on the left
or right side) and all the columns are another category (labels
displayed on the top or bottom). The individual rows and columns are
divided into the subcategories, which all match up with each other in a
matrix. The cells contained within the table either contain colour-coded
categorical data or numerical data, that is based on a colour scale. The
data contained within a cell is based on the relationship between the
two variables in the connecting row and column.

A legend is required alongside a Heatmap in order for it to be
successfully read. Categorical data is colour-coded, while numerical
data requires a colour scale that blends from one colour to another, in
order to represent the difference in high and low values. A selection of
solid colours can be used to represent multiple value ranges (0-10,
11-20, 21-30, etc) or you can use a gradient scale for a single range
(for example 0 - 100) by blending two or more colours together.

Because of their reliance on colour to communicate values, Heatmaps are
a chart better suited to displaying a more generalised view of numerical
data, as it's harder to accurately tell the differences between colour
shades and to extract specific data points from (unless of course, you
include the raw data in the cells).

Heatmaps can also be used to show the changes in data over time if one
of the rows or columns are set to time intervals. An example of this
would be to use a Heatmap to compare the temperature changes across the
year in multiple cities, to see where's the hottest or coldest places.
So the rows could list the cities to compare, the columns contain each
month and the cells would contain the temperature values.

-\href{https://datavizcatalogue.com/methods/heatmap.html}{data viz
catalogue}

So let's make a heatmap for burglary, from a crosstab looking at the
frequency of burglary across days of the week \emph{and} hours of the
day.

You can know, by the way that question is phrased, that your task here
will require a bivariate analysis. To produce this, we will need a pivot
table. So let's create our crosstab with pivot table, where we have day
of week in the columns, and hour of day in the rows:

\includegraphics{imgs/hm_pivot.png}

Now this tells you all the information you want to know, but there are a
lot of cells, with a lot of numbers, and it's not immediately obvious
where we should be looking. A heatmap (via conditional formatting)
creates an option for us to nudge people to find the high and low values
in this table.

To implement this, highlight the values in your table:

\includegraphics{imgs/cond_1.png}

Then find the conditional formatting tab, and select a colour scheme
that works for you. Remember to consider whether red values should be
high or low? Is a high number good? Or is it bad?

\includegraphics{imgs/cond_2.png}

In this case, red is bad because it means more burglaries! So select
this colourscheme and as if by magic, our table will now be highlighted
in the form of a heatmap:

\includegraphics{imgs/cond_3.png}

Now your eyes are immediately drawn to where the high values are, which
apparently is Monday to Friday, 7 and 8 AM. We can see that on Saturday
and Sunday even burglars want a lie-in, and therefore you see the green
low burglary rate creep up later than on weekdays. Exciting stuff! Right
now let's calculate the moving average in excel, using some example data
on temperature measurement. You can download this data from the week 7
folder, in the data for week 7 subfolder. It should be called
mov\_avg\_temp\_example.xlsx. Then you can use it to follow along below
this tutorial taken from
\href{http://www.dummies.com/software/microsoft-office/excel/how-to-calculate-moving-averages-in-excel/}{dummies.com}

The Data Analysis command provides a tool for calculating moving and
exponentially smoothed averages in Excel. Suppose, for sake of
illustration, that you've collected daily temperature information. You
want to calculate the three-day moving average --- the average of the
last three days --- as part of some simple weather forecasting. To
calculate moving averages for this data set, take the following steps.

\includegraphics{http://d2r5da613aq50s.cloudfront.net/wp-content/uploads/430314.image0.jpg}

To calculate a moving average, first click the Data tab's Data Analysis
command button.

When Excel displays the Data Analysis dialog box, select the Moving
Average item from the list and then click OK.

\includegraphics{http://d2r5da613aq50s.cloudfront.net/wp-content/uploads/430315.image1.jpg}

Identify the data that you want to use to calculate the moving average.
Click in the Input Range text box of the Moving Average dialog box. Then
identify the input range, either by typing a worksheet range address or
by using the mouse to select the worksheet range. Your range reference
should use absolute cell addresses. An absolute cell address precedes
the column letter and row number with \texttt{\$} signs, as in
\texttt{\$A\$1:\$A\$10}. If the first cell in your input range includes
a text label to identify or describe your data, select the Labels in
First Row check box.

In the Interval text box, tell Excel how many values to include in the
moving average calculation. You can calculate a moving average using any
number of values. By default, Excel uses the most recent three values to
calculate the moving average. To specify that some other number of
values be used to calculate the moving average, enter that value into
the Interval text box.

Tell Excel where to place the moving average data. Use the Output Range
text box to identify the worksheet range into which you want to place
the moving average data. In the worksheet example, the moving average
data has been placed into the worksheet range B2:B10.

(Optional) Specify whether you want a chart. If you want a chart that
plots the moving average information, select the Chart Output check box.

(Optional) Indicate whether you want standard error information
calculated. If you want to calculate standard errors for the data,
select the Standard Errors check box. Excel places standard error values
next to the moving average values. (The standard error information goes
into C2:C10.)

After you finish specifying what moving average information you want
calculated and where you want it placed, click OK.

\includegraphics{http://d2r5da613aq50s.cloudfront.net/wp-content/uploads/430316.image2.jpg}

\textbf{Note:} If Excel doesn't have enough information to calculate a
moving average for a standard error, it places the error message into
the cell. You can see several cells that show this error message as a
value.

If any of that was a bit unclear, you can have a go with another,
different example, by following the steps in
\href{http://www.excel-easy.com/examples/moving-average.html}{this
tutorial here}

So that's about it. If you're feeling very adventurous you can have a
look at
\href{https://www.searchlaboratory.com/2013/09/time-series-decomposition-using-excel/}{seasonal
decomposition in excel} and ome more in-depth learning about
\href{http://www.informit.com/articles/article.aspx?p=2433607}{moving
average and soothing techniques in excel} as well.

\hypertarget{summary-6}{%
\section{Summary}\label{summary-6}}

In sum, you should now be able to begin to make sense of temporal data
in criminology. You should know how to extract variables from a date
variable, how to aggregate into meaningful intervals, know how to best
visualise temporal trends, and knwo the appraoch of the moving average
in order to be able to find the signal amidst the noise. You should be
comfortable with the following terms:

\begin{itemize}
\tightlist
\item
  time-series
\item
  seasonality
\item
  cyclical variable
\item
  radar graph
\item
  heatmap
\item
  moving average
\item
  smoothing
\item
  signal \& noise
\item
  trend, seasonality, and randomness
\item
  how to use the following formulas

  \begin{itemize}
  \tightlist
  \item
    text()
  \item
    year()
  \item
    hour()
  \end{itemize}
\end{itemize}

\hypertarget{week8}{%
\chapter{Week 8}\label{week8}}

\hypertarget{learning-outcomes-7}{%
\section{Learning outcomes}\label{learning-outcomes-7}}

Hello, and welcome to the first of two qualitative data analysis lab
sessions. Today we will learn about the software package NVivo and how
you can use this to analyse and manage your qualitative data (and
literature).

In this session you'll learn:

\begin{itemize}
\tightlist
\item
  Why we use computer aided qualitative data analysis software (CAQDAS)
  to manage and interrogate qualitative data, and more importantly, how
  to do it!
\item
  Some basic but fundamental processes for importing your data into the
  software and how to organise your files.
\item
  How to run some preliminary analysis in preparation for the real,
  heavy duty thought work next week. Qualitative Data Analysis
\end{itemize}

\hypertarget{why-use-software-in-qualitative-data-analysis}{%
\section{Why use software in qualitative data
analysis?}\label{why-use-software-in-qualitative-data-analysis}}

To give you the corporate spiel:

NVivo provides a flexible range of different ways to handle the analysis
of qualitative data - good code and retrieve tools with powerful tools
for data visualisation and interrogation. A wide range of multimedia and
social media data types are acceptable to NVivo. Certain types of
information are auto-processed on import.

Essentially, the NVivo Software allows you to analyse and manage a range
of qualitative data, from textual data such as interview transcripts or
government documents, to videos, pictures and audio files. In these two
sessions we'll focus on textual data mainly, but once you feel
comfortable with NVivo you might want to explore all the available
functionality.

Note: the directions below are for the Mac version of NVivo. For those
of you using the University computers, the Windows versions of NVivo 11
should be installed and ready for you to use. The functionality between
the two versions is slightly different, but not drastic, so you should
be able to figure it out.

I know some of you are Mac users too. If you prefer to use your Mac,
you'll need to download the latest version of NVivo. You can do this
from the QSR website
\href{http://www.qsrinternational.com/nvivo/support-overview/downloads}{here}.
Download NVivo for Mac (Version 11).

You'll need the License code to complete installation:
\textbf{NVT11-LZ000-BHA2U-CI6N7-WNTJ0}

The lab sessions will be accompanied by chapter readings from this:

\includegraphics{imgs/qual_01.png}

I've uploaded relevant chapters to Blackboard so you can make do with
that for now. There is a eBook of the first edition available via the
University Library catalogue too. Or, if you're really into qualitative
data analysis, you might want to purchase the book at some point, but
that is definitely not vital.

The lab sessions will also be strongly supplemented with video tutorials
from the QSR website. QSR is the organisation that manages NVivo.

\hypertarget{activity-1}{%
\subsection{Activity 1:}\label{activity-1}}

Discover more about the NVivo tutorials
\href{https://www.qsrinternational.com/nvivo/free-nvivo-resources/tutorials}{here}.
Take a look at the website now and watch some of the starter videos. You
can choose to watch the Windows or Mac tutorials; whichever suits your
needs. If you haven't watched them already, I would recommend watching
the following before you get into the main lab work:

\begin{itemize}
\tightlist
\item
  Explore NVivo 11 Pro for Windows -
  \href{https://www.youtube.com/watch?v=S7Z8izUiQjA}{video here} -- and
  Mac -- \href{https://www.youtube.com/watch?v=ONUACL9UcWY}{video here}.
\item
  How to \href{https://www.youtube.com/watch?v=68OOsulWjGM}{`Import
  Documents'}.
\item
  How to \href{https://www.youtube.com/watch?v=6Mkgh2B25RM}{`Create
  Memos'}.
\item
  How to \href{https://www.youtube.com/watch?v=Pm2sgWuGvTI}{`Run a Word
  Frequency Query'}.
\end{itemize}

I'll be talking you through most of the above in the session notes today
(see below). If you get stuck at any point, it's worth re-watching these
videos or seeing if there is a video available for your specific query.
It's a useful website.

\hypertarget{what-kinds-of-data-can-i-input-into-nvivo-for-analysis}{%
\section{What kinds of data can I input into NVivo for
analysis?}\label{what-kinds-of-data-can-i-input-into-nvivo-for-analysis}}

In brief, and just for your information, you can analyse the following
kinds of data in NVivo:

\textbf{Text} -- NVivo 11 can manage files that are .txt, .rtf, .doc,
.docx, and .pdf. It is fine to include embedded charts, tables, graphs,
or images. However, in Word files, headers and footers, page numbers,
line numbering, comment boxes (as per MSWord) will not be visible once
imported.

\textbf{PDF format} - where the file has been converted into PDF from
Word or similar application etc., the file can be imported as it is,
though some functions- like Text search will not work quite as well as
if they were in Word/RTF/plain text. If the PDF document was created
(possibly scanned) without optical character recognition it may just be
an image style format which can still be imported but the more limited
`region selection' mode not `text' mode will be required to apply codes
or annotations and text search tools are unlikely to work.

\begin{itemize}
\tightlist
\item
  WE'LL BE DOING SOME IMPORTING OF DOCUMENTS AND PDFs TODAY\ldots{}
\end{itemize}

\textbf{Datasets} -- Import e.g.~survey data direct from Excel and other
database formats.

\textbf{Multimedia} -- NVivo 8 onwards recognizes many formats for
visual, audio, and audiovisual materials. Most common formats are
useable. For video files, the general rule of thumb is that if they will
play in Windows Media PlayerTM, they will work in NVivo.

\textbf{Social media data} (e.g.~from Facebook, LinkedIn and Twitter)
and Evernotes -- can be imported into NVivo. - WE'LL BE DOING THIS NEXT
WEEK\ldots{}

\textbf{Ncapture} -- an interface between Internet Explorer and NVivo
can be used to capture web pages, pre-code them and then import them
into a project

\hypertarget{activity-2-find-some-data}{%
\subsection{Activity 2: Find some
data\ldots{}}\label{activity-2-find-some-data}}

Before we start with NVivo, I'd like you to find some data that you'll
use in the session today -- you can use the data that I use and which
are available on Blackboard but I think it's more interesting for you
(and will help you remember the processes better!) if you find some data
that reflect your own criminological interests.

For instance, given my research expertise and the recent news coverage
of the Panama Papers, in my example I've collected the top 5 Google news
stories on Lewis Hamilton and Tax Avoidance:

\includegraphics{imgs/qual_02.png}

But not everyone gets as excited as I do about researching the tax
noncompliance of global individual and corporate elites\ldots{}(Though I
can never understand why!).

I've simply saved each news article in a folder on my desktop:

\includegraphics{imgs/qual_03.png}

I then searched for reports and documents on tax avoidance from
governmental authorities and non-governmental organisations. I simply
searched `tax avoidance pdf' and an array of official documents were
found. I've saved five of these to my documents folder.

I then searched for `tax avoidance criminology' on Google Scholar -- you
could also try the University's online databases (probably a more
systematic approach in actual fact).

In order to keep it nice and tidy, I've organised my documents into
three folders: `Media Articles', `Literature' and `Journal Articles'

Nice and tidy.

\includegraphics{imgs/qual_04.png}

They are now ready for `importing' into NVivo later.

So, go and find your own media articles (x 5), academic journal articles
(x 5) and non-academic official sources (x 5). Save these to your
p:Drive or in an accessible location.

\textbf{NOTE: this is just an example data set, in a real research
project, your literature and data search would need to be much more
extensive and systematic, but for the purposes of demonstrating NVivo,
it will suffice!.}

Ok, so now you've identified 15 textual data sources and saved them
somewhere easily accessible (e.g.~in a folder entitled `NVivo Sessions')

So forget about these for now. We'll be back.

\hypertarget{creating-your-nvivo-project}{%
\section{Creating your NVivo
project}\label{creating-your-nvivo-project}}

\hypertarget{activity-3-getting-started-with-nvivo}{%
\subsection{Activity 3: Getting started with
NVivo}\label{activity-3-getting-started-with-nvivo}}

Let's get started then. First, you'll need to open NVivo from your
computer. Search for it on your computer. This will open up a folder
something like the following:

\includegraphics{imgs/qual_05.png}

Second, you need to `create new project'. So click on the appropriate
option. You'll then need to name and save the project, put it in the
same folder as your data:

\includegraphics{imgs/qual_06.png}

Click `Create' and hey presto:

\includegraphics{imgs/qual_07.png}

Everything you do in NVivo saves automatically. However, it's worthwhile
getting into the habit of saving after you do something important, just
in case something goes wrong! You can `Save' via the file tab.

So, what can you see in the interface?

\hypertarget{understanding-the-interface}{%
\section{Understanding the
interface}\label{understanding-the-interface}}

At the top of the screen are the `ribbon tabs':

\includegraphics{imgs/qual_08.png}

Ribbon tabs provide access to varying functions.

Basic ribbon tabs consist of:

\begin{itemize}
\tightlist
\item
  \emph{File} - Saving, Managing etc,
\item
  \emph{Home} - editing functions etc.,
\item
  \emph{Create} - making new things
\item
  \emph{Data} -- various import options,
\item
  \emph{Analyse} - Coding, linking, annotating,
\item
  \emph{Query} - range of functions to vary queries and query views and
  output,
\item
  \emph{Explore} - Charting, queries, models,
\item
  \emph{Layout} -- manipulating tabular outputs,
\item
  \emph{View} -- altering what you can see, arrangement of windows,
  coding stripes etc.
\end{itemize}

\textbf{NOTE: There are a few aspects of work that are only accessible
via the ribbon tabs (for instance some of the editing tools are only
accessible from the Home Tab, varying Code stripe views is only
available from the View ribbon tabs, Charts and other visualisations are
only accessible via the Analyse ribbon tab). But there are many other
functions which are accessible more easily from the right button in the
List panes. Some ribbon tabs only open up when you are in a particular
function, for instance when you are in a Model, a specialized Model
ribbon tab appears, but you create a new Model from the Explore Ribbon
tab. You'll figure this out through trial and error.}

So, below is the main window of the interface:

\includegraphics{imgs/qual_09.png}

You organise all your research materials in the `Navigation View'. The
navigation pane (left hand portion of the screen) is the main way of
moving around the main functional areas of the software, Sources, Nodes
etc. -- and getting into the right folder you need in order to see the
relevant `list' so that you can open individual items. When you select a
function -- a set of folders appears in the top half of the pane. Select
a folder and the relevant List appears on the right. Double click on an
item in the list and the relevant item opens up in the Detail pane
below. Successive opened items are tabbed I along the central bar
separating the List pane from the Detail pane.

\hypertarget{navigation-view}{%
\section{Navigation View}\label{navigation-view}}

Today you'll learn about three of the sections in the Navigation View:
Sources, Nodes and Memos

\hypertarget{sources}{%
\subsection{Sources}\label{sources}}

Sources can be any type of data file or memo, embedded or external to
the project.

Sources can be text, multimedia and datasets. There are three main
folders associated with sources.

\hypertarget{the-internals-section}{%
\subsection{The `internals' section:}\label{the-internals-section}}

Internals folder is designed to hold all ready-made data (to be
`imported') that you wish to work with directly within NVivo.

\hypertarget{memos}{%
\subsection{Memos}\label{memos}}

Memo folder allows you to create any number of new documents as
locations to write notes and keep track of your analysis. If the new
documents are created within or moved into the memo folder, the software
sees them as memos and each memo can be linked to one document or node.

Material inside any of the above sources can be classified, coded,
linked, and annotated according to needs of the researcher. We'll do
this mostly next week\ldots{}

\hypertarget{activity-4-folders}{%
\subsection{Activity 4: Folders}\label{activity-4-folders}}

Have a go at creating folders to house your data in the Internals
section. In a project consisting of different types of data we'd usually
recommend that folders could be based on the type of data. This choice
will vary depending on the complexity and variety of types of data. If
you only have Interview data for instance, you might just create a
folder called `Interviews'. You can decide to organise your folders in a
way that suits your own thinking style.

\textbf{TIP:} Keep the folder structure simple - although you can scope
and filter later queries by folder - folders cannot reflect all the
different features of your data. The assigning of attributes (via node
classifications) will eventually reflect things like socio demographic
information about your data/respondents/cases etc. You'll need to think
about this more if you decide to use NVivo at some point in your
research, but don't worry about these complexities just now!

So, right click at the top level (e.g.~Internals) \textgreater{} New
Folder -- provide a name for your folder:

\includegraphics{imgs/qual_10.png}

Then select `New Folder'

\includegraphics{imgs/qual_11.png}

This will open a dialogue box where you can input your folder name. In
this one, type `Media Articles', or something along these lines.

\includegraphics{imgs/qual_12.png}

Then click `Done'. You can then create a second folder entitled
`Literature' and a third folder entitled `Journal Articles'. We're now
beginning to organise where our documents will go.

\hypertarget{importing-data}{%
\section{Importing Data}\label{importing-data}}

Now we have folders we can import our data into these.

There are different things enabled during the import of data. For now,
the straightforward importing of data, whether it is Text, PDF or the
full range of audio-visual data -- can follow essentially the same
process. You just have to be careful to tell the software what type of
data you are looking for e.g. `document', `PDF' etc.

Remember that `data' and sources in NVivo are any material at all that
will help you to integrate all the information that feeds into your
project.

\hypertarget{activity-5-import-data}{%
\subsection{Activity 5: Import data}\label{activity-5-import-data}}

For now, let's just import your data and not worry about how you will
organise data (with attributes or socio demographic variables). First,
make sure the Internals folder where you want to import your documents
in the Navigation View is highlighted. For instance, we'll start by
importing our media articles, so single click the `Media Articles'
folder to highlight it.

\includegraphics{imgs/qual_13.png}

Second, right click in the `List View' to access the menu. Your `List
View' always corresponds to which `Source' is in use in your Navigation
View. Now select `Import' and then `Documents'.

\includegraphics{imgs/qual_14.png}

This will bring up your computer's file search folder. Locate the folder
where you saved your media articles and select the first one to import.
Double click or select it and click Import.

Once imported, you'll then need to name this document. I've called my
first media article `Sky News LH'. Then click `Done'.

\includegraphics{imgs/qual_15.png}

Your first article will now be imported. You'll see it has appeared in
your List View and the contents of the article can be seen in the Detail
View.

You now need to repeat this process for all your media articles, your
journal articles and your other literature. You'll then have 15
documents imported into NVivo.

\textbf{NOTE: The key point to note here is that we can integrate
different sources of data (e.g.~media articles, academic literature,
official reports) into one place to manage them together. You can even
use NVivo to undertake your literature reviews in preparation for essay
writing -- we'll look at this next week.}

When you import documents they can sometimes look a bit messy as below:

\includegraphics{imgs/qual_16.png}

So you might want to tidy these up. To do this, simply open the document
by double-clicking on it in the List View. Then ensure the `edit' option
is selected on the right hand side. You can then highlight text you wish
to delete to tidy up your documents. NOTE: Edit mode/Read only mode:
note that it is always possible to make changes to textual documents
once you've imported them, but unless you change the default option,
your files by default will be in protected Read only format when on
display in the Detail pane. Everything is now neat and tidy:

\includegraphics{imgs/qual_17.png}

If you wish to close documents, you need to do this in the bottom left
hand corner by clicking the small cross in the corner of the document
you wish to close.

So we now have all our documents in NVivo. So what next? We need to
think about why we are going to analyse these documents. To do this,
let's use the `Memos' folder.

\hypertarget{memos-1}{%
\section{Memos}\label{memos-1}}

Memos are related to planning, tracking processes and thinking `out
loud' about what is going on in your data. With that in mind, let's
create a framework of memos. You can create folders to organize
different aspects of note-making. Or you can have the one predefined
folder Memos, but within it name your memos carefully with standard
prefixes which tell you and others what type of memo it is --
``PROCESS-\ldots{}'', ``THEORY\ldots{}''

You create folders from the memos folder -- Right click/New folder /name
the folder as above. Or create memos within your chosen folder by using
the Create ribbon tab along the tab/choose the memo icon - or
alternatively as usual -- you can select the correct folder and then
right click in the List pane to create the new memo. I find right
clicking is always the most straightforward way. The new memo opens up
in Edit mode so that you can begin work in it. If the memo closes,
double click on the memo in the list, it will re-open but you will have
to `click to edit' in order to write in it. So let's try this.

\hypertarget{activity-6-memos}{%
\subsection{Activity 6: Memos}\label{activity-6-memos}}

See if you can follow what I've done for your own data.

I've created two folders: `Research Questions' and `Theoretical
Framework'

\includegraphics{imgs/qual_18.png}

I've highlighted the research questions folder, then right clicked in
the List View to create a new Memo called `RQs'. I've then clicked in
the Detail View and starting making notes on my research questions.

\includegraphics{imgs/qual_19.png}

As you can see, in my first research question I'm interested in
analysing how the media represented Lewis Hamilton following allegations
of tax avoidance.

In the second question I'm aiming to analysis how media constructions
differ from official constructions, if at all.

I've also created a second Memo entitled `Theoretical Framework'. In
here I've made notes on which criminological theories I might use to
inform my analysis. For instance, I've made notes about rational choice
theory and routine activities theory. These theories can guide my
analysis.

\includegraphics{imgs/qual_20.png}

When I analyse my data (we'll do this properly next week), we can begin
to integrate our different data sources together and organise the data
around key themes and codes. Coding our data is by far the most
important step!!! Coding schemes and coded retrieval are key tools of
qualitative analysis. To do this in NVivo, we use `Nodes'.

For now, forget about `nodes' and coding your data. We'll do this next
week in much more depth. For now, we're going to learn to do a basic,
superficial analysis of our textual data.

\hypertarget{word-frequency}{%
\section{Word frequency}\label{word-frequency}}

Finding ways to give others insight into your qualitative survey data
can be challenging. You often end up with pages of response text, which
would quickly overwhelm readers. However, considering word frequency is
a great option for instant accessibility to this qualitative data.

Word clouds, a visualisation of word frequency, can add clarity during
text analysis in order to effectively communicate your data results.
Word clouds can also reveal patterns in your responses that may guide
future analysis.

\hypertarget{activity-7-word-frequency}{%
\subsection{Activity 7: Word
frequency}\label{activity-7-word-frequency}}

You're now going to analyse your imported documents by the frequency
with which words appear in them. To do this, first, open up the
`Queries' section in the Navigation View. Then select `Queries' and
right click in the List View pane. You'll then get the options as below.
Select `New Query' and then `Word Frequency':

\includegraphics{imgs/qual_21.png}

This will open up the `Unnamed Query' panel in the Detail View. (Ignore
the untimely email that came from the TSSO\ldots{}!). At this point,
simply ensure all the default settings are selected and click `Run
Query'.

\includegraphics{imgs/qual_22.png}

This will produce a list of all the words in all your documents order
them according to `Count' i.e.~the word with the most hits appears at
the top:

\includegraphics{imgs/qual_23.png}

This is not great to look at so what we can do is create a `Word Cloud.
Simply choose the `Word Cloud' option at the top of the Detail View and
this translates your query into a neat visualisation of the word
frequencies:

\includegraphics{imgs/qual_24.png}

We can then save this image as a picture. Right click anywhere in the
Detail View and you'll have an option to `Export'. Select this:

\includegraphics{imgs/qual_25.png}

Make sure you choose a suitable file format, such as JPEG Image. Give
the Word Cloud a name and click Ok.

\includegraphics{imgs/qual_26.png}

Your Word Cloud is then saved as a picture, like below. You could then
insert this into any of essays or reports as an example of a
superficial, but indicative, insight into the content of your documents.

\includegraphics{imgs/qual_27.png}

That said, considering all my documents were identified in relation to
`Tax', it was pretty obvious that `Tax' would be the main word, isn't
it?! In other words, the findings here are an artefact of my data search
at the start.

So how about we remove `Tax' from our word frequency query? To do this,
you can go back to the `Summary' tab, right click on the word you want
to exclude, and then choose `Add to Stop Words List'.

\includegraphics{imgs/qual_28.png}

Then run another query, as above, and see what happens now.

If you like the new Word Cloud better, you can then `Save Query', give
it an appropriate title and it will save in your project.

\includegraphics{imgs/qual_29.png}

I've then followed the above directions again to save the Word Cloud as
a JPEG for future use.

\includegraphics{imgs/qual_30.png}

What does this tell us? Well, to be clear, this is a crude analysis. But
it does throw up some interesting key words: `HMRC', `Scheme',
`Arrangements'.

We can use these words to direct our coding of the data (or not). We'll
look at this more next week.

\hypertarget{summary-7}{%
\section{Summary}\label{summary-7}}

You now know how to create a project in NVivo, import relevant
documents, organise them into folders, tidy them up, create memos to
structure your thinking, and undertake a basic analysis of your
documents for frequencies.

These are all fundamentals to using NVivo. Of course, there are lots of
tabs, functions and areas we haven't explored, and won't explore. These
are much more advanced tools.

Next week we'll build on these basics and put our minds to work on
developing conceptual frameworks through coding our data -- this is
where the systematic analysis begins\ldots{}

\hypertarget{week9}{%
\chapter{Week 9}\label{week9}}

\hypertarget{learning-outcomes-8}{%
\section{Learning outcomes}\label{learning-outcomes-8}}

In this session you'll learn: - How to interrogate qualitative data
using NVivo. - How to build theory through deductive, inductive and
abductive strategising. - Some useful analysis features in NVivo,
including how to work with non-textual data.

\hypertarget{deduction-induction-adaption-key-analytical-strategies-for-qualitative-data}{%
\section{Deduction, induction, adaption -- key analytical strategies for
qualitative
data}\label{deduction-induction-adaption-key-analytical-strategies-for-qualitative-data}}

To recap from the lecture, this diagram gives some indication as to the
directions of data analysis:

\includegraphics{imgs/qual_31.png}

We discussed this in the lecture so you'll have an understanding now of
strategies for approaching research including data collection and data
analysis.

\textbf{Induction} is considered to be `bottom-up' where we start with
very broad general interests but build theory and concepts by first
coding and interrogating the data. The key here is to ensure existing
theoretical perspectives and concepts do not over-define our analysis
and thus obscure the possibility of identifying and developing new
concepts and theories.

\textbf{Deduction} is considered to be `top-down' reasoning where we
have predetermined ideas and theories and seek to test or evidence
these. The coding categories we use are therefore explicit in directing
the focus of our analysis.

\textbf{Abduction} integrates these two approaches, giving flexibility
across the analytical process, and ensuring interplay between our ideas
and data.

In this session you'll learn how to interrogate your qualitative data
using NVivo from all perspectives. You can read more about these
strategies here.

\hypertarget{coding-qualitative-data-using-nodes}{%
\section{`Coding' qualitative data using
`nodes'}\label{coding-qualitative-data-using-nodes}}

A `code in qualitative inquiry is most often a word or short phrase that
symbolically assigns a summative, salient, essence-capturing, and/or
evocative attribute for a portion of language-based or visual data'
(Saldaña, 2016: 3). You can read the
\href{http://stevescollection.weebly.com/uploads/1/3/8/6/13866629/saldana_2009_the-coding-manual-for-qualitative-researchers.pdf}{first
chapter of Saldaña's The Coding Manual here}.

`Coding', `coding schemes' and `coded retrieval' of our data are key
tools of qualitative analysis. The terminology and philosophies that
underpin coding processes are explained below (but mainly in the
lectures) and we find that specific methodologies use particular
routines when coding. For instance, one common approach is informed by
`Grounded Theory' that involves both induction and deduction to code
data. Here, researchers would undertake a first layer of `coding', often
called
\href{http://methods.sagepub.com/reference/sage-encyc-qualitative-research-methods/n299.xml}{`open
coding'}, to break down data into indicative themes and concepts. This
precedes more granular coding referred to as
\href{http://methods.sagepub.com/reference/encyc-of-case-study-research/n54.xml}{`axial
coding'}, where categories and concepts are further refined. We'll draw
on this approach in today's session. The structures of coding schemes,
alternate groupings and basic retrieval mechanisms are key to moving
forward with analysis.

In NVivo we code our data using `nodes'. This is sometimes also termed
the `indexing' of our data.

\textbf{NOTE: If you only take one thing away from today's session then
it should be the importance of nodes. These are the fundamental building
blocks of the theories and concepts we interpret in our data.}

So, let's learn how to code using `nodes'.

\hypertarget{what-are-nodes}{%
\subsection{What are `Nodes'?}\label{what-are-nodes}}

Node is a term which refers to a point in the NVivo database but a code
label may be the name you give the node. Codes or nodes can be your
ideas about the data -- they can be generated inductively, deductively
or abductively and may be refined, changed, grouped or deleted at any
time. Applying nodes etc., to passages of source data at a minimum,
provides the basic code and retrieve actions needed to accumulate
together, all the bits of data linked by common threads and themes.

\textbf{Nodes can be containers} within which we locate our data related
to particular themes or ideas of interest. We can use them in this way
organise our thinking. For instance, if we were looking at the Lewis
Hamilton data from last week, we might have created nodes such as:
`methods', `ethics' or `perceptions' of tax avoidance. Within each of
these `nodes' we could copy key sections from our media articles,
journal articles and policy documents into them in order to collect data
about key themes. Nodes here then are more reflective and act as
containers for or links to data exemplars based on, conceptual ideas,
themes, codes or more structurally for people, contexts, places etc.
Essentially, the terms nodes, codes, keywords, and themes are used
similarly.

\textbf{Nodes can also be empty} -- for example, they can act like
hierarchical top-level codes with nodes underneath them that do contain
or have been applied to, data. So, we might have created a top-level
node entitled `tax avoidance' and then created sub-level nodes entitled
`methods', `ethics' and `perceptions'. We might even created further
layers of nodes within each sub-level e.g.~in the `perceptions' node, we
might break that down further to `perceptions of the public',
`perceptions of enforcement authorities', `perception of politicians',
and so on.

These node layers therefore build a structural framework for our data.
Plus, each Node of any sort can be linked directly to one memo -- so
that relevant analytic notes are easily accessible from the node itself.

Let's do this then. Here we'll focus on using nodes for reflective
purposes and for thematic purposes.

\hypertarget{activity-1-coding}{%
\subsection{Activity 1: Coding}\label{activity-1-coding}}

We need to start by importing our data. Start by opening NVivo and
creating a new project -- call it something relevant for today's session
e.g.~Lab Session Week 8.

On Blackboard you'll fine two datasets in the Week 9 folder: 1. Dataset
1 Probation Interviews 2. Dataset 2 GMP Twitter data.

You'll need to download and save these to your p:drive. (Dataset 1 is a
zip file, you'll need to unzip it). Once you've done this, go back to
NVivo and import them, using the knowledge you gained last week. For
instance, first, create folders within your internals section to house
the data (see below). Second, import your data. If you import all the
interviews at once, there is a small chance you computer will explode,
catch fire, and burn the lab down. But let's hope not. Be patient
though, it can take a few minutes to import so much data at once. If it
does crash though, you might just need to import the transcripts in
smaller numbers. You can see what I've done in the screenshot below:

\includegraphics{imgs/qual_32.png}

If you double click on any of the transcripts in the List Pane you'll be
able to see the content in the Detail Pane.

For Mac users, when you import the GMP Twitter Data, you can import as a
`Dataset', rather than as `Documents'. You'll first encounter the screen
below:

\includegraphics{imgs/qual_33.png}

Simply click `next', then `next' again in the following window and then
`Import'. You'll then be able to see the Twitter data in the Detail View
as below:

\includegraphics{imgs/qual_34.png}

For Windows users, the process is slightly different as there is no
`Datasets' option. To import data from an Excel file on the Windows
version of NVivo, go to ``Import'', ``Import survey'' and ``From
Microsoft Excel File''. Alternatively, click on the ``Data'' heading and
select ``Survey''. In the drop down menu, select the file type (Excel or
CSV, in this case our data is an Excel file).

\includegraphics{imgs/qual_35.png}

You will have to click through the Survey Import Wizard which looks like
this:

\includegraphics{imgs/qual_36.png}

There are four steps to click through where you check to ensure the data
will be imported correctly. In this case, you can just click ok through
each step without making any changes. After step 4, click ``Finish''. It
may take a few minutes for NVivo to process the request.

\includegraphics{imgs/qual_37.png}

This should be the end result when you finish the import wizard:

\includegraphics{imgs/qual_38.png}

Ok. So now we have both our datasets imported into NVivo. It's probably
worth saving your project now, just in case something goes wrong.

Usually, as these two datasets are very separate in terms of the
purpose, we would probably create two separate projects in NVivo in
order to keep our separate research projects apart. For the purposes of
getting to know the software though, we can analyse both in one project
today.

\hypertarget{using-mind-maps-to-organise-your-analytical-process-and-framework}{%
\section{Using `mind maps' to organise your `analytical process and
framework'}\label{using-mind-maps-to-organise-your-analytical-process-and-framework}}

At this point, we need to give our coding some focus. To do this we can
use `mind maps' so we know how to code our data. To demonstrate this, we
are going to code the interview data in Dataset 1 using a deductive
approach; and then code the Twitter data in Dataset 2 using an inductive
approach.

\hypertarget{activity-2-deductive-coding}{%
\subsection{Activity 2: Deductive
coding}\label{activity-2-deductive-coding}}

First, single click on `Maps' in the Navigation Pane, then right click
in the List Pane to create a new mind map. Call it Deductive Approach to
Probation Interview Data. You should end up with the following:

\includegraphics{imgs/qual_39.png}

Now, deductive logic assumes that we have a predetermined set of codes,
ideas and themes that we intend to better understand. For instance, the
interviews that were carried out with Chief Probation Officers (CPOs) in
this research had the following clear research objectives:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  to investigate the social and educational backgrounds and career
  histories of CPOs;
\item
  to explore their perceptions of community penalties, developments in
  these over the course of their careers and potential changes in the
  future;
\item
  to examine the role of CPOs as managers and their relationships with
  central government, local agencies and their probation committees. So
  we can extrapolate key ideas from these objectives and use them to
  guide our coding of the data. I've broken down the research objectives
  into key concepts and organised this in my mind map. Take a look at my
  screenshot below and see if you can replicate what I've done. You
  might choose to pick a different structure, or different codes, that's
  fine too. Play around with creating `children', `siblings' and
  `floating ideas'. Play around with colours and structure.
\end{enumerate}

\includegraphics{imgs/qual_40.png}

What we've done here is create a predetermined framework for coding the
interview data. That is, when we read the transcripts, we can assign
these themes to what was said in the interviews. Plus, to save us having
to create all the nodes individually, we can now just use the `create as
nodes' option. Click it then choose `Nodes'. Now click on `Nodes' in the
Navigation Pane and you'll see the following -- open up the folders so
you can see all the nodes:

\includegraphics{imgs/qual_41.png}

This is where the hard work begins. We now have to code our interview
data.

There are multiple ways to code data at nodes you have already created.
You can also use some of these methods to select more than one code to
apply to the selected passage of data.

The easiest way to do this is as follows (though NVivo gives you many
ways to code the data). First, go back to your internals, click on the
Dataset 1 folder, and double click on Interview 1 (int01). This will
open the transcript in the Detail Pane. For ease, now click on `Nodes'
so you can see all your created nodes and have them in mind when you
read the transcript. Now start reading the transcript and each time you
read something that corresponds with your `nodes', you need to `code'
it.

For Mac users, to do this, simply highlight the text and right-click on
it, choose `code selection' and then `at existing nodes or cases':

\includegraphics{imgs/qual_42.png}

For Windows users this looks slightly different. You have to have to
click on ``Code'' then a new window pops up with the nodes and you
select from there. The right-click menu to code selected text looks like
this:

\includegraphics{imgs/qual_43.png}

You'll notice the passage I've selected relates to career histories so
I'm going to code this sentence in the corresponding node. However, I
also think it relates to `educational background' so I'm going to code
this sentence in two `nodes':

\includegraphics{imgs/qual_44.png}

In Windows, to select multiple codes, you must hit ``ctrl'' and then
click the additional node you want:

\includegraphics{imgs/qual_45.png}

Keep reading through the transcript and coding segments at single or
multiple nodes as you see fit. You're now coding your data deductively
using a predetermined conceptual/thematic framework.

Keep coding until you feel like you fully understand how this works and
until you've coded some data for each of the predetermined `nodes'.
There is an option for ``recent codes'' that shows what nodes you have
been using and this can be slightly easier than opening the window back
up, but it depends on how you wish to code the data. Perhaps also code
text from other interviews too. You don't have to read all the
transcripts, just read enough to understand the process and be
comfortable with the software.

But where has all our coding gone? The best way to visualise what we've
done is to go to the ribbon tab at the top, chose `view', then `coding
stripes', then `nodes recently coding' and you'll see the following:

\includegraphics{imgs/qual_46.png}

At the bottom right corner in the Detail Pane you'll be able to see the
nodes that we have coded.

To speed up the coding, you might want to `drag and drop'. This is a
favourite for many researchers, as coding in this way allows you to drag
a highlighted selection of data onto any code showing in the List pane.
This, of course, necessitates the list pane be showing Nodes (as opposed
to Sources or anything else from the Navigation pane). Simply hold down
left click of mouse and drag it onto the desired node

IMPORTANT: One way to speed up drag and drop coding and see the most
nodes possible on the screen at one time is to rearrange the windows so
that the List containing the nodes is on the left side, and the Detail
pane of the source is on the right side. Do this by going to View Ribbon
tab /Detail View /Right. Then arrange vertical splitter bars to
accommodate as much text on the right as possible while seeing enough of
the code labels on the left. You'll then be able to view like this:

\includegraphics{imgs/qual_47.png}

In theory, you would now read every single transcript and code your
data. We'll look at what you can do with this coding after we've looked
at inductive coding.

\hypertarget{activity-3-inductive-coding}{%
\subsection{Activity 3: Inductive
coding}\label{activity-3-inductive-coding}}

To better organise our data and to keep our nodes for our two datasets
separate, it's worth creating two `node folders'. If we were working on
only one dataset we wouldn't necessarily need to do this. However, for
now, right-click on the `Nodes' folder in the Nodes section in the
Navigation Pane. Create a folder called Dataset 1, then another called
Dataset 2. Now, click back on the Nodes folder, select all the Nodes you
created, and drag them into the Dataset 1 folder. We can then put all
our new Nodes for the Twitter data in the folder entitled Dataset 2, as
below:

\includegraphics{imgs/qual_48.png}

Now let's start our Twitter data inductive coding. As above, let's start
by creating another mind map, but this time for the Twitter data. We're
not looking to create predetermined codes as with the deductive
approach. Instead, we want to remain open minded about what themes are
in the data. So let's remind ourselves of this. See if you can produce
something like the following:

\includegraphics{imgs/qual_49.png}

Remember, we don't want to approach the data with too many concrete,
predetermined ideas about what we will find in the data. Instead, we
want to have a few open, guiding questions that will allow us to code
our data as we read it.

So now to the hard work again. But this time we'll need to create out
nodes, inductively, as we read through the tweets.

Start by clicking on Dataset 2 in the Sources section in the Navigation
Pane, then double clicking on the dataset in the List Pane to open it up
in the Detail Pane. For the purpose of the coding today we're only
really interested in the texts of the tweets.

Read through the tweets one by one and each time you come across a theme
of interest, code it by highlighting it, right-clicking and choosing
`code selection' then `at new node'. I think the first tweet relates to
`partnership working', so this is what I've called the node. I've now
gone on to create more Nodes, some of the text relates to more than one
Node. I've also started to create a hierarchy of Nodes and Sub-Nodes, as
with the `Crime Types' node folder. As you'll see, I've changed the view
to make it easier to create new Nodes and code at existing Nodes by
dragging and dropping. I've also viewed the code stripes on the right
hand side. You'll notice that when you create Nodes that they are
located in the top-level Nodes folder. So once you've finished your
coding you can select and drag them into your Dataset 2 Nodes folder to
keep it all well organised:

\includegraphics{imgs/qual_50.png}

In theory, you'd read through all the tweets and code everything you
consider important thematically in this case. You'd then read them all
again; and again; and then again, and so on, until you're happy with the
coding framework you've developed. You can do this in your own time. You
could end up with hundreds of Nodes; you can merge Nodes; un-code text;
delete nodes etc., until you're satisfied.

\textbf{TIP: The great thing about folders in NVivo is that you can
change your mind, restructure them and move or drag nodes around between
them whenever you like -- really easily. I'll let you explore all this
in your own time. It's all intuitive and if you ever get stuck you can
watch the tutorials on the QSR website.}

\hypertarget{abductive-coding}{%
\section{Abductive coding}\label{abductive-coding}}

Essentially, this involves going back and forth between the deductive
approach and the inductive approach. You might wish to create some
pre-determined codes/nodes based on your interests and/or theory and
then flexibly adapt these as you interrogate your data.

\hypertarget{activity-4-retrieval-viewing-coded-data}{%
\subsection{Activity 4: Retrieval -- viewing coded
data}\label{activity-4-retrieval-viewing-coded-data}}

If you need to look vertically through one file at a time, reviewing
what you have done, open a document and ensure Select Coding stripes
/All has been selected:

\includegraphics{imgs/qual_51.png}

This way, you will see any codes appearing in the data. Though this is
not so easy since all codes may occupy a very wide margin space and
require much scrolling. Or you can selectively review codes (

Once you have done some coding, you may want to review the passages you
have selected for a particular node. To do so, double click on the code
you want in the List pane. This will open the code in the Detail pane.
Each source that has references to the chosen code is listed, headed by
a hyperlink back to the source.

The tabs down the side provide different ways to view the references,
and are dependent on the type of data media coded there.

\textbf{ANALYSIS COMMENT: There are many reasons why you may wish to see
what coding you've done, or what content is coded at a node. You may
want to review your coding, to compare each passage with the other
passages or by means of showing Coding stripes to browse what else is
coded in this source at that node, or simply to get back to coding where
you left off. }

\hypertarget{activity-5-viewing-more-context}{%
\subsection{Activity 5: Viewing more
context}\label{activity-5-viewing-more-context}}

It may be useful to be able to see the surrounding context of a coding
reference you're examining. This can be done in several ways:

\begin{itemize}
\tightlist
\item
  You can jump to the source by Right-clicking on the passage
  \textgreater{} Open Referenced Source. This will take you to the
  source and highlight the passage in which you were interested:
  \includegraphics{imgs/qual_52.png}
\item
  You can view additional content surrounding your passage of interest
  without going back to the whole source by R-clicking on the passage
  \textgreater{} Coding Context \textgreater{} and choosing how much you
  wish to see: \includegraphics{imgs/qual_52.png}
\end{itemize}

this will bring in your selected quantity in light grey type so you can
differentiate it from what's actually coded at that node at which point
you can select more of it to code into that node if you would like.

\hypertarget{export-coded-data}{%
\section{Export coded data}\label{export-coded-data}}

There are extensive ways to output data material about your data to
other formats and applications. Not all ways are included here since
there are infinite combinations of settings required for different
reasons which will be based on your own particular requirements.

\hypertarget{activity-6-exporting-via-the-list-pane}{%
\subsection{Activity 6: Exporting via the list
pane}\label{activity-6-exporting-via-the-list-pane}}

This is the general and usually used qualitative form of output which
can be achieved almost anywhere in the package by different methods.
This would usually be what is required for e.g.~coded output and is
easier to generate than the more formal Reporting functions below.

In List pane -- select the item/s you want to export content for, say a
Node or a Document, Right click \textgreater{} Export

\textbf{IMPORTANT: If you accept the default Entire Content an html file
will be created}

If you want a Word file or files to be created click on down arrow and
select Reference View(Check any of the additional options carefully
e.g.~always check the Name option, this will insert the name of the item
in the content itself. If you are uncertain about where the file will
go, pay special attention to the File saving window which opens to allow
you to create a new folder or to locate the file usefully. Similarly
check the Open on Export option so that you can see the editable file/s
you have generated on screen before closing them down.

Give this a go now with the Nodes for Dataset 1. First, click on Dataset
1 Nodes in the Navigation pane. Then Right-click on the Node you want a
report on (I've chosen the `Career History' Node) and choose Export.
Find a suitable location to save the data and ensure you save as a Word
file. This will be easiest to view. Now, you should have a neat overview
of your Node that you could use for quotes in your essays,
dissertations, or journal articles.

\hypertarget{reports-note-windows-only-users-not-supported-by-mac-yet}{%
\section{Reports (NOTE: WINDOWS ONLY USERS! NOT SUPPORTED BY MAC,
YET!!)}\label{reports-note-windows-only-users-not-supported-by-mac-yet}}

The formalized \textbf{Reports} and associated \textbf{Extracts}
function (in Navigation bar or Explore/Ribbon group) specifically
concerns the support provided for mixed qual/quant methods. Some of the
reports only provide quantitative information or summaries. Experiment
with these via the Help Menu.

Two standardized Reports will also provide e.g.~qualitative coded source
data.

\begin{itemize}
\tightlist
\item
  Coding Summary Report by Node
\item
  Coding Summary Report by Source
\end{itemize}

These reports are essentially the same but allow for different sorting
mechanisms. This would be a quick way to export many codes at once.
Experiment with all the drop-down options and Select buttons.

See if you can create a Report on your Nodes and Sources.

MAC USERS -- play around to see what kind of things you can create. It's
all about trial and error at this stage.

\hypertarget{some-other-useful-things-to-do}{%
\section{Some other useful things to
do}\label{some-other-useful-things-to-do}}

Below are some more tools you might get some use out of.

\hypertarget{create-another-word-cloud}{%
\subsection{Create another Word
Cloud:}\label{create-another-word-cloud}}

See if you can replicate something like this from last week:

\includegraphics{imgs/qual_54.png}

This is a Word Cloud for Dataset 1 only.

\hypertarget{create-a-word-tree}{%
\subsection{Create a Word Tree:}\label{create-a-word-tree}}

Using the Query function, I've done a text search for the words `Career'
AND `Probation' AND `Money' AND `Lifestyle' in Dataset 1 and then chosen
the `Word Tree' option to better understand the key discourse around the
term. In Windows, go to ``Query'' and then ``Query Wizard''. Are there
any relationships here? My query produced this:

\includegraphics{imgs/qual_55.png}

Whilst you can export your Word Tree into jpeg or PDF form, in NVivo you
can also click on the different branches to better understand the
connections. Try it out.

\hypertarget{create-a-concept-map-windows-only}{%
\subsection{Create a Concept Map (WINDOWS
ONLY!!)}\label{create-a-concept-map-windows-only}}

Windows users might want to create a concept map to connect data to
different ideas and thoughts. It is very similar to the Mind Map
function that we did earlier.
\href{https://www.youtube.com/watch?v=3R5gZKdOJD4}{You can learn more
about it here}.

\hypertarget{other-features}{%
\section{Other features\ldots{}}\label{other-features}}

Finally, here are some features that the more innovative amongst you may
wish to make use of. Much of my data is textual in that I often carry
out interviews with people knowledgeable of white-collar and corporate
crimes. But you might be interested in generating different data, such
as audio or visual:

\hypertarget{graphicspictures---making-linked-notes-and-coding}{%
\subsection{Graphics/pictures - making linked notes and
coding}\label{graphicspictures---making-linked-notes-and-coding}}

Try importing a picture. Find something online and save it. Then import
it. Here's my example:

\includegraphics{imgs/qual_56.png}

With the picture on view, click to edit, or enable the editing feature.
You can then:

\begin{itemize}
\tightlist
\item
  Make a selection within the graphic/right click/ Insert a row -- and
  write notes.
\item
  Make a selection/right click /Code selection. (
\item
  Code the notes instead(
\end{itemize}

Have a play with coding and analysing pictures.

\hypertarget{coding-audiovideo-data}{%
\subsection{Coding audio/video data}\label{coding-audiovideo-data}}

Coding multimedia data is very similar to coding textual data. For
either audio or video sources, you can select transcript text and code
per usual, or you can select a segment on the progress bar and code that
directly as if you'd highlighted text.

So, if you have some music on your iTunes, try importing a song to see
what it looks like then see if you can code lyrics and sections of the
song. I've put some Bruce Springsteen in my project if you want to try
it out there\ldots{}

\hypertarget{more-on-capturing-web-material}{%
\section{More on capturing web
material}\label{more-on-capturing-web-material}}

Last week we tried to import html pages for our media articles. But
another, and actually better, way of doing this is by using NCapture.

A growing variety of web content and social media is available to you
using NCapture, a web browser extension, that is available for free with
NVivo. It allows you to capture and import a screenshot of any webpage
as a PDF or various social media sites (such as Twitter, Facebook, and
LinkedIn) as a dataset (table). Once you have NCapture installed, open
your browser, and navigate to the website of interest.

To learn more about this
\href{http://www.qsrinternational.com/nvivo/support-overview/faqs/what-is-ncapture}{click
here}.

You'll need to do this on your personal machines as it is unlikely to be
installed on the lab computers.

Once installed, the process is as follows:

\begin{itemize}
\tightlist
\item
  Click the NCapture icon in your browser bar
\item
  A dialogue box opens (right)
\item
  Choose your source type (probably `Web Page as PDF'), source name,
  optional description, memo nodes, etc.
\item
  Click Capture
\item
  See NCapture Progress page below
\end{itemize}

\textbf{NOTE: WEB PAGES ARE CAPTURED USING THE PDF OPTION -- SOCIAL
MEDIA WOULD BE CAPTURED USING THE DATASET OPTION}

\begin{itemize}
\tightlist
\item
  Go to the Sources section of the Navigation pane \textgreater{}
  Internals \textgreater{} Web Content subfolder (or wherever you'd like
  the new Sources to be created) (
\item
  Go to the External Data Ribbon tab \textgreater{} From Other Sources
  \textgreater{} From NCapture (as seen left) (
\item
  A dialogue box opens, as seen below , showing recent captures. Choose
  the captures you'd like, and click Import. Webpages will be brought in
  as pdfs; social media files as tables, depending on your choices
  during capture.
\end{itemize}

Then analyse as we have been doing in the sessions.

\hypertarget{summary-8}{%
\section{Summary}\label{summary-8}}

You can use your nodes to build your ideas and theories, or identify
evidence to support your theoretical propositions. You can focus in on
particular themes, compare across nodes and the data, and extract to
support your academic work in a systematic way. Using NVIVO gives you
structure and management for your data, helps you develop clear
organizing and conceptual frameworks, and allows you to interrogate the
data at different levels and in different directions.

\bibliography{book.bib,packages.bib}


\end{document}
